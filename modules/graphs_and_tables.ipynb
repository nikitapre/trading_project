{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa02f229-1d5f-4980-9aec-6db3ffb6eda6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall_kline_changes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_target_column_mod\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import shap\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn.inspection import permutation_importance\n",
    "from datetime import timedelta\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from .all_kline_changes import add_target_column_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4fccc-7cca-4d06-96a0-84d4a0a3022f",
   "metadata": {},
   "source": [
    "**–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ F1, Precision, recall, –ø–æ—Ä–æ–≥–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è (thrashold) –∏ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc2c483-9650-4cc8-b24a-10329c3e8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_threshold(model, X_train, y_train, X_valid, y_valid, X_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "    \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n",
    "    {\n",
    "        'model': model,  # –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "        'metrics': {\n",
    "            'train': {–º–µ—Ç—Ä–∏–∫–∏},\n",
    "            'valid': {–º–µ—Ç—Ä–∏–∫–∏},\n",
    "            'test': {–º–µ—Ç—Ä–∏–∫–∏} (–µ—Å–ª–∏ –µ—Å—Ç—å),\n",
    "            'optimal_threshold': float\n",
    "        },\n",
    "        'features': list,  # —Å–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π\n",
    "        'timestamp': str   # –≤—Ä–µ–º—è –æ—Ü–µ–Ω–∫–∏\n",
    "    }\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    # 1. –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "    y_valid_proba = model.predict_proba(X_valid)[:, 1]\n",
    "    \n",
    "    if X_test is not None and y_test is not None:\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # 2. –°–æ–∑–¥–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ—Ä–æ–≥–æ–≤\n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    \n",
    "    # 3. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è F1 –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–∞—Ö\n",
    "    def find_best_threshold(y_true, y_proba, thresholds):\n",
    "        f1_scores = []\n",
    "        for t in thresholds:\n",
    "            y_pred = (y_proba >= t).astype(int)\n",
    "            f1_scores.append(f1_score(y_true, y_pred, zero_division=0))\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        return thresholds[best_idx], f1_scores\n",
    "    \n",
    "    # 4. –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è train –∏ valid\n",
    "    train_best_threshold, train_f1_scores = find_best_threshold(y_train, y_train_proba, thresholds)\n",
    "    valid_best_threshold, valid_f1_scores = find_best_threshold(y_valid, y_valid_proba, thresholds)\n",
    "    \n",
    "    # 5. –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–π –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥\n",
    "    optimal_threshold = np.mean([train_best_threshold, valid_best_threshold])\n",
    "    \n",
    "    # 6. –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
    "    train_metrics = {\n",
    "        'thresholds': thresholds,\n",
    "        'f1_scores': train_f1_scores,\n",
    "        'precision': [precision_score(y_train, (y_train_proba >= t).astype(int), zero_division=0) for t in thresholds],\n",
    "        'recall': [recall_score(y_train, (y_train_proba >= t).astype(int), zero_division=0) for t in thresholds],\n",
    "        'y_proba': y_train_proba,\n",
    "        'max_f1_threshold': train_best_threshold,\n",
    "        'roc_auc': roc_auc_score(y_train, y_train_proba)  # –î–æ–±–∞–≤–ª–µ–Ω–æ ROC AUC\n",
    "    }\n",
    "    \n",
    "    valid_metrics = {\n",
    "        'thresholds': thresholds,\n",
    "        'f1_scores': valid_f1_scores,\n",
    "        'precision': [precision_score(y_valid, (y_valid_proba >= t).astype(int), zero_division=0) for t in thresholds],\n",
    "        'recall': [recall_score(y_valid, (y_valid_proba >= t).astype(int), zero_division=0) for t in thresholds],\n",
    "        'y_proba': y_valid_proba,\n",
    "        'max_f1_threshold': valid_best_threshold,\n",
    "        'roc_auc': roc_auc_score(y_valid, y_valid_proba)  # –î–æ–±–∞–≤–ª–µ–Ω–æ ROC AUC\n",
    "    }\n",
    "    \n",
    "    # 7. –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    print(f\"üéØ –õ—É—á—à–∏–π –ø–æ—Ä–æ–≥ –ø–æ F1 (Train): {train_best_threshold:.4f}\")\n",
    "    print(f\"üéØ –õ—É—á—à–∏–π –ø–æ—Ä–æ–≥ –ø–æ F1 (Valid): {valid_best_threshold:.4f}\")\n",
    "    print(f\"‚úÖ –£—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {optimal_threshold:.4f}\")\n",
    "    print(f\"\\nüìä ROC AUC Scores:\")\n",
    "    print(f\"‚úÖ Train ROC AUC: {train_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"‚úÖ Valid ROC AUC: {valid_metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    # 8. –°—á–∏—Ç–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º\n",
    "    def calculate_final_metrics(y_true, y_proba, threshold, set_name):\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        metrics = {\n",
    "            'F1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'ROC_AUC': roc_auc_score(y_true, y_proba)  # –î–æ–±–∞–≤–ª–µ–Ω–æ ROC AUC\n",
    "        }\n",
    "        print(f\"\\nüìä {set_name} set (Threshold = {threshold:.4f}):\")\n",
    "        print(f\"‚úÖ F1: {metrics['F1']:.4f}\")\n",
    "        print(f\"‚úÖ Precision: {metrics['Precision']:.4f}\")\n",
    "        print(f\"‚úÖ Recall: {metrics['Recall']:.4f}\")\n",
    "        print(f\"‚úÖ ROC AUC: {metrics['ROC_AUC']:.4f}\")\n",
    "        return metrics\n",
    "    \n",
    "    train_metrics['final_metrics'] = calculate_final_metrics(y_train, y_train_proba, optimal_threshold, \"Train\")\n",
    "    valid_metrics['final_metrics'] = calculate_final_metrics(y_valid, y_valid_proba, optimal_threshold, \"Valid\")\n",
    "    \n",
    "    results = {\n",
    "        'train': train_metrics,\n",
    "        'valid': valid_metrics,\n",
    "        'optimal_threshold': optimal_threshold\n",
    "    }\n",
    "    \n",
    "    if X_test is not None and y_test is not None:\n",
    "        test_metrics = {\n",
    "            'thresholds': thresholds,\n",
    "            'f1_scores': [f1_score(y_test, (y_test_proba >= t).astype(int), zero_division=0) for t in thresholds],\n",
    "            'precision': [precision_score(y_test, (y_test_proba >= t).astype(int), zero_division=0) for t in thresholds],\n",
    "            'recall': [recall_score(y_test, (y_test_proba >= t).astype(int), zero_division=0) for t in thresholds],\n",
    "            'y_proba': y_test_proba,\n",
    "            'roc_auc': roc_auc_score(y_test, y_test_proba)  # –î–æ–±–∞–≤–ª–µ–Ω–æ ROC AUC\n",
    "        }\n",
    "        test_metrics['final_metrics'] = calculate_final_metrics(\n",
    "            y_test, y_test_proba, optimal_threshold, \"Test\"\n",
    "        )\n",
    "        results['test'] = test_metrics\n",
    "    \n",
    "    # 9. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # 1. –ö—Ä–∏–≤—ã–µ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_metrics['thresholds'], train_metrics['precision'], label='Precision', color='blue')\n",
    "    plt.plot(train_metrics['thresholds'], train_metrics['recall'], label='Recall', color='green')\n",
    "    plt.plot(train_metrics['thresholds'], train_metrics['f1_scores'], label='F1', color='red')\n",
    "    plt.axvline(optimal_threshold, color='k', linestyle='-', label=f'Avg Optimal: {optimal_threshold:.3f}')\n",
    "    plt.axvline(train_best_threshold, color='b', linestyle=':', label=f'Train Max F1: {train_best_threshold:.3f}')\n",
    "    plt.title('Train Selection')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    # 2. –ö—Ä–∏–≤—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(valid_metrics['thresholds'], valid_metrics['precision'], label='Precision', color='blue')\n",
    "    plt.plot(valid_metrics['thresholds'], valid_metrics['recall'], label='Recall', color='green')\n",
    "    plt.plot(valid_metrics['thresholds'], valid_metrics['f1_scores'], label='F1', color='red')\n",
    "    plt.axvline(optimal_threshold, color='k', linestyle='-', label=f'Avg Optimal: {optimal_threshold:.3f}')\n",
    "    plt.axvline(valid_best_threshold, color='orange', linestyle=':', label=f'Valid Max F1: {valid_best_threshold:.3f}')\n",
    "    plt.title('Test Set')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    # 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ F1-–∫—Ä–∏–≤—ã—Ö —Å –Ω–æ–≤—ã–º –ø–æ—Ä–æ–≥–æ–º\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(train_metrics['thresholds'], train_metrics['f1_scores'], label='Train F1', color='blue')\n",
    "    plt.plot(valid_metrics['thresholds'], valid_metrics['f1_scores'], label='Valid F1', color='orange')\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª–µ–Ω–∞ —Ç—Ä–µ—Ç—å—è –ª–∏–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å\n",
    "    if X_test is not None and y_test is not None:\n",
    "        plt.plot(test_metrics['thresholds'], test_metrics['f1_scores'], label='Test F1', color='green')\n",
    "    \n",
    "    plt.axvline(optimal_threshold, color='k', linestyle='-', label=f'Avg Optimal: {optimal_threshold:.3f}')\n",
    "    plt.axvline(train_best_threshold, color='b', linestyle=':', alpha=0.5)\n",
    "    plt.axvline(valid_best_threshold, color='orange', linestyle=':', alpha=0.5)\n",
    "    plt.title('F1 Comparison with Optimal Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 10. –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ (–¥–æ–±–∞–≤–ª—è–µ–º ROC AUC)\n",
    "    final_table = [\n",
    "        [\"Dataset\", \"Threshold Type\"] + list(train_metrics['final_metrics'].keys()),\n",
    "        [\"Train\", f\"Average Optimal ({optimal_threshold:.4f})\"] + list(train_metrics['final_metrics'].values()),\n",
    "        [\"Test\", f\"Average Optimal ({optimal_threshold:.4f})\"] + list(valid_metrics['final_metrics'].values())\n",
    "    ]\n",
    "    \n",
    "    if X_test is not None and y_test is not None:\n",
    "        final_table.append(\n",
    "            [\"Test\", f\"Average Optimal ({optimal_threshold:.4f})\"] + list(results['test']['final_metrics'].values())\n",
    "        )\n",
    "    \n",
    "    print(\"\\n–ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ —Å—Ä–µ–¥–Ω–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º:\")\n",
    "    print(tabulate(final_table, headers=\"firstrow\", floatfmt=\".4f\", tablefmt=\"grid\"))\n",
    "\n",
    "     # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'metrics': {\n",
    "            'train': train_metrics['final_metrics'],\n",
    "            'valid': valid_metrics['final_metrics'],\n",
    "            'optimal_threshold': optimal_threshold\n",
    "        },\n",
    "        'features': list(X_train.columns),\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    if X_test is not None and y_test is not None:\n",
    "        model_package['metrics']['test'] = results['test']['final_metrics']\n",
    "    \n",
    "    return model_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d80be6-d251-401c-9a4c-0284423fd8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "661085b7-0f34-4d81-83a8-d8b306347232",
   "metadata": {},
   "source": [
    "–ê–Ω–∞–ª–æ–≥–∏—á–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Å —É—á–µ—Ç–æ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5129ae33-6bed-455e-9851-f6b10cbe6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_threshold_mod(model, X_train, y_train, X_valid, y_valid, X_test=None, y_test=None, model_type='sklearn'):\n",
    "    \"\"\"\n",
    "    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å (ML –∏–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—å) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø–æ–¥–±–æ—Ä–æ–º –ø–æ—Ä–æ–≥–∞\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : object\n",
    "        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (sklearn/lightgbm –∏–ª–∏ keras)\n",
    "    model_type : str\n",
    "        –¢–∏–ø –º–æ–¥–µ–ª–∏: 'sklearn' (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é) –∏–ª–∏ 'keras'\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # 1. –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏\n",
    "    if model_type == 'keras':\n",
    "        # –î–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏\n",
    "        y_train_proba = model.predict(X_train, verbose=0)\n",
    "        y_valid_proba = model.predict(X_valid, verbose=0)\n",
    "        \n",
    "        if len(y_train_proba.shape) > 1 and y_train_proba.shape[1] > 1:\n",
    "            y_train_proba = y_train_proba[:, 1] if y_train_proba.shape[1] == 2 else np.argmax(y_train_proba, axis=1)\n",
    "            y_valid_proba = y_valid_proba[:, 1] if y_valid_proba.shape[1] == 2 else np.argmax(y_valid_proba, axis=1)\n",
    "            \n",
    "        if X_test is not None and y_test is not None:\n",
    "            y_test_proba = model.predict(X_test, verbose=0)\n",
    "            if len(y_test_proba.shape) > 1 and y_test_proba.shape[1] > 1:\n",
    "                y_test_proba = y_test_proba[:, 1] if y_test_proba.shape[1] == 2 else np.argmax(y_test_proba, axis=1)\n",
    "    else:\n",
    "        # –î–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö ML-–º–æ–¥–µ–ª–µ–π\n",
    "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        y_valid_proba = model.predict_proba(X_valid)[:, 1]\n",
    "        \n",
    "        if X_test is not None and y_test is not None:\n",
    "            y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # 2. –°–æ–∑–¥–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –ø–æ—Ä–æ–≥–æ–≤\n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    \n",
    "    # 3. –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞ –º–µ–∂–¥—É Precision –∏ Recall\n",
    "    def find_balanced_threshold(y_true, y_proba, thresholds):\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        for t in thresholds:\n",
    "            y_pred = (y_proba >= t).astype(int)\n",
    "            precisions.append(precision_score(y_true, y_pred, zero_division=0))\n",
    "            recalls.append(recall_score(y_true, y_pred, zero_division=0))\n",
    "        \n",
    "        diff = np.abs(np.array(precisions) - np.array(recalls))\n",
    "        exclude = int(len(thresholds) * 0.1)\n",
    "        valid_range = range(exclude, len(thresholds)-exclude)\n",
    "        \n",
    "        if len(valid_range) > 0:\n",
    "            best_idx = valid_range[np.argmin(diff[valid_range])]\n",
    "        else:\n",
    "            best_idx = np.argmin(diff)\n",
    "        \n",
    "        return thresholds[best_idx], precisions, recalls\n",
    "    \n",
    "    # 4. –ù–∞—Ö–æ–¥–∏–º —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏\n",
    "    train_balanced_threshold, train_precisions, train_recalls = find_balanced_threshold(y_train, y_train_proba, thresholds)\n",
    "    valid_balanced_threshold, valid_precisions, valid_recalls = find_balanced_threshold(y_valid, y_valid_proba, thresholds)\n",
    "    optimal_threshold = np.mean([train_balanced_threshold, valid_balanced_threshold])\n",
    "    \n",
    "    # 5. –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫\n",
    "    def calculate_final_metrics(y_true, y_proba, threshold, set_name):\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        metrics = {\n",
    "            'F1': f1_score(y_true, y_pred, zero_division=0),\n",
    "            'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "            'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "            'ROC_AUC': roc_auc_score(y_true, y_proba)\n",
    "        }\n",
    "        print(f\"\\nüìä {set_name} set (Threshold = {threshold:.4f}):\")\n",
    "        print(f\"‚úÖ F1: {metrics['F1']:.4f}\")\n",
    "        print(f\"‚úÖ Precision: {metrics['Precision']:.4f}\")\n",
    "        print(f\"‚úÖ Recall: {metrics['Recall']:.4f}\")\n",
    "        print(f\"‚úÖ ROC AUC: {metrics['ROC_AUC']:.4f}\")\n",
    "        return metrics\n",
    "    \n",
    "    # 6. –í—ã—á–∏—Å–ª—è–µ–º –∏ –≤—ã–≤–æ–¥–∏–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "    print(f\"üéØ –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ (Train): {train_balanced_threshold:.4f}\")\n",
    "    print(f\"üéØ –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ (Valid): {valid_balanced_threshold:.4f}\")\n",
    "    print(f\"‚úÖ –£—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {optimal_threshold:.4f}\")\n",
    "    \n",
    "    train_metrics = {\n",
    "        'thresholds': thresholds,\n",
    "        'precision': train_precisions,\n",
    "        'recall': train_recalls,\n",
    "        'f1_scores': [f1_score(y_train, (y_train_proba >= t).astype(int), zero_division=0) for t in thresholds],\n",
    "        'y_proba': y_train_proba,\n",
    "        'balanced_threshold': train_balanced_threshold,\n",
    "        'final_metrics': calculate_final_metrics(y_train, y_train_proba, optimal_threshold, \"Train\")\n",
    "    }\n",
    "    \n",
    "    valid_metrics = {\n",
    "        'thresholds': thresholds,\n",
    "        'precision': valid_precisions,\n",
    "        'recall': valid_recalls,\n",
    "        'f1_scores': [f1_score(y_valid, (y_valid_proba >= t).astype(int), zero_division=0) for t in thresholds],\n",
    "        'y_proba': y_valid_proba,\n",
    "        'balanced_threshold': valid_balanced_threshold,\n",
    "        'final_metrics': calculate_final_metrics(y_valid, y_valid_proba, optimal_threshold, \"Valid\")\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        'train': train_metrics,\n",
    "        'valid': valid_metrics,\n",
    "        'optimal_threshold': optimal_threshold\n",
    "    }\n",
    "    \n",
    "    if X_test is not None and y_test is not None:\n",
    "        test_balanced_threshold, test_precisions, test_recalls = find_balanced_threshold(y_test, y_test_proba, thresholds)\n",
    "        test_metrics = {\n",
    "            'thresholds': thresholds,\n",
    "            'precision': test_precisions,\n",
    "            'recall': test_recalls,\n",
    "            'f1_scores': [f1_score(y_test, (y_test_proba >= t).astype(int), zero_division=0) for t in thresholds],\n",
    "            'y_proba': y_test_proba,\n",
    "            'balanced_threshold': test_balanced_threshold,\n",
    "            'final_metrics': calculate_final_metrics(y_test, y_test_proba, optimal_threshold, \"Test\")\n",
    "        }\n",
    "        results['test'] = test_metrics\n",
    "    \n",
    "    # 9. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –æ–±–ª–∞—Å—Ç—å –≤–æ–∫—Ä—É–≥ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    \n",
    "    # 1. –ö—Ä–∏–≤—ã–µ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_metrics['thresholds'], train_metrics['precision'], label='Precision', color='blue')\n",
    "    plt.plot(train_metrics['thresholds'], train_metrics['recall'], label='Recall', color='green')\n",
    "    plt.plot(train_metrics['thresholds'], train_metrics['f1_scores'], label='F1', color='red')\n",
    "    plt.axvline(optimal_threshold, color='k', linestyle='-', label=f'Avg Optimal: {optimal_threshold:.3f}')\n",
    "    plt.axvline(train_balanced_threshold, color='b', linestyle=':', label=f'Train Balanced: {train_balanced_threshold:.3f}')\n",
    "    plt.title('Train Selection')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    # 2. –ö—Ä–∏–≤—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(valid_metrics['thresholds'], valid_metrics['precision'], label='Precision', color='blue')\n",
    "    plt.plot(valid_metrics['thresholds'], valid_metrics['recall'], label='Recall', color='green')\n",
    "    plt.plot(valid_metrics['thresholds'], valid_metrics['f1_scores'], label='F1', color='red')\n",
    "    plt.axvline(optimal_threshold, color='k', linestyle='-', label=f'Avg Optimal: {optimal_threshold:.3f}')\n",
    "    plt.axvline(valid_balanced_threshold, color='orange', linestyle=':', label=f'Valid Balanced: {valid_balanced_threshold:.3f}')\n",
    "    plt.title('Validation Set')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    # 3. Zoom –Ω–∞ –æ–±–ª–∞—Å—Ç—å –≤–æ–∫—Ä—É–≥ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "    plt.subplot(1, 3, 3)\n",
    "    zoom_range = 0.2  # +/- 20% –æ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n",
    "    zoom_min = max(0.01, optimal_threshold - zoom_range)\n",
    "    zoom_max = min(0.99, optimal_threshold + zoom_range)\n",
    "    \n",
    "    mask = (thresholds >= zoom_min) & (thresholds <= zoom_max)\n",
    "    \n",
    "    plt.plot(thresholds[mask], np.array(train_metrics['precision'])[mask], label='Train Precision', color='blue', linestyle='--')\n",
    "    plt.plot(thresholds[mask], np.array(train_metrics['recall'])[mask], label='Train Recall', color='green', linestyle='--')\n",
    "    plt.plot(thresholds[mask], np.array(valid_metrics['precision'])[mask], label='Valid Precision', color='blue')\n",
    "    plt.plot(thresholds[mask], np.array(valid_metrics['recall'])[mask], label='Valid Recall', color='green')\n",
    "    \n",
    "    plt.axvline(optimal_threshold, color='k', linestyle='-', label=f'Optimal: {optimal_threshold:.3f}')\n",
    "    plt.title(f'Zoom Around Optimal Threshold ({zoom_min:.2f}-{zoom_max:.2f})')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'metrics': {\n",
    "            'train': train_metrics['final_metrics'],\n",
    "            'valid': valid_metrics['final_metrics'],\n",
    "            'test': results['test']['final_metrics'] if 'test' in results else None,\n",
    "            'optimal_threshold': optimal_threshold\n",
    "        },\n",
    "        'features': list(X_train.columns) if hasattr(X_train, 'columns') else None,\n",
    "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20708ffc-8e3c-4539-947e-e5d865e82e3a",
   "metadata": {},
   "source": [
    "**–¢–æ—á–µ—á–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f64be1-8dde-4fa0-9eb7-127665e37713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_pairplot(df, features_list):\n",
    "    \"\"\"\n",
    "    –°—Ç—Ä–æ–∏—Ç pairplot –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π 'target'\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "    features_list : list\n",
    "        –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (–Ω–µ –≤–∫–ª—é—á–∞—è 'target')\n",
    "    \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "    -----------\n",
    "    None (–æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫)\n",
    "    \"\"\"\n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º target –∫ —Å–ø–∏—Å–∫—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    columns_to_plot = features_list + ['target']\n",
    "    \n",
    "    try:\n",
    "        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –∏ –≤—ã–±–∏—Ä–∞–µ–º –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
    "        subset_df = df[columns_to_plot].dropna()\n",
    "        \n",
    "        # –°—Ç—Ä–æ–∏–º pairplot\n",
    "        sns.pairplot(subset_df, hue='target', diag_kind='kde', palette='viridis')\n",
    "        plt.suptitle('Pairplot: selected features vs target', y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞: –≤ DataFrame –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "# plot_feature_pairplot(df, ['senkou_span_a_norm', 'senkou_span_b_norm', 'breakout_in_5', 'atr_14_norm%', 'EFI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384dafc2-5c7f-4242-955e-7faa4df7b959",
   "metadata": {},
   "source": [
    "**–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45db092a-8ec3-43f5-8eca-21c238ce746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_correlation_matrix(df, drop_columns=['Data', 'High', 'Low', 'Close', 'Open', 'Volume']):\n",
    "    \"\"\"\n",
    "    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç—å—é –¥–ª—è –±–æ–ª—å—à–æ–≥–æ —á–∏—Å–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
    "        data = df.drop(drop_columns, axis=1, errors='ignore')\n",
    "        \n",
    "        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É\n",
    "        corr_matrix = data.corr()\n",
    "        num_features = len(corr_matrix)\n",
    "        \n",
    "        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "        if num_features <= 15:\n",
    "            figsize = (10, 8)\n",
    "            font_scale = 1.2\n",
    "            annot = True\n",
    "            label_size = 10\n",
    "        elif num_features <= 30:\n",
    "            figsize = (16, 14)\n",
    "            font_scale = 1.0\n",
    "            annot = False\n",
    "            label_size = 9\n",
    "        else:  # –î–ª—è 50+ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "            figsize = (20, 18)\n",
    "            font_scale = 0.8\n",
    "            annot = False\n",
    "            label_size = 8\n",
    "            # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–≥–æ —á–∏—Å–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å –ø–ª–æ—Ç–Ω–æ—Å—Ç—å –º–µ—Ç–æ–∫\n",
    "            plt.rcParams['xtick.major.pad'] = 0.5\n",
    "            plt.rcParams['ytick.major.pad'] = 0.5\n",
    "        \n",
    "        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è\n",
    "        sns.set(font_scale=font_scale)\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏\n",
    "        heatmap = sns.heatmap(\n",
    "            corr_matrix,\n",
    "            cmap='coolwarm',\n",
    "            annot=annot,\n",
    "            fmt=\".2f\",\n",
    "            square=True,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.7},\n",
    "            mask=np.triu(np.ones_like(corr_matrix, dtype=bool)),\n",
    "            annot_kws={\"size\": 8} if annot else None\n",
    "        )\n",
    "        \n",
    "        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–¥–ø–∏—Å–µ–π –æ—Å–µ–π\n",
    "        heatmap.set_xticklabels(\n",
    "            heatmap.get_xticklabels(),\n",
    "            rotation=45,\n",
    "            ha='right',\n",
    "            fontsize=label_size\n",
    "        )\n",
    "        heatmap.set_yticklabels(\n",
    "            heatmap.get_yticklabels(),\n",
    "            rotation=0,\n",
    "            fontsize=label_size\n",
    "        )\n",
    "        \n",
    "        plt.title(f'–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ ({num_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {str(e)}\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "# plot_correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048e95a-d19e-4ce1-8872-de7af6f388bb",
   "metadata": {},
   "source": [
    "**–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Ç–æ–ø 20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61fedce2-17d0-4b6d-a3c5-c27630a47bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_correlated_pairs(df, top_n=20):\n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π\n",
    "    corr_matrix = df.corr().abs()  # –ë–µ—Ä–µ–º –º–æ–¥—É–ª—å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ä (–±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è A:B –∏ B:A)\n",
    "    pairs = []\n",
    "    cols = corr_matrix.columns\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):  # –ò—Å–∫–ª—é—á–∞–µ–º –¥–∏–∞–≥–æ–Ω–∞–ª—å –∏ –∑–µ—Ä–∫–∞–ª—å–Ω—ã–µ –ø–∞—Ä—ã\n",
    "            pairs.append((cols[i], cols[j], corr_matrix.iloc[i, j]))\n",
    "    \n",
    "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ä—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏\n",
    "    pairs_sorted = sorted(pairs, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º —Ç–æ–ø-N –ø–∞—Ä\n",
    "    print(f\"–¢–æ–ø-{top_n} –ø–∞—Ä –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏:\")\n",
    "    for pair in pairs_sorted[:top_n]:\n",
    "        print(f\"{pair[0]} : {pair[1]} : {pair[2]:.4f}\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "# get_top_correlated_pairs(df, top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0b8e62-7078-4022-a845-b39e7fe4ec2f",
   "metadata": {},
   "source": [
    "**SHAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60e8a525-9255-464c-abb5-483ed130da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_model_shap(X_train, model, sample_size=2000, top_n=20, n_jobs = -1):\n",
    "    \"\"\"\n",
    "    –û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç —Ä–∞—Å—á–µ—Ç SHAP-–≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    X_train : pd.DataFrame\n",
    "        –î–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    model : sklearn/xgboost –º–æ–¥–µ–ª—å\n",
    "        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (RandomForest, LogisticRegression, XGB –∏ –¥—Ä.)\n",
    "    sample_size : int\n",
    "        –†–∞–∑–º–µ—Ä —Å–ª—É—á–∞–π–Ω–æ–π –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏\n",
    "    top_n : int\n",
    "        –ö–æ–ª-–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_start_time = time.time()\n",
    "        model_type = type(model).__name__\n",
    "        \n",
    "        print(f\"‚ÑπÔ∏è Model type: {model_type}\")\n",
    "        print(f\"‚ÑπÔ∏è Number of classes: {getattr(model, 'n_classes_', 'unknown')}\")\n",
    "        \n",
    "        # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Explainer\n",
    "        print(\"üîÑ Initializing SHAP explainer...\")\n",
    "        explainer_start = time.time()\n",
    "        if model_type in ['RandomForestClassifier', 'RandomForestRegressor', \n",
    "                          'XGBClassifier', 'XGBRegressor', \n",
    "                          'LGBMClassifier', 'LGBMRegressor']:\n",
    "            explainer = shap.TreeExplainer(model, feature_perturbation=\"tree_path_dependent\")\n",
    "        elif model_type in ['LogisticRegression', 'LinearRegression']:\n",
    "            explainer = shap.LinearExplainer(model, X_train)\n",
    "        else:\n",
    "            explainer = shap.Explainer(model, X_train)\n",
    "        explainer_time = time.time() - explainer_start\n",
    "        print(f\"‚úÖ SHAP explainer initialized in {timedelta(seconds=explainer_time)}\")\n",
    "        \n",
    "        # 2. –ü–æ–¥–≤—ã–±–æ—Ä–∫–∞\n",
    "        sample_size = min(sample_size, len(X_train))\n",
    "        sample_idx = np.random.choice(X_train.index, size=sample_size, replace=False)\n",
    "        X_sample = X_train.loc[sample_idx]\n",
    "\n",
    "        print(f\"\\nüîÑ Calculating SHAP values for {sample_size} samples...\")\n",
    "        shap_start = time.time()\n",
    "\n",
    "        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "        n_jobs = n_jobs\n",
    "        n_chunks = 4 * (os.cpu_count() or 1)\n",
    "\n",
    "        def calc_chunk(chunk):\n",
    "            return explainer.shap_values(chunk, approximate=True, check_additivity=False)\n",
    "\n",
    "        chunks = np.array_split(X_sample, n_chunks)\n",
    "        results = Parallel(n_jobs=n_jobs)(delayed(calc_chunk)(chunk) for chunk in chunks)\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        if isinstance(results[0], list):\n",
    "            shap_values = [np.concatenate([r[i] for r in results]) for i in range(len(results[0]))]\n",
    "        else:\n",
    "            shap_values = np.concatenate(results)\n",
    "\n",
    "        shap_time = time.time() - shap_start\n",
    "        print(f\"‚úÖ SHAP values calculated in {timedelta(seconds=shap_time)}\")\n",
    "        print(f\"‚è± Average time per sample: {shap_time/sample_size:.4f} seconds\")\n",
    "\n",
    "        # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ SHAP\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1] if len(shap_values) == 2 else np.mean(shap_values, axis=0)\n",
    "        elif isinstance(shap_values, np.ndarray) and shap_values.ndim == 3:\n",
    "            shap_values = shap_values[:, :, 1]\n",
    "\n",
    "        print(f\"‚ÑπÔ∏è Processed SHAP values shape: {shap_values.shape}\")\n",
    "\n",
    "        # 4. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏\n",
    "        print(\"\\nüîÑ Calculating feature importance...\")\n",
    "        analysis_start = time.time()\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'SHAP_Importance': np.abs(shap_values).mean(axis=0),\n",
    "            'Direction': np.where(np.mean(shap_values, axis=0) > 0, 'Positive', 'Negative')\n",
    "        })\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df['Model_Importance'] = model.feature_importances_\n",
    "            importance_df['Model_%'] = 100 * importance_df['Model_Importance'] / importance_df['Model_Importance'].max()\n",
    "\n",
    "        importance_df['SHAP_%'] = 100 * importance_df['SHAP_Importance'] / importance_df['SHAP_Importance'].max()\n",
    "        importance_df = importance_df.sort_values('SHAP_%', ascending=False)\n",
    "        importance_df['Rank'] = range(1, len(importance_df) + 1)\n",
    "        importance_df['Cumulative_SHAP_%'] = importance_df['SHAP_%'].cumsum()\n",
    "        analysis_time = time.time() - analysis_start\n",
    "        print(f\"‚úÖ Feature analysis completed in {timedelta(seconds=analysis_time)}\")\n",
    "\n",
    "        # 5. –¢–∞–±–ª–∏—Ü–∞\n",
    "        print(\"\\nüîç Top Features by SHAP Importance:\")\n",
    "        display_cols = ['Rank', 'Feature', 'SHAP_%', 'Direction']\n",
    "        if 'Model_%' in importance_df.columns:\n",
    "            display_cols.append('Model_%')\n",
    "        print(importance_df.head(top_n)[display_cols].to_markdown(index=False, floatfmt=\".1f\"))\n",
    "\n",
    "        print(\"\\nüìä Key Metrics:\")\n",
    "        print(f\"‚Ä¢ Top-5 features explain: {importance_df['Cumulative_SHAP_%'].iloc[4]:.1f}%\")\n",
    "        pos_count = (importance_df['Direction'] == 'Positive').sum()\n",
    "        neg_count = (importance_df['Direction'] == 'Negative').sum()\n",
    "        print(f\"‚Ä¢ Positive/Negative: {pos_count}/{neg_count}\")\n",
    "\n",
    "        # 6. –ü—Ä–æ—Å—Ç–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        plt.figure(figsize=(10, min(6, top_n * 0.3)))\n",
    "        colors = importance_df['Direction'].head(top_n).map({'Positive': 'tomato', 'Negative': 'dodgerblue'})\n",
    "        plt.barh(importance_df['Feature'].head(top_n)[::-1], \n",
    "                 importance_df['SHAP_%'].head(top_n)[::-1],\n",
    "                 color=colors[::-1])\n",
    "        plt.title(f'Top {top_n} Features by SHAP')\n",
    "        plt.xlabel('Relative SHAP Importance (%)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 7. –û–±—â–µ–µ –≤—Ä–µ–º—è\n",
    "        total_time = time.time() - total_start_time\n",
    "        print(f\"\\n‚è± Total execution time: {timedelta(seconds=total_time)}\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"Time breakdown:\")\n",
    "        print(f\"- Explainer init: {timedelta(seconds=explainer_time)}\")\n",
    "        print(f\"- SHAP values: {timedelta(seconds=shap_time)} ({shap_time/total_time*100:.1f}%)\")\n",
    "        print(f\"- Analysis: {timedelta(seconds=analysis_time)} ({analysis_time/total_time*100:.1f}%)\")\n",
    "\n",
    "        return importance_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "        if 'shap_values' in locals():\n",
    "            print(f\"SHAP values type: {type(shap_values)}\")\n",
    "            if hasattr(shap_values, 'shape'):\n",
    "                print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "        print(f\"X_train shape: {X_train.shape if X_train is not None else 'N/A'}\")\n",
    "        if hasattr(model, 'n_features_in_'):\n",
    "            print(f\"Model features: {model.n_features_in_}\")\n",
    "        return None\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "# explain_model_shap(X_train, logreg_model)\n",
    "# explain_model_shap(X_train, rf_model, sample_size=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48525a07-ac73-4a84-9020-140d2e470046",
   "metadata": {},
   "source": [
    "**Permutation importance (—Ä–∞—Å—á–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏ –ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∏)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b969d1e1-109a-44c8-bda1-4d4448005940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_model_permutation(X, y, model, scoring='f1', n_repeats=5, top_n=20, random_state=3):\n",
    "    \"\"\"\n",
    "    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é Permutation Importance.\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        –ü—Ä–∏–∑–Ω–∞–∫–∏ (X_train –∏–ª–∏ X_valid)\n",
    "    y : pd.Series\n",
    "        –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è\n",
    "    model : –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "        RandomForest, LogisticRegression, XGBoost –∏ —Ç.–¥.\n",
    "    scoring : str\n",
    "        –ú–µ—Ç—Ä–∏–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'f1', 'accuracy', 'roc_auc')\n",
    "    n_repeats : int\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏\n",
    "    top_n : int\n",
    "        –ö–æ–ª-–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    random_state : int\n",
    "        –°–ª—É—á–∞–π–Ω–æ–µ –∑–µ—Ä–Ω–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "    \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "    -----------\n",
    "    pd.DataFrame ‚Äî —Ç–∞–±–ª–∏—Ü–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"‚ÑπÔ∏è Model type: {type(model).__name__}\")\n",
    "        print(f\"‚ÑπÔ∏è Scoring metric: {scoring}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤\n",
    "        n_jobs = os.cpu_count() - 1 if os.cpu_count() else 1\n",
    "\n",
    "        print(\"üîÑ Calculating permutation importance...\")\n",
    "        result = permutation_importance(\n",
    "            model, X, y,\n",
    "            scoring=scoring,\n",
    "            n_repeats=n_repeats,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úÖ Completed in {timedelta(seconds=elapsed)}\")\n",
    "\n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º\n",
    "        importances_df = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Mean Importance': result.importances_mean,\n",
    "            'Std': result.importances_std\n",
    "        })\n",
    "        importances_df['Significant'] = importances_df['Mean Importance'] - 2 * importances_df['Std'] > 0\n",
    "        importances_df = importances_df.sort_values(by='Mean Importance', ascending=False).reset_index(drop=True)\n",
    "        importances_df['Rank'] = importances_df.index + 1\n",
    "\n",
    "        print(\"\\nüîç Top Features by Permutation Importance:\")\n",
    "        display_cols = ['Rank', 'Feature', 'Mean Importance', 'Std', 'Significant']\n",
    "        print(importances_df.head(top_n)[display_cols].to_markdown(index=False, floatfmt=\".3f\"))\n",
    "\n",
    "        # –ü—Ä–æ—Å—Ç–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        top_features = importances_df.head(top_n)\n",
    "        plt.figure(figsize=(10, min(6, top_n * 0.3)))\n",
    "        bars = plt.barh(top_features['Feature'][::-1], top_features['Mean Importance'][::-1],\n",
    "                        xerr=top_features['Std'][::-1], color='mediumseagreen')\n",
    "        plt.xlabel(\"Mean Importance\")\n",
    "        plt.title(f\"Top {top_n} Features by Permutation Importance\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return importances_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during permutation importance: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b3f09c-bee4-41fe-87a0-ee404c2d85ae",
   "metadata": {},
   "source": [
    "**RFECV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e8b32e6-de77-4402-b9c3-4826dd4f123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rfecv_results(X_train, y_train, estimator, \n",
    "                      scoring='f1', step=1, n_splits=3, \n",
    "                      n_jobs=-1, verbose=0):\n",
    "    \"\"\"\n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é RFECV –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
    "    - —Å–ø–∏—Å–æ–∫ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    - –≥—Ä–∞—Ñ–∏–∫ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    X_train : pd.DataFrame –∏–ª–∏ array-like\n",
    "        –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    y_train : pd.Series –∏–ª–∏ array-like\n",
    "        –í–µ–∫—Ç–æ—Ä —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
    "    estimator : –æ–±—ä–µ–∫—Ç –º–æ–¥–µ–ª–∏\n",
    "        –£–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, RandomForestClassifier())\n",
    "    scoring : str, default='f1'\n",
    "        –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
    "    step : int, default=1\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—è–µ–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏\n",
    "    n_splits : int, default=3\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è TimeSeriesSplit\n",
    "    n_jobs : int, default=-1\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–¥–µ—Ä –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
    "    verbose : int, default=0\n",
    "        –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã–≤–æ–¥–∞\n",
    "    \"\"\"\n",
    "    \n",
    "    # TimeSeries split\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    # RFECV —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é –∏ TSCV\n",
    "    rfecv_selector = RFECV(\n",
    "        estimator=estimator,\n",
    "        step=step,\n",
    "        cv=tscv,\n",
    "        scoring=scoring,\n",
    "        verbose=verbose,\n",
    "        n_jobs=n_jobs\n",
    "    )\n",
    "    \n",
    "    # –ö–æ–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π\n",
    "    X_rfecv = X_train.copy() if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    y_rfecv = y_train.copy()\n",
    "    \n",
    "    # –í—ã–ø–æ–ª–Ω—è–µ–º –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    rfecv_selector.fit(X_rfecv, y_rfecv)\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        rfecv_features = X_train.columns[rfecv_selector.support_].tolist()\n",
    "    else:\n",
    "        rfecv_features = [f\"feature_{i}\" for i, selected in enumerate(rfecv_selector.support_) if selected]\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    print(\"\\n‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏, –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ RFECV:\")\n",
    "    print(rfecv_features)\n",
    "    \n",
    "    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(rfecv_selector.cv_results_['mean_test_score']) + 1),\n",
    "             rfecv_selector.cv_results_['mean_test_score'])\n",
    "    plt.xlabel(\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
    "    plt.ylabel(f\"{scoring} score\")\n",
    "    plt.title(f\"–ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ ({estimator.__class__.__name__}) –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —á–∏—Å–ª–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (RFECV)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8dda2-0527-400d-8f8d-ced1e8c7cf8a",
   "metadata": {},
   "source": [
    "**Boruta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c84285-9dad-4e4d-aaa3-04406f72946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_model_boruta(\n",
    "    X_train, \n",
    "    X_valid, \n",
    "    y_train, \n",
    "    y_valid, \n",
    "    model, \n",
    "    max_iter=100, \n",
    "    n_splits=3, \n",
    "    random_state=3,\n",
    "    perc=100,\n",
    "    alpha=0.05,\n",
    "    two_step=True,\n",
    "    n_estimators='auto',\n",
    "    verbose=0\n",
    "):\n",
    "    \"\"\"\n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é Boruta –∏ TimeSeriesSplit.\n",
    "\n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    X_train : pd.DataFrame\n",
    "        –ü—Ä–∏–∑–Ω–∞–∫–∏ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    X_valid : pd.DataFrame\n",
    "        –ü—Ä–∏–∑–Ω–∞–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –≤–∫–ª—é—á–µ–Ω –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è)\n",
    "    y_train : pd.Series\n",
    "        –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    y_valid : pd.Series\n",
    "        –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)\n",
    "    model : –æ–±—ä–µ–∫—Ç\n",
    "        –û–±—É—á–∞–µ–º–∞—è –º–æ–¥–µ–ª—å (RandomForest, XGBoost, LogisticRegression –∏ –¥—Ä.)\n",
    "    max_iter : int, default=100\n",
    "        –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π Boruta (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º —Ç—â–∞—Ç–µ–ª—å–Ω–µ–µ –æ—Ç–±–æ—Ä,\n",
    "        –Ω–æ –¥–æ–ª—å—à–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ)\n",
    "    n_splits : int, default=3\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–±–∏–µ–Ω–∏–π –≤ TimeSeriesSplit (–¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤)\n",
    "    random_state : int, default=3\n",
    "        –ó–µ—Ä–Ω–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "    perc : int, default=100\n",
    "        –ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —à—É–º–æ–º (–º–µ–Ω—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–µ–ª–∞—é—Ç\n",
    "        –æ—Ç–±–æ—Ä –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–º)\n",
    "    alpha : float, default=0.05\n",
    "        –£—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –¥–ª—è –æ—Ç–±—Ä–∞–∫–æ–≤–∫–∏ –≥–∏–ø–æ—Ç–µ–∑ (–º–µ–Ω—å—à–µ = —Å—Ç—Ä–æ–∂–µ –æ—Ç–±–æ—Ä)\n",
    "    two_step : bool, default=True\n",
    "        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–≤—É—Ö—ç—Ç–∞–ø–Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É (True —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö)\n",
    "    n_estimators : int –∏–ª–∏ 'auto', default='auto'\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –∞–Ω—Å–∞–º–±–ª–µ (–µ—Å–ª–∏ 'auto', –±–µ—Ä–µ—Ç—Å—è –∏–∑ –º–æ–¥–µ–ª–∏)\n",
    "    verbose : int, default=0\n",
    "        –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—ã–≤–æ–¥–∞ (0 - –Ω–µ—Ç –≤—ã–≤–æ–¥–∞, 1 - –±–∞–∑–æ–≤—ã–π, 2 - –ø–æ–¥—Ä–æ–±–Ω—ã–π)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"‚ÑπÔ∏è Model type: {type(model).__name__}\")\n",
    "        print(f\"‚ÑπÔ∏è Boruta params: max_iter={max_iter}, perc={perc}, alpha={alpha}\")\n",
    "        print(f\"‚ÑπÔ∏è TimeSeriesSplit n_splits: {n_splits}\")\n",
    "\n",
    "        X_df = X_train.copy()\n",
    "        X_array, y_array = X_df.values, y_train.values\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "        boruta = BorutaPy(\n",
    "            estimator=model,\n",
    "            n_estimators=n_estimators,\n",
    "            verbose=verbose,\n",
    "            random_state=random_state,\n",
    "            max_iter=max_iter,\n",
    "            perc=perc,\n",
    "            alpha=alpha,\n",
    "            two_step=two_step\n",
    "        )\n",
    "\n",
    "        print(\"üîÑ Running Boruta feature selection on time series splits...\")\n",
    "\n",
    "        support_masks = []\n",
    "        for i, (train_idx, test_idx) in enumerate(tscv.split(X_array)):\n",
    "            print(f\"  ‚Ä¢ Fold {i+1}/{n_splits} ‚Äî Train size: {len(train_idx)}, Test size: {len(test_idx)}\")\n",
    "            X_fold_train, y_fold_train = X_array[train_idx], y_array[train_idx]\n",
    "            boruta.fit(X_fold_train, y_fold_train)\n",
    "            support_masks.append(boruta.support_.copy())\n",
    "\n",
    "        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –º–∞—Å–æ–∫ –ø–æ –≤—Å–µ–º —Ñ–æ–ª–¥–∞–º\n",
    "        final_support = np.all(support_masks, axis=0)\n",
    "        selected_features = X_df.columns[final_support].tolist()\n",
    "\n",
    "        print(f\"\\n‚úÖ –ò—Ç–æ–≥–æ–≤—ã–µ –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(selected_features)}):\")\n",
    "        for i, feat in enumerate(selected_features, 1):\n",
    "            print(f\"{i:>2}. {feat}\")\n",
    "\n",
    "        return selected_features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ Boruta: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcddf16a-3c53-429e-8b0c-c3e7522d8ff2",
   "metadata": {},
   "source": [
    "**Mutual Information (–≤–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0f9d2c64-90cd-40ab-81d0-d6ab45b5cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_model_mutual_info(X_train, y_train, top_n=20, random_state=3):\n",
    "    \"\"\"\n",
    "    –†–∞—Å—á—ë—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ Mutual Information.\n",
    "\n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    X_train : pd.DataFrame\n",
    "        –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    y_train : pd.Series\n",
    "        –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è\n",
    "    top_n : int\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    random_state : int\n",
    "        –ó–µ—Ä–Ω–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª –¥–ª—è MI-–æ—Ü–µ–Ω–∫–∏\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        print(f\"‚ÑπÔ∏è Calculating Mutual Information for {X_train.shape[1]} features...\")\n",
    "\n",
    "        # 1. –†–∞—Å—á—ë—Ç MI\n",
    "        mi_scores = mutual_info_classif(X_train, y_train, random_state=random_state)\n",
    "        mi_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'MI_Score': mi_scores\n",
    "        }).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"‚úÖ MI calculation completed in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # 2. –¢–∞–±–ª–∏—Ü–∞ —Ç–æ–ø-N\n",
    "        print(f\"\\nüîç Top {top_n} Features by Mutual Information:\")\n",
    "        print(mi_df.head(top_n).to_markdown(index=False, floatfmt=\".4f\"))\n",
    "\n",
    "        # 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        plt.figure(figsize=(10, min(6, top_n * 0.3)))\n",
    "        plt.barh(mi_df['Feature'].head(top_n)[::-1], \n",
    "                 mi_df['MI_Score'].head(top_n)[::-1], \n",
    "                 color='skyblue')\n",
    "        plt.xlabel('Mutual Information Score')\n",
    "        plt.title(f'Top {top_n} Features by Mutual Information')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á—ë—Ç–µ Mutual Information: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c40c400-0e00-4347-bac5-b25441123017",
   "metadata": {},
   "source": [
    "**Granger Causality (–ø—Ä–∏—á–∏–Ω–Ω–æ—Å—Ç—å –ø–æ –ì—Ä–µ–π–Ω–¥–∂–µ—Ä—É)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d06b104-c1f2-4424-a1a7-8a39ab764afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_model_granger(X_train, y_train, target_name='target', max_lag=5, top_n=20):\n",
    "    \"\"\"\n",
    "    –ê–Ω–∞–ª–∏–∑ Granger Causality –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π.\n",
    "\n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    X_train : pd.DataFrame\n",
    "        –ü—Ä–∏–∑–Ω–∞–∫–∏ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "    y_train : pd.Series\n",
    "        –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è\n",
    "    target_name : str\n",
    "        –ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–¥–ª—è –ø–æ–¥–ø–∏—Å–∏)\n",
    "    max_lag : int\n",
    "        –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞–≥–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞ –ì—Ä–µ–π–Ω–¥–∂–µ—Ä–∞\n",
    "    top_n : int\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        print(f\"‚ÑπÔ∏è Calculating Granger causality for {X_train.shape[1]} features...\")\n",
    "\n",
    "        def check_granger_causality(feature_series, target_series, max_lag=5):\n",
    "            data = pd.DataFrame({\n",
    "                'target': target_series,\n",
    "                'feature': feature_series\n",
    "            }).dropna()\n",
    "            try:\n",
    "                test_result = grangercausalitytests(data[['target', 'feature']], maxlag=max_lag, verbose=False)\n",
    "                p_values = [test_result[i+1][0]['ssr_chi2test'][1] for i in range(max_lag)]\n",
    "                return np.min(p_values)\n",
    "            except:\n",
    "                return np.nan  # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–µ—Ä–Ω—É—Ç—å NaN\n",
    "\n",
    "        granger_results = {\n",
    "            feature: check_granger_causality(X_train[feature], y_train)\n",
    "            for feature in X_train.columns\n",
    "        }\n",
    "\n",
    "        granger_df = pd.DataFrame({\n",
    "            'Feature': list(granger_results.keys()),\n",
    "            'Granger_p_value': list(granger_results.values())\n",
    "        }).dropna().sort_values('Granger_p_value')\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Granger analysis completed in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # –í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü—ã\n",
    "        print(f\"\\nüîç Top {top_n} Features by Granger Causality (lowest p-values):\")\n",
    "        print(granger_df.head(top_n).to_markdown(index=False, floatfmt=\".4e\"))\n",
    "\n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "        plt.figure(figsize=(10, min(6, top_n * 0.3)))\n",
    "        plt.barh(granger_df['Feature'].head(top_n)[::-1],\n",
    "                 -np.log10(granger_df['Granger_p_value'].head(top_n)[::-1]),\n",
    "                 color='salmon')\n",
    "        plt.xlabel(r'$-\\log_{10}$(p-value)')\n",
    "        plt.title(f'Top {top_n} Features by Granger Causality vs \"{target_name}\"')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ Granger Causality: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d6b46-7930-4b3c-aab4-75ae4b51577d",
   "metadata": {},
   "source": [
    "**–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å F1 –º–æ–¥–µ–ª–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–µ—Ä–µ–≤—å–µ–≤**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8cd29fb-819a-409d-a5e4-fd732306658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_n(n, base_model, cv_splits, X_train_arr, y_train_arr):\n",
    "    \"\"\"–ì–ª–æ–±–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º n_estimators.\"\"\"\n",
    "    model = clone(base_model).set_params(\n",
    "        n_estimators=n,\n",
    "        warm_start=True,  # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "        verbose=0\n",
    "    )\n",
    "    scores = []\n",
    "    for train_idx, test_idx in cv_splits:\n",
    "        X_train_fold = X_train_arr[train_idx]\n",
    "        X_test_fold = X_train_arr[test_idx]\n",
    "        y_train_fold = y_train_arr[train_idx]\n",
    "        y_test_fold = y_train_arr[test_idx]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        pred = model.predict(X_test_fold)\n",
    "        scores.append(f1_score(y_test_fold, pred))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def plot_f1_vs_n_estimators(X_train, y_train, base_model, \n",
    "                          n_estimators_range=(100, 1000), step=50, \n",
    "                          n_splits=3, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º joblib.Parallel.\n",
    "    \"\"\"\n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    if not hasattr(base_model, 'fit') or not hasattr(base_model, 'predict'):\n",
    "        raise ValueError(\"base_model –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–æ–¥–µ–ª—å—é scikit-learn —Å –º–µ—Ç–æ–¥–∞–º–∏ fit –∏ predict\")\n",
    "    \n",
    "    n_estimators_values = np.arange(\n",
    "        n_estimators_range[0], \n",
    "        n_estimators_range[1] + 1, \n",
    "        step\n",
    "    )\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_splits = list(tscv.split(X_train))\n",
    "    \n",
    "    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ numpy array –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "    X_train_arr = X_train.values if hasattr(X_train, 'iloc') else X_train\n",
    "    y_train_arr = y_train.values if hasattr(y_train, 'iloc') else y_train\n",
    "    \n",
    "    # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        \n",
    "        print(f\"‚è≥ –ê–Ω–∞–ª–∏–∑ {len(n_estimators_values)} –∑–Ω–∞—á–µ–Ω–∏–π n_estimators...\")\n",
    "        \n",
    "        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ joblib\n",
    "        f1_scores = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
    "            delayed(evaluate_n)(\n",
    "                n, base_model, cv_splits, X_train_arr, y_train_arr\n",
    "            ) for n in tqdm(n_estimators_values, desc=\"n_estimators\")\n",
    "        )\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è NaN —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        valid_mask = ~np.isnan(f1_scores)\n",
    "        n_estimators_values = n_estimators_values[valid_mask]\n",
    "        f1_scores = np.array(f1_scores)[valid_mask]\n",
    "        \n",
    "        if not len(f1_scores):\n",
    "            print(\"‚ö†Ô∏è –í—Å–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å —Å –æ—à–∏–±–∫–æ–π!\")\n",
    "            return\n",
    "        \n",
    "        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(n_estimators_values, f1_scores, 'b-o', alpha=0.7)\n",
    "        plt.title(f'–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å F1 –æ—Ç n_estimators ({base_model.__class__.__name__})')\n",
    "        plt.xlabel('n_estimators')\n",
    "        plt.ylabel('F1-score (CV —Å—Ä–µ–¥–Ω–µ–µ)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        \n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        plt.scatter(\n",
    "            n_estimators_values[best_idx], f1_scores[best_idx],\n",
    "            color='red', s=150, zorder=5,\n",
    "            label=f'–õ—É—á—à–µ–µ: {f1_scores[best_idx]:.4f} (n={n_estimators_values[best_idx]})'\n",
    "        )\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {n_estimators_values[best_idx]} —Å F1={f1_scores[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e3bec-a32f-4f4b-8282-606158788da6",
   "metadata": {},
   "source": [
    "**–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤–Ω—É—Ç—Ä–∏ –≤—ã–±–æ—Ä–æ–∫**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191b7a75-79b0-46ef-bfbd-3e580f677b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_class_balance(y, y_train, y_valid, y_test):\n",
    "    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—É\n",
    "    balance_df = pd.DataFrame({\n",
    "        '–í–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç': y.value_counts(normalize=True).round(3),\n",
    "        '–û–±—É—á–∞—é—â–∞—è': y_train.value_counts(normalize=True).round(3),\n",
    "        '–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è': y_valid.value_counts(normalize=True).round(3),\n",
    "        '–¢–µ—Å—Ç–æ–≤–∞—è': y_test.value_counts(normalize=True).round(3)\n",
    "    }).fillna(0)  # –Ω–∞ —Å–ª—É—á–∞–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–ª–∞—Å—Å–æ–≤\n",
    "    \n",
    "    # –í—ã–≤–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É –≤ —Å—Ç–∏–ª–µ \"plain\"\n",
    "    print(\"üìä –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ (–¥–æ–ª–∏):\")\n",
    "    print(\n",
    "        balance_df.to_markdown(\n",
    "            tablefmt=\"simple\",  # –ß–∏—Å—Ç—ã–π —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ –ª–∏—à–Ω–∏—Ö –ª–∏–Ω–∏–π\n",
    "            stralign=\"center\",  # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ —Ü–µ–Ω—Ç—Ä—É\n",
    "            floatfmt=\".3f\"       # –§–æ—Ä–º–∞—Ç —á–∏—Å–µ–ª\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    balance_df.plot(kind='bar', width=0.8, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –ø–æ –≤—ã–±–æ—Ä–∫–∞–º', pad=20)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('–î–æ–ª—è –∫–ª–∞—Å—Å–∞')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    plt.legend(framealpha=0.9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132a029-cdc5-494d-9094-455317c04389",
   "metadata": {},
   "source": [
    "**–ü–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ä–∏—Å–∫ –ø—Ä–∏–±—ã–ª—å**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "585506cf-1d9d-4b64-b03d-87c5b5665bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_parameters(df, X, train_index, valid_index, \n",
    "                       target_candles=20,\n",
    "    \n",
    "                       rr_thresholds=np.arange(1.5, 4.1, 0.5), \n",
    "                       targets=np.arange(0.001, 0.0061, 0.0005)):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # –ü–µ—Ä–µ–±–æ—Ä –≤—Å–µ—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "    for rr in tqdm(rr_thresholds, desc='Processing rr_threshold'):\n",
    "        for target in tqdm(targets, desc='Processing target', leave=False):\n",
    "            # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å —Ç–µ–∫—É—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏\n",
    "            df_temp = df.copy()\n",
    "            df_temp = add_target_column_mod(\n",
    "                df_temp,\n",
    "                target_candles=target_candles,\n",
    "                target=target,\n",
    "                rr_threshold=rr\n",
    "            )\n",
    "            \n",
    "            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞\n",
    "            y = df_temp['target'].values\n",
    "            \n",
    "            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "            \n",
    "            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏—é –µ—Å–ª–∏ –≤ valid –Ω–µ—Ç –æ–±–æ–∏—Ö –∫–ª–∞—Å—Å–æ–≤\n",
    "            if len(np.unique(y_valid)) < 2:\n",
    "                continue\n",
    "                \n",
    "            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "            model = RandomForestClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=13,\n",
    "            max_features=0.8,\n",
    "            min_samples_leaf=6,\n",
    "            min_samples_split=5,\n",
    "            max_samples=0.7,\n",
    "            min_impurity_decrease=0.0002,\n",
    "            class_weight={0: 1, 1: 5},\n",
    "            criterion='entropy',\n",
    "            bootstrap=True,\n",
    "            random_state=3,\n",
    "            n_jobs=-1)\n",
    "            # model = lgb.LGBMClassifier(\n",
    "            # num_leaves=30,\n",
    "            # max_depth=9,\n",
    "            # learning_rate=0.05,\n",
    "            # n_estimators=200,\n",
    "            # min_child_samples=50,\n",
    "            # subsample=0.5,\n",
    "            # colsample_bytree=0.8,\n",
    "            # reg_alpha=0.1,\n",
    "            # reg_lambda=0.1,\n",
    "            # scale_pos_weight=1,\n",
    "            # boosting_type='gbdt',\n",
    "            # importance_type='split',\n",
    "            # random_state=3,\n",
    "            # verbose=-1)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # –ü—Ä–æ–≥–Ω–æ–∑ –∏ —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫\n",
    "            y_pred = model.predict(X_valid)\n",
    "            results.append({\n",
    "                'rr_threshold': rr,\n",
    "                'target': target,\n",
    "                'f1': f1_score(y_valid, y_pred),\n",
    "                'precision': precision_score(y_valid, y_pred),\n",
    "                'recall': recall_score(y_valid, y_pred),\n",
    "                'positive_ratio': np.mean(y_train)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e5f66-82a5-4c08-8050-95720ce868f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
