{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d010138-0fb4-4521-ae35-766b49d81f6e",
   "metadata": {},
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5efa824c-6772-499e-a047-72df37452a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import datetime as dt\n",
    "import time\n",
    "import math\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler, minmax_scale\n",
    "from datetime import datetime\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.special import expit\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import linregress\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import wasserstein_distance  # Альтернатива, если нет POT\n",
    "import ot\n",
    "from numba import njit\n",
    "import swifter\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from pytz import timezone\n",
    "from scipy.signal import savgol_filter\n",
    "from numba import jit\n",
    "from scipy.signal import welch\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pandas_ta.trend import psar\n",
    "from pandas_ta.volatility import atr\n",
    "from pandas_ta.momentum import rsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eac064-e790-421a-8924-419efeb429ad",
   "metadata": {},
   "source": [
    "# Функция расчета RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e54ee2ae-e7e1-4eae-8f6f-79392b3ab8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RSI вручную\n",
    "def compute_rsi(series, period=14, eps=1e-8):\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0.0)\n",
    "    loss = -delta.where(delta < 0, 0.0)\n",
    "    \n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "\n",
    "    rs = avg_gain / (avg_loss + eps)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb7819-422b-4ce0-a95c-eebd1145b0b1",
   "metadata": {},
   "source": [
    "RSI levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc94bc6-d447-4dcc-adbd-c3f949b71d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rsi_level_signal(\n",
    "    df: pd.DataFrame,\n",
    "    rsi_period: int = 14,\n",
    "    lookback_window: int = 200,\n",
    "    tolerance: float = 0.03,\n",
    "    min_touch_count: int = 2,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Находит уровни RSI с помощью кластеризации экстремумов и добавляет бинарный признак:\n",
    "    - `rsi_near_level = 1`, если текущий RSI близко к одному из уровней.\n",
    "\n",
    "    Параметры:\n",
    "        df: DataFrame с ценами (должен содержать столбец 'Close').\n",
    "        rsi_period: период для расчета RSI (по умолчанию 14).\n",
    "        lookback_window: глубина анализа (по умолчанию 200 свечей).\n",
    "        tolerance: допустимое отклонение уровня (относительное, 0.03 = 3%).\n",
    "        min_touch_count: минимальное количество касаний для формирования уровня.\n",
    "\n",
    "    Возвращает:\n",
    "        DataFrame с добавленным столбцом `rsi_near_level` (1 только для последней свечи).\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"DataFrame пуст!\")\n",
    "\n",
    "    df = df.copy()\n",
    "    lookback_window = min(lookback_window, len(df))\n",
    "\n",
    "    # 1. Вычисляем RSI (используем pandas_ta.rsi)\n",
    "    df[\"rsi\"] = ta.rsi(df[\"Close\"], length=rsi_period)  # <-- Исправлено здесь\n",
    "\n",
    "    # 2. Берем последние `lookback_window` значений RSI (игнорируем NaN)\n",
    "    recent_rsi = df[\"rsi\"].dropna().iloc[-lookback_window:]\n",
    "    if len(recent_rsi) < 2:  # Недостаточно данных для анализа\n",
    "        df[\"rsi_near_level\"] = 0\n",
    "        return df\n",
    "\n",
    "    # 3. Находим локальные экстремумы RSI (максимумы и минимумы)\n",
    "    rsi_values = recent_rsi.values\n",
    "    extrema = []\n",
    "    for i in range(1, len(rsi_values) - 1):\n",
    "        if (rsi_values[i] > rsi_values[i - 1]) and (rsi_values[i] > rsi_values[i + 1]):\n",
    "            extrema.append(rsi_values[i])  # Локальный максимум\n",
    "        elif (rsi_values[i] < rsi_values[i - 1]) and (rsi_values[i] < rsi_values[i + 1]):\n",
    "            extrema.append(rsi_values[i])  # Локальный минимум\n",
    "\n",
    "    if not extrema:  # Нет экстремумов → нет уровней\n",
    "        df[\"rsi_near_level\"] = 0\n",
    "        return df\n",
    "\n",
    "    # 4. Кластеризация DBSCAN (группировка уровней)\n",
    "    X = np.array(extrema).reshape(-1, 1)\n",
    "    eps_abs = np.mean(X) * tolerance  # Переводим относительный tolerance в абсолютный\n",
    "    db = DBSCAN(eps=eps_abs, min_samples=min_touch_count).fit(X)\n",
    "\n",
    "    # 5. Извлекаем уровни (игнорируем шум `label=-1`)\n",
    "    levels = [\n",
    "        np.mean(X[db.labels_ == lbl])\n",
    "        for lbl in set(db.labels_)\n",
    "        if lbl != -1\n",
    "    ]\n",
    "\n",
    "    # 6. Проверяем, находится ли текущий RSI рядом с каким-то уровнем\n",
    "    current_rsi = df[\"rsi\"].iloc[-1]\n",
    "    close_to_level = any(\n",
    "        abs(current_rsi - level) / level <= tolerance\n",
    "        for level in levels\n",
    "    )\n",
    "\n",
    "    df[\"rsi_near_level\"] = 0\n",
    "    if close_to_level:\n",
    "        df.at[df.index[-1], \"rsi_near_level\"] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c352c-5f2f-454e-b2a4-410a754fc614",
   "metadata": {},
   "source": [
    "Угл наклона RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c989f1e0-44ba-4723-bd45-4ba67e914b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi_slope(df, close_col='Close', length=50, lookback=10, slope_coeff=10):\n",
    "    \"\"\"\n",
    "    Рассчитывает RSI и его нормализованный наклон за lookback период\n",
    "    \n",
    "    Параметры:\n",
    "        df - DataFrame с ценовыми данными\n",
    "        close_col - название столбца с ценами закрытия\n",
    "        length - период для RSI\n",
    "        lookback - окно для расчета угла наклона\n",
    "        slope_coeff - коэффициент усиления наклона перед tanh\n",
    "        \n",
    "    Возвращает:\n",
    "        Series с нормализованными значениями наклона RSI\n",
    "    \"\"\"\n",
    "    # Рассчитываем RSI\n",
    "    rsi = ta.rsi(df[close_col], length=length)\n",
    "    \n",
    "    # Инициализируем массив для наклонов\n",
    "    slopes = np.zeros(len(df))\n",
    "    slopes[:] = np.nan  # Первые значения будут NaN\n",
    "    \n",
    "    # Рассчитываем наклон для каждого окна\n",
    "    for i in range(lookback, len(df)):\n",
    "        y = rsi.iloc[i-lookback:i].values\n",
    "        x = np.arange(len(y))\n",
    "        \n",
    "        # Игнорируем окна с пропусками\n",
    "        if np.isnan(y).any():\n",
    "            continue\n",
    "            \n",
    "        slope = np.polyfit(x, y, 1)[0]  # Линейный коэффициент\n",
    "        slopes[i] = np.tanh(slope * slope_coeff)  # Нормализация\n",
    "        \n",
    "    return slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2674eb-aeab-448c-a2a9-bbf0ea7ced6f",
   "metadata": {},
   "source": [
    "**CMF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d83052e9-2ca0-4df7-9a4a-c72d214bce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cmf(df, length=20, eps=1e-8):\n",
    "    mfv = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'] + eps)\n",
    "    mfv *= df['Volume']\n",
    "    cmf = mfv.rolling(window=length).sum() / (df['Volume'].rolling(window=length).sum() + eps)\n",
    "    return cmf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de27ec-fa6e-4cb6-b28d-2888b01a8dbc",
   "metadata": {},
   "source": [
    "**Пробой уровня сопротивления**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94f612c8-f67d-4337-bb77-9afcccd1f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_breakout_flags(df, resistance_window=20, lookback=5):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Сопротивление = максимум из прошлых resistance_window свечей\n",
    "    df['resistance'] = df['High'].rolling(window=resistance_window, min_periods=1).max().shift(1)\n",
    "\n",
    "    # Текущий пробой (High > resistance)\n",
    "    df['breakout_now'] = (df['High'] > df['resistance']).astype(int)\n",
    "\n",
    "    # Проверяем пробой в последние lookback свечей (включая текущую)\n",
    "    df['breakout_in_5'] = 0\n",
    "    for i in range(lookback + 1):  # +1, потому что range(5) даёт 0,1,2,3,4 (5 значений)\n",
    "        df['breakout_in_5'] |= (df['High'].shift(i) > df['resistance'].shift(i)).astype(int)\n",
    "\n",
    "    # Удаляем resistance (если не нужно для отладки)\n",
    "    df.drop(columns=['resistance'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204b1f5-a530-48c3-b5b8-542e9ed81cae",
   "metadata": {},
   "source": [
    "**Пробой уровня поддержки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2efd113-dbd4-47aa-9d91-db95827b179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_breakdown_flags(df, support_window=20, lookback=5):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Расчёт уровня поддержки (минимум за support_window свечей, сдвинутый на 1 вперёд)\n",
    "    df['support'] = df['Low'].rolling(window=support_window, min_periods=1).min().shift(1)\n",
    "\n",
    "    # Текущий пробой вниз (Low < support)\n",
    "    df['breakdown_now'] = (df['Low'] < df['support']).astype(int)\n",
    "\n",
    "    # Проверяем пробой вниз в последние lookback свечей (включая текущую)\n",
    "    df['breakdown_in_5'] = 0\n",
    "    for i in range(lookback + 1):  # +1 чтобы включить текущую свечу (i=0)\n",
    "        df['breakdown_in_5'] |= (df['Low'].shift(i) < df['support'].shift(i)).astype(int)\n",
    "\n",
    "    # Удаляем промежуточные столбцы (можно закомментировать для отладки)\n",
    "    df.drop(columns=['support'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbf9c9-8316-4839-9662-24615dc5263f",
   "metadata": {},
   "source": [
    "# Уровни фибоначчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "396b1f06-9981-438e-9adc-d2682f7445b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fibonacci_flags(df, window=300, threshold=0.003, debug=False):\n",
    "    try:\n",
    "        if df is None or df.empty or len(df) < window:\n",
    "            return pd.DataFrame(index=df.index)\n",
    "\n",
    "        fib_levels = [0.236, 0.382, 0.5, 0.618, 0.786]\n",
    "        level_cols = [f'fib_dist_{int(level*1000)}' for level in fib_levels]\n",
    "        result = pd.DataFrame(np.nan, index=df.index, columns=level_cols)\n",
    "\n",
    "        highs = df['High'].values\n",
    "        lows = df['Low'].values\n",
    "        closes = df['Close'].values\n",
    "\n",
    "        rolling_high = pd.Series(highs).rolling(window, min_periods=window).max().values\n",
    "        rolling_low = pd.Series(lows).rolling(window, min_periods=window).min().values\n",
    "        price_ranges = rolling_high - rolling_low\n",
    "        rel_changes = price_ranges / (rolling_low + 1e-10)\n",
    "\n",
    "        for i in range(window, len(df)):\n",
    "            if rel_changes[i] < threshold or np.isnan(price_ranges[i]):\n",
    "                continue\n",
    "\n",
    "            high = rolling_high[i]\n",
    "            low = rolling_low[i]\n",
    "            price_range = price_ranges[i]\n",
    "\n",
    "            window_highs = highs[i-window:i]\n",
    "            window_lows = lows[i-window:i]\n",
    "            if len(window_highs) == 0 or len(window_lows) == 0:\n",
    "                continue\n",
    "\n",
    "            high_pos = np.argmax(window_highs)\n",
    "            low_pos = np.argmin(window_lows)\n",
    "            is_uptrend = high_pos > low_pos\n",
    "            current_price = closes[i]\n",
    "\n",
    "            if is_uptrend:\n",
    "                fib_values = high - price_range * np.array(fib_levels)\n",
    "            else:\n",
    "                fib_values = low + price_range * np.array(fib_levels)\n",
    "\n",
    "            dists = (current_price - fib_values) / (current_price + 1e-10)\n",
    "            result.iloc[i] = dists.astype(np.float32)\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"⚠️ Fibonacci error: {str(e)}\")\n",
    "        return pd.DataFrame(index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18364f1e-ff50-4d28-9887-88b4c6efdc00",
   "metadata": {},
   "source": [
    "Фиба 0,5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b41ba52-5367-43c5-a3c0-174ba7b35e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib_dist_050_last50(df: pd.DataFrame,\n",
    "                        window: int = 50,\n",
    "                        threshold: float = 0.03,\n",
    "                        debug: bool = False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Вычисляет расстояние от текущей цены до уровня 0.5 по последней сетке Фибоначчи\n",
    "    (если был рост/падение более threshold за последние window свечей).\n",
    "    \n",
    "    Возвращает Series со значениями от -1 до 1, -1 если уровень не построен.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df is None or df.empty or len(df) < window:\n",
    "            return pd.Series(-1, index=df.index, name='fib_dist_050_last50')\n",
    "\n",
    "        highs = df['High'].values\n",
    "        lows = df['Low'].values\n",
    "        closes = df['Close'].values\n",
    "        result = np.full(len(df), -1.0, dtype=np.float32)  # по умолчанию -1\n",
    "\n",
    "        for i in range(window, len(df)):\n",
    "            high = np.max(highs[i - window:i])\n",
    "            low = np.min(lows[i - window:i])\n",
    "            current_price = closes[i]\n",
    "            price_range = high - low\n",
    "            if low == 0 or price_range / low < threshold:\n",
    "                continue\n",
    "\n",
    "            high_pos = np.argmax(highs[i - window:i])\n",
    "            low_pos = np.argmin(lows[i - window:i])\n",
    "            is_uptrend = high_pos > low_pos  # рост\n",
    "\n",
    "            # Уровень 0.5 по тренду\n",
    "            fib_level = high - 0.5 * price_range if is_uptrend else low + 0.5 * price_range\n",
    "\n",
    "            # Расстояние до уровня 0.5, нормализуем\n",
    "            dist = (current_price - fib_level) / price_range\n",
    "            dist = max(-1, min(1, dist))  # ограничим в пределах [-1, 1]\n",
    "\n",
    "            result[i] = dist\n",
    "\n",
    "        return pd.Series(result, index=df.index, name='fib_dist_050_last50')\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"⚠️ Fibonacci error: {e}\")\n",
    "        return pd.Series(-1, index=df.index, name='fib_dist_050_last50')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde0d2b-973a-4675-9fcb-9709e6d77147",
   "metadata": {},
   "source": [
    "# Cтохастик RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8ddc615-df63-45f0-93c4-0b44e5c3e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stochastic_rsi(df, rsi_period=14, stoch_period=14, smooth_k=3, smooth_d=3):\n",
    "\n",
    "    # RSI\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "\n",
    "    avg_gain = gain.rolling(window=rsi_period).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period).mean()\n",
    "\n",
    "    rs = avg_gain / (avg_loss + 1e-10)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Stochastic RSI\n",
    "    min_rsi = rsi.rolling(window=stoch_period).min()\n",
    "    max_rsi = rsi.rolling(window=stoch_period).max()\n",
    "    stoch_rsi = (rsi - min_rsi) / (max_rsi - min_rsi + 1e-10)\n",
    "\n",
    "    # Smoothed %K and %D\n",
    "    k = stoch_rsi.rolling(window=smooth_k).mean()\n",
    "    d = k.rolling(window=smooth_d).mean()\n",
    "\n",
    "    df['stoch_rsi_k'] = k\n",
    "    df['stoch_rsi_d'] = d\n",
    "\n",
    "    # Бычье пересечение: %K пересекает %D снизу вверх\n",
    "    bull_cross = (k.shift(1) < d.shift(1)) & (k >= d)\n",
    "    bull_cross_indices = df.index[bull_cross.fillna(False)]\n",
    "\n",
    "    # Создаём колонку для бычьего пересечения\n",
    "    stoch_cross_bull = pd.Series(0, index=df.index)\n",
    "\n",
    "    for idx in bull_cross_indices:\n",
    "        start = df.index.get_loc(idx)\n",
    "        stoch_cross_bull.iloc[start:start + 4] = 1  # текущий бар + 3 после него\n",
    "\n",
    "    df['stoch_cross_bull'] = stoch_cross_bull.clip(upper=1).fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7c52fe-b7a3-4bed-a4ff-d4ef46c0777a",
   "metadata": {},
   "source": [
    "# RSI дивергенции с ценой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34eef468-c10a-40d6-9bc0-8bd660422080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bullish_rsi_divergence(df, rsi_column='RSI7', lookback=5):\n",
    "    \"\"\"\n",
    "    Находит бычьи дивергенции RSI-цена (классическая, скрытая, расширенная)\n",
    "    \n",
    "    Возвращает:\n",
    "        - 'has_divergence_<lookback>'\n",
    "        - 'divergence_power_<lookback>': нормализованная сила дивергенции (-1..1)\n",
    "    \"\"\"\n",
    "    price_col = f'price_low_{lookback}'\n",
    "    rsi_col = f'rsi_low_{lookback}'\n",
    "    signal_col = f'has_divergence_{lookback}'\n",
    "    power_col = f'divergence_power_{lookback}'\n",
    "\n",
    "    # Локальные минимумы\n",
    "    df[price_col] = df['Low'].rolling(lookback, center=True).min() == df['Low']\n",
    "    df[rsi_col] = df[rsi_column].rolling(lookback, center=True).min() == df[rsi_column]\n",
    "    \n",
    "    # Инициализация\n",
    "    df[signal_col] = 0\n",
    "    df[power_col] = 0.0\n",
    "\n",
    "    for i in range(lookback * 2, len(df)):\n",
    "        if df[price_col].iloc[i] and df[rsi_col].iloc[i]:\n",
    "            for j in range(i-1, max(-1, i - lookback*2), -1):\n",
    "                if df[price_col].iloc[j]:\n",
    "                    current_low = df['Low'].iloc[i]\n",
    "                    prev_low = df['Low'].iloc[j]\n",
    "                    current_rsi = df[rsi_column].iloc[i]\n",
    "                    prev_rsi = df[rsi_column].iloc[j]\n",
    "                    rsi_diff = current_rsi - prev_rsi\n",
    "                    \n",
    "                    # Диапазон RSI в окне\n",
    "                    rsi_window = df[rsi_column].iloc[j:i+1]\n",
    "                    rsi_range = rsi_window.max() - rsi_window.min()\n",
    "                    if rsi_range == 0:\n",
    "                        normalized_power = 0\n",
    "                    else:\n",
    "                        normalized_power = round(rsi_diff / rsi_range, 3)\n",
    "\n",
    "                    # Проверка типов дивергенций\n",
    "                    if ((current_low < prev_low and current_rsi > prev_rsi) or    # Классическая\n",
    "                        (current_low > prev_low and current_rsi < prev_rsi) or    # Скрытая\n",
    "                        (abs(current_low - prev_low) < 0.01 * prev_low and current_rsi > prev_rsi)):  # Расширенная\n",
    "                        \n",
    "                        df.loc[df.index[i], signal_col] = 1\n",
    "                        df.loc[df.index[i], power_col] = normalized_power\n",
    "                    break\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a206ef9-7692-41e7-8a99-199e74cbb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsi_divergence(df, length=14, lbL=5, lbR=5, rangeLower=5, rangeUpper=60, signal_duration=5):\n",
    "    \"\"\"\n",
    "    Векторизованная и пригодная для реального времени версия RSI-дивергенций.\n",
    "    Сигналы выставляются на текущей свече и нескольких следующих.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['rsi'] = df.ta.rsi(close='Close', length=length)\n",
    "    df['bull_signal'] = 0\n",
    "    df['hidden_bull_signal'] = 0\n",
    "\n",
    "    lows = df['Low'].values\n",
    "    rsi = df['rsi'].values\n",
    "    n = len(df)\n",
    "\n",
    "    for i in range(lbL + lbR, n):\n",
    "        pivot_idx = i - lbR\n",
    "\n",
    "        # Проверка текущего пивота (на момент i)\n",
    "        is_low_pivot = (\n",
    "            lows[pivot_idx] == np.min(lows[pivot_idx - lbL:pivot_idx + lbR + 1]) and\n",
    "            rsi[pivot_idx] == np.min(rsi[pivot_idx - lbL:pivot_idx + lbR + 1])\n",
    "        )\n",
    "\n",
    "        if not is_low_pivot:\n",
    "            continue\n",
    "\n",
    "        # Ищем предыдущий пивот\n",
    "        for j in range(i - lbR - 1, lbL + lbR - 1, -1):\n",
    "            prev_pivot_idx = j - lbR\n",
    "            if prev_pivot_idx - lbL < 0 or prev_pivot_idx + lbR >= n:\n",
    "                continue\n",
    "\n",
    "            prev_pivot = (\n",
    "                lows[prev_pivot_idx] == np.min(lows[prev_pivot_idx - lbL:prev_pivot_idx + lbR + 1]) and\n",
    "                rsi[prev_pivot_idx] == np.min(rsi[prev_pivot_idx - lbL:prev_pivot_idx + lbR + 1])\n",
    "            )\n",
    "\n",
    "            if not prev_pivot:\n",
    "                continue\n",
    "\n",
    "            # Классическая бычья дивергенция\n",
    "            price_cond = lows[pivot_idx] < lows[prev_pivot_idx]\n",
    "            rsi_cond = rsi[pivot_idx] > rsi[prev_pivot_idx]\n",
    "\n",
    "            if price_cond and rsi_cond:\n",
    "                for k in range(signal_duration):\n",
    "                    if i + k < n:\n",
    "                        df.at[i + k, 'bull_signal'] = 1\n",
    "\n",
    "            # Скрытая бычья дивергенция\n",
    "            hidden_price_cond = lows[pivot_idx] > lows[prev_pivot_idx]\n",
    "            hidden_rsi_cond = rsi[pivot_idx] < rsi[prev_pivot_idx]\n",
    "\n",
    "            if hidden_price_cond and hidden_rsi_cond:\n",
    "                for k in range(signal_duration):\n",
    "                    if i + k < n:\n",
    "                        df.at[i + k, 'hidden_bull_signal'] = 1\n",
    "\n",
    "            break  # используем только первый найденный предыдущий пивот\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481766d-f988-4b84-b1ca-1b609d94ae83",
   "metadata": {},
   "source": [
    "**Профиль объема**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f74c95a-d33b-4c47-b1ae-1a2d937bcf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normalized_volume_profile(df, window=20, lag=0, change_window=5, sma_window=10):\n",
    "    \"\"\"\n",
    "    Добавляет в DataFrame ТОЛЬКО нормализованные значения:\n",
    "    - Нормализованный профиль объёмов (VWAP).\n",
    "    - Нормализованный профиль с лагом (если lag > 0).\n",
    "    - Нормализованную скорость изменения профиля.\n",
    "    - Нормализованную SMA профиля.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Расчёт VWAP (профиль объёмов)\n",
    "    df['Typical_Price'] = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    df['VP_Price'] = (df['Typical_Price'] * df['Volume']).rolling(window).sum() / df['Volume'].rolling(window).sum()\n",
    "    \n",
    "    # 2. Расчёт SMA профиля\n",
    "    df['VP_SMA'] = df['VP_Price'].rolling(sma_window).mean()\n",
    "    \n",
    "    # 3. Нормализация VP_Price\n",
    "    price_mean = df['VP_Price'].rolling(sma_window).mean()\n",
    "    price_std = df['VP_Price'].rolling(sma_window).std()\n",
    "    df['VP_Norm'] = (df['VP_Price'] - price_mean) / price_std\n",
    "    \n",
    "    # 4. Нормализация SMA (используем ОТДЕЛЬНЫЕ среднее и отклонение для SMA)\n",
    "    sma_mean = df['VP_SMA'].rolling(sma_window).mean()\n",
    "    sma_std = df['VP_SMA'].rolling(sma_window).std()\n",
    "    df['VP_SMA_Norm'] = (df['VP_SMA'] - sma_mean) / sma_std\n",
    "    \n",
    "    # 5. Нормализация изменения профиля\n",
    "    change_mean = df['VP_Price'].pct_change(change_window).rolling(sma_window).mean()\n",
    "    change_std = df['VP_Price'].pct_change(change_window).rolling(sma_window).std()\n",
    "    df['VP_Change_Norm'] = (df['VP_Price'].pct_change(change_window) - change_mean) / change_std\n",
    "    \n",
    "    # 6. Добавление лага (если требуется)\n",
    "    if lag > 0:\n",
    "        df['VP_Norm_Lag'] = df['VP_Norm'].shift(lag)\n",
    "    \n",
    "    # Удаляем все ненормализованные колонки\n",
    "    cols_to_drop = ['Typical_Price', 'VP_Price', 'VP_SMA']\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True, errors='ignore')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d401903-6e19-420d-9ed1-f20beaf342b7",
   "metadata": {},
   "source": [
    "**Фильтр объема**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdb16db8-355b-4262-a633-a4d976ef276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_volume_filter(df: pd.DataFrame, \n",
    "                      use_ema: bool = False,\n",
    "                      window_short: int = 20,\n",
    "                      window_long: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет индикатор Volume Filter с защитой от inf/nan и возможностью использовать EMA.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame с колонками ['High','Low','Open','Close','Volume']\n",
    "        use_ema (bool): Если True, использует EMA вместо SMA (меньше лага)\n",
    "        window_short (int): Период короткого скользящего среднего (по умолчанию 20)\n",
    "        window_long (int): Период длинного скользящего среднего (по умолчанию 100)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Исходный DataFrame с добавленной колонкой 'volume_filter'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Проверка валидности окон\n",
    "    if window_short >= window_long:\n",
    "        raise ValueError(\"Короткое окно должно быть меньше длинного\")\n",
    "    \n",
    "    # Копируем DataFrame чтобы избежать предупреждений\n",
    "    df = df.copy()\n",
    "    \n",
    "    try:\n",
    "        if use_ema:\n",
    "            # Версия с экспоненциальным сглаживанием\n",
    "            short_ma = df['Volume'].ewm(span=window_short, min_periods=1).mean()\n",
    "            long_ma = df['Volume'].ewm(span=window_long, min_periods=1).mean()\n",
    "        else:\n",
    "            # Стандартная версия с простым скользящим средним\n",
    "            short_ma = df['Volume'].rolling(window=window_short, min_periods=1).mean()\n",
    "            long_ma = df['Volume'].rolling(window=window_long, min_periods=1).mean()\n",
    "        \n",
    "        # Вычисляем процентное отклонение с защитой от деления на 0\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            ratio = np.where(long_ma != 0, short_ma / long_ma, 1)  # При делении на 0 возвращаем 1 (0% изменения)\n",
    "            volume_filter = (ratio - 1) * 100\n",
    "            \n",
    "            # Заменяем inf/nan на 0 (кроме начальных значений, где это нормально)\n",
    "            mask = (long_ma == 0) | (~np.isfinite(volume_filter))\n",
    "            volume_filter = np.where(mask, 0, volume_filter)\n",
    "        \n",
    "        df['volume_filter'] = volume_filter\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Ошибка при расчете Volume Filter: {str(e)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Пример использования:\n",
    "# df = add_volume_filter(df)  # SMA версия по умолчанию\n",
    "# df = add_volume_filter(df, use_ema=True)  # EMA версия\n",
    "# df = add_volume_filter(df, window_short=14, window_long=50)  # С кастомными периодами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693049a-5a37-4d04-84ab-cffee9711ab9",
   "metadata": {},
   "source": [
    "# ATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85e200bb-1dee-4fba-9428-e548fb2f6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_atr_normalized(\n",
    "    df: pd.DataFrame,\n",
    "    windows: list = [14, 24, 48],\n",
    "    ema: bool = True,\n",
    "    normalization_window: int = 100,\n",
    "    add_velocity: bool = True,\n",
    "    add_acceleration: bool = True,\n",
    "    velocity_window: int = 5,\n",
    "    eps: float = 1e-8,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Улучшенная нормализация ATR в диапазон [-1, 1] через скользящие квантили.\n",
    "    Подходит для ML и разных криптопар.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame с колонками ['High', 'Low', 'Close']\n",
    "        windows: Периоды ATR\n",
    "        ema: Использовать EMA вместо SMA\n",
    "        normalization_window: Окно для расчета квантилей\n",
    "        add_velocity: Добавить скорость изменения\n",
    "        add_acceleration: Добавить ускорение\n",
    "        velocity_window: Шаг для производных\n",
    "        eps: Малая константа для избежания деления на 0\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Расчёт True Range\n",
    "    prev_close = df['Close'].shift(1)\n",
    "    tr = pd.DataFrame({\n",
    "        'HL': df['High'] - df['Low'],\n",
    "        'HC': abs(df['High'] - prev_close),\n",
    "        'LC': abs(df['Low'] - prev_close)\n",
    "    }).max(axis=1)\n",
    "\n",
    "    for window in windows:\n",
    "        # Расчёт ATR\n",
    "        atr = tr.ewm(span=window, min_periods=1).mean() if ema else tr.rolling(window=window, min_periods=1).mean()\n",
    "        col_name = f'atr_{window}_norm'\n",
    "        \n",
    "        # Нормализация в [-1, 1] через скользящие квантили\n",
    "        rolling_min = atr.rolling(normalization_window, min_periods=1).min()\n",
    "        rolling_max = atr.rolling(normalization_window, min_periods=1).max()\n",
    "        range_ = rolling_max - rolling_min + eps\n",
    "        \n",
    "        df[col_name] = 2 * ((atr - rolling_min) / range_) - 1  # Формула для [-1, 1]\n",
    "        \n",
    "        # Производные\n",
    "        if add_velocity:\n",
    "            df[f'{col_name}_velocity'] = df[col_name].diff(velocity_window) / velocity_window\n",
    "        \n",
    "        if add_acceleration and add_velocity:\n",
    "            df[f'{col_name}_acceleration'] = df[f'{col_name}_velocity'].diff(velocity_window)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca966c7b-cc9b-46a9-b823-11a2cbdbb236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_atr_normalized_rsi_weight(df: pd.DataFrame, \n",
    "                    windows: list = [14, 24, 48],\n",
    "                    ema: bool = True,\n",
    "                    normalize: bool = True,\n",
    "                    add_velocity: bool = True,\n",
    "                    add_acceleration: bool = True,\n",
    "                    rsi_period: int = 14,\n",
    "                    slope_window: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет несколько ATR с разными окнами и их производные, включая RSI-взвешенные версии.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame с колонками ['High', 'Low', 'Close']\n",
    "        windows (list): Список периодов ATR (по умолчанию [14, 24, 48])\n",
    "        ema (bool): Использовать EMA (True) или SMA (False) \n",
    "        normalize (bool): Если True, возвращает ATR в % от цены закрытия и добавляет RSI-взвешенные колонки\n",
    "        add_velocity (bool): Добавлять скорость изменения ATR\n",
    "        add_acceleration (bool): Добавлять ускорение изменения ATR\n",
    "        rsi_period (int): Период для расчета RSI (по умолчанию 14)\n",
    "        slope_window (int): Окно для расчета угла наклона RSI (по умолчанию 5)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: С добавленными колонками ATR и производными для каждого окна\n",
    "    \"\"\"\n",
    "    eps=1e-8\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Вычисляем RSI\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    avg_gain = gain.rolling(window=rsi_period, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=rsi_period, min_periods=1).mean()\n",
    "    \n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Рассчитываем угол наклона RSI за последние slope_window свечей\n",
    "    rsi_slope = rsi.diff(slope_window) / slope_window\n",
    "    \n",
    "    # Упрощенный расчет веса: RSI * угол наклона RSI\n",
    "    rsi_weight = rsi / (rsi_slope+ eps)\n",
    "    \n",
    "    for window in windows:\n",
    "        # Расчет True Range (TR)\n",
    "        prev_close = df['Close'].shift(1)\n",
    "        tr = pd.DataFrame({\n",
    "            'HL': df['High'] - df['Low'],\n",
    "            'HC': abs(df['High'] - prev_close),\n",
    "            'LC': abs(df['Low'] - prev_close)\n",
    "        }).max(axis=1)\n",
    "        \n",
    "        # Расчет ATR (EMA или SMA)\n",
    "        atr = tr.ewm(span=window, min_periods=1).mean() if ema else tr.rolling(window=window, min_periods=1).mean()\n",
    "        \n",
    "        # Базовое имя колонки\n",
    "        base_col = f'atr_{window}'\n",
    "        \n",
    "        # Нормализация (если включена)\n",
    "        if normalize:\n",
    "            norm_col = f'{base_col}_norm'\n",
    "            df[norm_col] = (atr / df['Close']) * 100  # ATR в % от цены закрытия\n",
    "            df[norm_col] = df[norm_col].replace([np.inf, -np.inf], np.nan)\n",
    "            \n",
    "            # Добавляем RSI-взвешенную версию\n",
    "            df[f'{norm_col}_rsi_weight'] = df[norm_col] * rsi_weight\n",
    "            \n",
    "            # Работаем с нормализованным ATR для производных\n",
    "            working_col = norm_col\n",
    "        else:\n",
    "            # Если нормализация отключена, работаем с абсолютным ATR\n",
    "            df[base_col] = atr\n",
    "            working_col = base_col\n",
    "        \n",
    "        # Добавляем velocity (скорость изменения)\n",
    "        if add_velocity:\n",
    "            velocity_col = f'{working_col}_velocity'\n",
    "            df[velocity_col] = df[working_col].diff() / df[working_col].shift(1)\n",
    "        \n",
    "        # Добавляем acceleration (ускорение изменения)\n",
    "        if add_acceleration and add_velocity:\n",
    "            acceleration_col = f'{working_col}_acceleration'\n",
    "            df[acceleration_col] = df[velocity_col].diff()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7317d7d-d906-4110-add9-d737a874bf71",
   "metadata": {},
   "source": [
    "Угол наклона ATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "517037f1-05f1-492f-a341-63e864c0cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_atr_slope(df, length=14, lookback=5, slope_coeff=5):\n",
    "    \"\"\"\n",
    "    Рассчитывает нормализованный наклон ATR за lookback период\n",
    "    \n",
    "    Parameters:\n",
    "        df - DataFrame с ценовыми данными\n",
    "        length - период для ATR\n",
    "        lookback - окно для расчета угла наклона\n",
    "        slope_coeff - коэффициент усиления наклона перед tanh\n",
    "        \n",
    "    Returns:\n",
    "        Series с нормализованными значениями наклона ATR\n",
    "    \"\"\"\n",
    "    # Сначала добавляем ATR с нужным периодом\n",
    "    df = add_atr_normalized(df, windows=[length], normalize=False, \n",
    "                           add_velocity=False, add_acceleration=False)\n",
    "    \n",
    "    atr_col = f'atr_{length}'\n",
    "    slopes = np.zeros(len(df))\n",
    "    slopes[:] = np.nan\n",
    "    \n",
    "    for i in range(lookback, len(df)):\n",
    "        y = df[atr_col].iloc[i-lookback:i].values\n",
    "        x = np.arange(len(y))\n",
    "        \n",
    "        if np.isnan(y).any():\n",
    "            continue\n",
    "            \n",
    "        slope = np.polyfit(x, y, 1)[0]\n",
    "        slopes[i] = np.tanh(slope * slope_coeff)\n",
    "        \n",
    "    return slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc342c42-24f9-4425-9918-41b666179a87",
   "metadata": {},
   "source": [
    "EMA для ATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "52b5b8d7-01b0-4f0b-bcce-6e7acfe688f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_atr_ema_normalized(df, length_atr=14, length_ema=20, eps=1e-5):\n",
    "    \"\"\"\n",
    "    Рассчитывает нормализованное расстояние между EMA от ATR и текущим ATR\n",
    "    \n",
    "    Parameters:\n",
    "        df - DataFrame с ценовыми данными\n",
    "        length_atr - период для ATR\n",
    "        length_ema - период для EMA\n",
    "        eps - малое число для избежания деления на 0\n",
    "        \n",
    "    Returns:\n",
    "        Series с нормализованными значениями расстояния в диапазоне [-1, 1]\n",
    "    \"\"\"\n",
    "    # Сначала добавляем ATR с нужным периодом\n",
    "    df = add_atr_normalized(df, windows=[length_atr], normalize=False,\n",
    "                          add_velocity=False, add_acceleration=False)\n",
    "    \n",
    "    atr_col = f'atr_{length_atr}'\n",
    "    atr = df[atr_col]\n",
    "    atr_ema = ta.ema(atr, length=length_ema)\n",
    "    \n",
    "    # Нормализация расстояния между EMA(ATR) и текущим ATR\n",
    "    normalized_diff = np.clip((atr_ema - atr) / (atr + eps), -1, 1)\n",
    "    \n",
    "    return normalized_diff\n",
    "\n",
    "\n",
    "# Пример использования: df['ATR_50_EMA_20_Norm'] = calculate_atr_ema_normalized(df, length_atr=50, length_ema=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8dc60b-b4c7-4421-a41a-a3339d6dbcec",
   "metadata": {},
   "source": [
    "atr_24_norm_dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "efe39c5c-3a5e-4182-8c3b-98417bbe39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_atr_24_norm_dynamics(df, col='atr_24_norm'):\n",
    "    \"\"\"\n",
    "    Добавляет производные признаки от atr_24_norm:\n",
    "    - Скользящее стандартное отклонение\n",
    "    - Скользящее среднее изменения\n",
    "    - Процентное изменение\n",
    "    - Скользящее среднее процентного изменения\n",
    "    - Скользящая производная (разность) первого порядка\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Скользящее стандартное отклонение за 10 и 30 баров\n",
    "    df[f'{col}_rolling_std_10'] = df[col].rolling(window=10).std()\n",
    "    df[f'{col}_rolling_std_30'] = df[col].rolling(window=30).std()\n",
    "\n",
    "    # Разность первого порядка\n",
    "    df[f'{col}_diff_1'] = df[col].diff()\n",
    "    df[f'{col}_rolling_diff_mean_10'] = df[col].diff().rolling(window=10).mean()\n",
    "\n",
    "    # Процентное изменение\n",
    "    df[f'{col}_pct_change'] = df[col].pct_change()\n",
    "    df[f'{col}_pct_change_rolling_mean_10'] = df[col].pct_change().rolling(window=10).mean()\n",
    "\n",
    "    # Производная второго порядка (ускорение)\n",
    "    df[f'{col}_diff2'] = df[col].diff().diff()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda1f16-944c-4845-adb2-6fd7fe118aae",
   "metadata": {},
   "source": [
    "доп варианты ATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f40cba0e-607c-4fde-a90a-62c59bb38504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr_local_extremes(df, atr_col='atr_24_norm', window=14):\n",
    "    \"\"\"Детектирует точки перегиба волатильности\"\"\"\n",
    "    df[f'{atr_col}_low'] = df[atr_col].rolling(window).min() == df[atr_col]\n",
    "    df[f'{atr_col}_high'] = df[atr_col].rolling(window).max() == df[atr_col]\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8762343-504e-493c-82d4-b4e8b36c26a5",
   "metadata": {},
   "source": [
    "Еще доп варианты ATR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ea51878-ae86-4b46-9e3c-fb2e55248de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_atr_features(df: pd.DataFrame, \n",
    "                     atr_col: str = 'atr_24_norm', \n",
    "                     compress_window: int = 5,\n",
    "                     expansion_threshold: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет признаки сжатия и расширения волатильности на основе ATR\n",
    "    \n",
    "    Параметры:\n",
    "    - atr_col: колонка с нормализованным ATR\n",
    "    - compress_window: окно для определения среднего уровня сжатия\n",
    "    - expansion_threshold: пороговое количество периодов для определения расширения\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Признак сжатия волатильности\n",
    "    df[f'{atr_col}_compress'] = df[atr_col] < df[atr_col].rolling(compress_window).mean()\n",
    "    \n",
    "    # Признак расширения волатильности\n",
    "    df[f'{atr_col}_expansion'] = (df[atr_col] > df[atr_col].shift(expansion_threshold)) & df[f'{atr_col}_compress'].shift(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_atr_regimes(df: pd.DataFrame, \n",
    "                    atr_col: str = 'atr_24_norm',\n",
    "                    regime_window: int = 50,\n",
    "                    low_multiplier: float = 0.5,\n",
    "                    high_multiplier: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет колонку с режимами волатильности на основе ATR\n",
    "    \n",
    "    Параметры:\n",
    "    - atr_col: колонка с нормализованным ATR\n",
    "    - regime_window: окно для определения среднего уровня волатильности\n",
    "    - low_multiplier: порог для низкой волатильности\n",
    "    - high_multiplier: порог для высокой волатильности\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Вычисление среднего ATR\n",
    "    atr_mean = df[atr_col].rolling(regime_window).mean()\n",
    "    \n",
    "    # Определение режимов\n",
    "    conditions = [\n",
    "        df[atr_col] < (atr_mean * low_multiplier),  # низкая волатильность\n",
    "        df[atr_col] > (atr_mean * high_multiplier)   # высокая волатильность\n",
    "    ]\n",
    "    \n",
    "    choices = [-1, 1]  # -1 = low, 0 = normal, 1 = high\n",
    "    \n",
    "    # Создание колонки с режимами\n",
    "    df[f'{atr_col}_regime'] = np.select(conditions, choices, default=0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_price_atr_interaction(df: pd.DataFrame,\n",
    "                             atr_col: str = 'atr_24_norm',\n",
    "                             amplitude_window: int = 3,\n",
    "                             ratio_threshold: float = 0.8,  # Более мягкий порог\n",
    "                             use_regime: bool = False):     # Опциональное использование режима\n",
    "    \"\"\"\n",
    "    Улучшенная версия с настраиваемыми параметрами\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    candle_amplitude = df['High'] - df['Low']\n",
    "    df['candle_atr_ratio'] = candle_amplitude / (df[atr_col] * df['Close'] / 100)\n",
    "    df['candle_atr_ratio_ma'] = df['candle_atr_ratio'].rolling(amplitude_window).mean()\n",
    "    \n",
    "    # Базовые условия\n",
    "    conditions = [\n",
    "        (df['Close'] > df['Open']),\n",
    "        (df['candle_atr_ratio'] > ratio_threshold),\n",
    "        (df[atr_col] < df[atr_col].shift(1))\n",
    "    ]\n",
    "    \n",
    "    # Добавляем условие по режиму только если нужно\n",
    "    if use_regime and f'{atr_col}_regime' in df.columns:\n",
    "        conditions.append(df[f'{atr_col}_regime'] == -1)\n",
    "    \n",
    "    # Комбинируем условия\n",
    "    df['atr_breakout_signal'] = np.logical_and.reduce(conditions).astype(int)\n",
    "    \n",
    "    # Добавляем отладочные колонки\n",
    "    if DEBUG_MODE:\n",
    "        for i, cond in enumerate(conditions):\n",
    "            df[f'breakout_cond_{i}'] = cond.astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64d950-79f9-4641-bf3b-ce14560cd0b2",
   "metadata": {},
   "source": [
    "**Тренд Ишимоку**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3282552e-d2c3-4a68-97b3-ef06939e1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normalized_ichimoku(\n",
    "    df: pd.DataFrame,\n",
    "    tenkan_period: int = 9,\n",
    "    kijun_period: int = 26,\n",
    "    senkou_period: int = 52,\n",
    "    normalization_base: str = \"Close\",\n",
    "    replace_inf_with: float = None,\n",
    "    fill_na: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет нормализованные компоненты Ichimoku без утечки будущего в исходный DataFrame.\n",
    "    \n",
    "    Параметры:\n",
    "        df: DataFrame с колонками ['High', 'Low', 'Close'].\n",
    "        tenkan_period: Период для Tenkan-sen (по умолчанию 9).\n",
    "        kijun_period: Период для Kijun-sen (по умолчанию 26).\n",
    "        senkou_period: Период для Senkou Span B (по умолчанию 52).\n",
    "        normalization_base: Столбец для нормализации (по умолчанию 'Close').\n",
    "        replace_inf_with: Если не None, заменяет inf на это значение (например, 0 или np.nan).\n",
    "        fill_na: Заполнять ли пропуски последним валидным значением.\n",
    "        \n",
    "    Возвращает:\n",
    "        Копию исходного DataFrame с добавленными нормализованными колонками Ichimoku.\n",
    "    \"\"\"\n",
    "    # Проверка наличия необходимых колонок\n",
    "    required_columns = {'High', 'Low', 'Close'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        missing = required_columns - set(df.columns)\n",
    "        raise ValueError(f\"Не хватает колонок: {missing}\")\n",
    "\n",
    "    if normalization_base not in df.columns:\n",
    "        raise ValueError(f\"Столбец для нормализации '{normalization_base}' не найден.\")\n",
    "\n",
    "    # Создаем копию DataFrame, чтобы не изменять исходный\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Проверка на нули в normalization_base (чтобы не было деления на 0)\n",
    "    if (df[normalization_base] == 0).any():\n",
    "        if replace_inf_with is None:\n",
    "            raise ValueError(\n",
    "                f\"В столбце '{normalization_base}' есть нули. \"\n",
    "                \"Используйте `replace_inf_with` для обработки.\"\n",
    "            )\n",
    "        else:\n",
    "            # Заменяем нули на очень маленькое число (1e-10), чтобы не было inf\n",
    "            base = df[normalization_base].replace(0, 1e-10)\n",
    "    else:\n",
    "        base = df[normalization_base]\n",
    "\n",
    "    # 1. Tenkan-sen (Conversion Line)\n",
    "    tenkan_high = df['High'].rolling(tenkan_period, min_periods=1).max()\n",
    "    tenkan_low = df['Low'].rolling(tenkan_period, min_periods=1).min()\n",
    "    tenkan_sen = (tenkan_high + tenkan_low) / 2\n",
    "    df['tenkan_sen_norm'] = tenkan_sen / base\n",
    "\n",
    "    # 2. Kijun-sen (Base Line)\n",
    "    kijun_high = df['High'].rolling(kijun_period, min_periods=1).max()\n",
    "    kijun_low = df['Low'].rolling(kijun_period, min_periods=1).min()\n",
    "    kijun_sen = (kijun_high + kijun_low) / 2\n",
    "    df['kijun_sen_norm'] = kijun_sen / base\n",
    "\n",
    "    # 3. Senkou Span A (сдвиг назад, а не вперёд, чтобы избежать look-ahead)\n",
    "    senkou_span_a = ((tenkan_sen + kijun_sen) / 2).shift(1) #kijun_period\n",
    "    df['senkou_span_a_norm'] = senkou_span_a / base\n",
    "\n",
    "    # 4. Senkou Span B (сдвиг назад)\n",
    "    senkou_high = df['High'].rolling(senkou_period, min_periods=1).max()\n",
    "    senkou_low = df['Low'].rolling(senkou_period, min_periods=1).min()\n",
    "    senkou_span_b = ((senkou_high + senkou_low) / 2).shift(1) #kijun_period\n",
    "    df['senkou_span_b_norm'] = senkou_span_b / base\n",
    "\n",
    "    # 5. Границы облака\n",
    "    #df['cloud_top_norm'] = df[['senkou_span_a_norm', 'senkou_span_b_norm']].max(axis=1)\n",
    "    #df['cloud_bottom_norm'] = df[['senkou_span_a_norm', 'senkou_span_b_norm']].min(axis=1)\n",
    "\n",
    "    # Замена inf (если replace_inf_with задан)\n",
    "    if replace_inf_with is not None:\n",
    "        df = df.replace([np.inf, -np.inf], replace_inf_with)\n",
    "\n",
    "    # Заполнение пропусков (если fill_na=True)\n",
    "    if fill_na:\n",
    "        pd.set_option('future.no_silent_downcasting', True)  # <- Добавить эту строку\n",
    "        df = df.ffill()  # Теперь без .infer_objects(), т.к. downcasting отключён\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae912c9b-ca83-4483-8605-f688b450b50a",
   "metadata": {},
   "source": [
    "**Ишимоку ХТФ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4481dc4c-2410-4af5-b010-26fd8ad0eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normalized_ichimoku_htf(\n",
    "    df: pd.DataFrame,\n",
    "    tenkan_period: int = 108,\n",
    "    kijun_period: int = 312,\n",
    "    senkou_period: int = 624,\n",
    "    normalization_base: str = \"Close\",\n",
    "    replace_inf_with: float = 0,\n",
    "    fill_na: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет нормализованные компоненты Ichimoku без утечки будущего в исходный DataFrame.\n",
    "    \n",
    "    Параметры:\n",
    "        df: DataFrame с колонками ['High', 'Low', 'Close'].\n",
    "        tenkan_period: Период для Tenkan-sen (по умолчанию 9).\n",
    "        kijun_period: Период для Kijun-sen (по умолчанию 26).\n",
    "        senkou_period: Период для Senkou Span B (по умолчанию 52).\n",
    "        normalization_base: Столбец для нормализации (по умолчанию 'Close').\n",
    "        replace_inf_with: Если не None, заменяет inf на это значение (например, 0 или np.nan).\n",
    "        fill_na: Заполнять ли пропуски последним валидным значением.\n",
    "        \n",
    "    Возвращает:\n",
    "        Копию исходного DataFrame с добавленными нормализованными колонками Ichimoku.\n",
    "    \"\"\"\n",
    "    # Проверка наличия необходимых колонок\n",
    "    required_columns = {'High', 'Low', 'Close'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        missing = required_columns - set(df.columns)\n",
    "        raise ValueError(f\"Не хватает колонок: {missing}\")\n",
    "\n",
    "    if normalization_base not in df.columns:\n",
    "        raise ValueError(f\"Столбец для нормализации '{normalization_base}' не найден.\")\n",
    "\n",
    "    # Создаем копию DataFrame, чтобы не изменять исходный\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Проверка на нули в normalization_base (чтобы не было деления на 0)\n",
    "    if (df[normalization_base] == 0).any():\n",
    "        if replace_inf_with is None:\n",
    "            raise ValueError(\n",
    "                f\"В столбце '{normalization_base}' есть нули. \"\n",
    "                \"Используйте `replace_inf_with` для обработки.\"\n",
    "            )\n",
    "        else:\n",
    "            # Заменяем нули на очень маленькое число (1e-10), чтобы не было inf\n",
    "            base = df[normalization_base].replace(0, 1e-10)\n",
    "    else:\n",
    "        base = df[normalization_base]\n",
    "\n",
    "    # 1. Tenkan-sen (Conversion Line)\n",
    "    tenkan_high = df['High'].rolling(tenkan_period, min_periods=1).max()\n",
    "    tenkan_low = df['Low'].rolling(tenkan_period, min_periods=1).min()\n",
    "    tenkan_sen = (tenkan_high + tenkan_low) / 2\n",
    "    df['tenkan_sen_norm_htf'] = tenkan_sen / base\n",
    "\n",
    "    # 2. Kijun-sen (Base Line)\n",
    "    kijun_high = df['High'].rolling(kijun_period, min_periods=1).max()\n",
    "    kijun_low = df['Low'].rolling(kijun_period, min_periods=1).min()\n",
    "    kijun_sen = (kijun_high + kijun_low) / 2\n",
    "    df['kijun_sen_norm_htf'] = kijun_sen / base\n",
    "\n",
    "    # 3. Senkou Span A (сдвиг назад, а не вперёд, чтобы избежать look-ahead)\n",
    "    senkou_span_a = ((tenkan_sen + kijun_sen) / 2).shift(1) #shift(kijun_period)\n",
    "    df['senkou_span_a_norm_htf'] = senkou_span_a / base\n",
    "\n",
    "    # 4. Senkou Span B (сдвиг назад)\n",
    "    senkou_high = df['High'].rolling(senkou_period, min_periods=1).max()\n",
    "    senkou_low = df['Low'].rolling(senkou_period, min_periods=1).min()\n",
    "    senkou_span_b = ((senkou_high + senkou_low) / 2).shift(1) #shift(kijun_period)\n",
    "    df['senkou_span_b_norm_htf'] = senkou_span_b / base\n",
    "\n",
    "    # 5. Границы облака\n",
    "    #df['cloud_top_norm_htf'] = df[['senkou_span_a_norm_htf', 'senkou_span_b_norm_htf']].max(axis=1)\n",
    "    #df['cloud_bottom_norm_htf'] = df[['senkou_span_a_norm_htf', 'senkou_span_b_norm_htf']].min(axis=1)\n",
    "\n",
    "    # Замена inf (если replace_inf_with задан)\n",
    "    if replace_inf_with is not None:\n",
    "        df = df.replace([np.inf, -np.inf], replace_inf_with)\n",
    "\n",
    "    # Заполнение пропусков (если fill_na=True)\n",
    "    if fill_na:\n",
    "        pd.set_option('future.no_silent_downcasting', True)  # <- Добавить эту строку\n",
    "        df = df.ffill()  # Теперь без .infer_objects(), т.к. downcasting отключён\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfefd602-e248-400a-bba8-fdc4f907fc81",
   "metadata": {},
   "source": [
    "**Ишимоку RSI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a88a392-8916-4c60-8f4f-74df6d357149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normalized_ichimoku_on_rsi(\n",
    "    df: pd.DataFrame,\n",
    "    rsi_period: int = 14,\n",
    "    tenkan_period: int = 9,\n",
    "    kijun_period: int = 26,\n",
    "    senkou_period: int = 52,\n",
    "    replace_inf_with: float = None,\n",
    "    fill_na: bool = True,\n",
    "    prefix: str = \"rsi14_ichimoku\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Вычисляет Ichimoku на основе RSI и добавляет нормализованные компоненты в DataFrame.\n",
    "    \n",
    "    Параметры:\n",
    "        df: DataFrame с колонкой 'Close'.\n",
    "        rsi_period: Период RSI (по умолчанию 14).\n",
    "        Остальные параметры — как в оригинальной Ichimoku-функции.\n",
    "        prefix: Префикс для названий новых колонок.\n",
    "    \n",
    "    Возвращает:\n",
    "        df с новыми колонками Ichimoku по RSI.\n",
    "    \"\"\"\n",
    "    if 'Close' not in df.columns:\n",
    "        raise ValueError(\"В DataFrame нет колонки 'Close' для расчета RSI.\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Расчет RSI\n",
    "    rsi = ta.rsi(df['Close'], length=rsi_period)\n",
    "    df[f'RSI_{rsi_period}'] = rsi\n",
    "\n",
    "    # Проверка на нули\n",
    "    if (rsi == 0).any():\n",
    "        if replace_inf_with is None:\n",
    "            raise ValueError(\"RSI содержит нули, что может привести к делению на 0.\")\n",
    "        else:\n",
    "            base = rsi.replace(0, 1e-10)\n",
    "    else:\n",
    "        base = rsi\n",
    "\n",
    "    # Ichimoku на основе RSI\n",
    "    # Tenkan\n",
    "    tenkan_high = rsi.rolling(tenkan_period, min_periods=1).max()\n",
    "    tenkan_low = rsi.rolling(tenkan_period, min_periods=1).min()\n",
    "    tenkan_sen = (tenkan_high + tenkan_low) / 2\n",
    "    df[f'{prefix}_tenkan_norm'] = tenkan_sen / base\n",
    "\n",
    "    # Kijun\n",
    "    kijun_high = rsi.rolling(kijun_period, min_periods=1).max()\n",
    "    kijun_low = rsi.rolling(kijun_period, min_periods=1).min()\n",
    "    kijun_sen = (kijun_high + kijun_low) / 2\n",
    "    df[f'{prefix}_kijun_norm'] = kijun_sen / base\n",
    "\n",
    "    # Senkou A\n",
    "    senkou_span_a = ((tenkan_sen + kijun_sen) / 2).shift(1) #shift(kijun_period)\n",
    "    df[f'{prefix}_senkou_a_norm'] = senkou_span_a / base\n",
    "\n",
    "    # Senkou B\n",
    "    senkou_high = rsi.rolling(senkou_period, min_periods=1).max()\n",
    "    senkou_low = rsi.rolling(senkou_period, min_periods=1).min()\n",
    "    senkou_span_b = ((senkou_high + senkou_low) / 2).shift(1) #shift(kijun_period)\n",
    "    df[f'{prefix}_senkou_b_norm'] = senkou_span_b / base\n",
    "\n",
    "    # Обработка inf\n",
    "    if replace_inf_with is not None:\n",
    "        df = df.replace([np.inf, -np.inf], replace_inf_with)\n",
    "\n",
    "    # Пропуски\n",
    "    if fill_na:\n",
    "        pd.set_option('future.no_silent_downcasting', True)\n",
    "        df = df.ffill()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53df1b-b08d-400d-8f34-8c3436a57baa",
   "metadata": {},
   "source": [
    "**compute_atr_trailing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8147f031-9fd3-48e9-9cdb-c3d6ac1b6dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_atr_trailing(df, atr_period=14, multiplier=3):\n",
    "    \"\"\"\n",
    "    Вычисляет ATR-based трейлинг-стоп и сигналы.\n",
    "    Добавляет столбцы:\n",
    "    - 'ATR' - Average True Range\n",
    "    - 'TrailingStop' - трейлинг-стоп уровень\n",
    "    - 'ATR_Signal' - бинарный сигнал (1/-1)\n",
    "    - 'ATR_Signal_Norm' - нормализованный сигнал (0-1)\n",
    "    - 'ATR_x10' - ATR * 10 (для наглядности)\n",
    "\n",
    "    Параметры:\n",
    "        df: DataFrame с колонками ['High', 'Low', 'Close']\n",
    "        atr_period: период для ATR (по умолчанию 14)\n",
    "        multiplier: множитель для ATR (по умолчанию 3)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Вычисляем True Range\n",
    "    tr = np.maximum(\n",
    "        df['High'] - df['Low'],\n",
    "        np.maximum(\n",
    "            abs(df['High'] - df['Close'].shift(1)),\n",
    "            abs(df['Low'] - df['Close'].shift(1))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Вычисляем ATR с защитой от NaN/inf\n",
    "    atr = tr.rolling(atr_period, min_periods=1).mean().replace([np.inf, -np.inf], np.nan).ffill()\n",
    "    \n",
    "    # Трейлинг-стоп\n",
    "    df['TrailingStop'] = df['Close'] - multiplier * atr\n",
    "    \n",
    "    # Генерация сигнала\n",
    "    df['ATR_Signal'] = np.where(df['Close'] > df['TrailingStop'].shift(1), 1, -1)\n",
    "    \n",
    "    # Нормализация сигнала от 0 до 1\n",
    "    df['ATR_Signal_Norm'] = (df['ATR_Signal'] + 1) / 2  # преобразует -1/1 в 0/1\n",
    "    \n",
    "    # Дополнительный столбец ATR*10\n",
    "    df['ATR_x10'] = atr * 10\n",
    "    \n",
    "    # Защита от оставшихся NaN (если они возникнут)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).ffill()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4635d30b-56bd-4608-b5f5-6c8add5a0861",
   "metadata": {},
   "source": [
    "**TRIX (Triple Exponential Average) и Elder Ray (Bull Power и Bear Power)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d1fff3f6-6e0c-4e45-95ae-95c023e8f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trix_elder(df, trix_period=30, ema_period=50, eps=1e-8):\n",
    "    df = df.copy()\n",
    "\n",
    "    # === TRIX ===\n",
    "    ema1 = df['Close'].ewm(span=trix_period, adjust=False).mean()\n",
    "    ema2 = ema1.ewm(span=trix_period, adjust=False).mean()\n",
    "    ema3 = ema2.ewm(span=trix_period, adjust=False).mean()\n",
    "    df['TRIX'] = ema3.pct_change() * 100\n",
    "\n",
    "    # === Elder Ray ===\n",
    "    ema = df['Close'].ewm(span=ema_period, adjust=False).mean()\n",
    "    df['BullPower'] = df['High'] - ema\n",
    "    df['BearPower'] = df['Low'] - ema\n",
    "\n",
    "    # === Обработка NaN/inf ===\n",
    "    features = ['TRIX', 'BullPower', 'BearPower']\n",
    "    df[features] = df[features].replace([np.inf, -np.inf], np.nan)\n",
    "    df[features] = df[features].ffill().bfill().fillna(0)\n",
    "\n",
    "    # === Простая нормализация по модулю максимального значения ===\n",
    "    for col in features:\n",
    "        max_val = df[col].abs().max() + eps\n",
    "        df[col] = df[col] / max_val\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532172d-5b71-474b-922a-24a8cf16195d",
   "metadata": {},
   "source": [
    "**Schaff Trend Cycle (STC) + Fisher Transform + Keltner Channel Width**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4493379-b53d-47c7-86cd-d1e72ae76de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_combined_indicators(df, \n",
    "                              stc_fast=23, \n",
    "                              stc_slow=50, \n",
    "                              stc_cycle=10,\n",
    "                              fisher_length=10,\n",
    "                              keltner_atr_period=20,\n",
    "                              keltner_multiplier=2):\n",
    "    \"\"\"\n",
    "    Улучшенная версия комбинированных индикаторов:\n",
    "    - Schaff Trend Cycle (STC)\n",
    "    - Fisher Transform\n",
    "    - Keltner Channel Width\n",
    "    \n",
    "    Возвращает только новые признаки без колонок Date и target.\n",
    "    Все значения нормализованы и защищены от NaN/Inf.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # === 1. Улучшенный Schaff Trend Cycle ===\n",
    "    def calculate_stc(close, fast, slow, cycle):\n",
    "        ema_fast = close.ewm(span=fast, adjust=False).mean()\n",
    "        ema_slow = close.ewm(span=slow, adjust=False).mean()\n",
    "        macd = ema_fast - ema_slow\n",
    "        macd_ema = macd.ewm(span=cycle, adjust=False).mean()\n",
    "        \n",
    "        # Более стабильный расчет с защитой от деления на 0\n",
    "        denominator = macd_ema.mask(macd_ema.abs() < 1e-10, np.nan)\n",
    "        stc_raw = macd.sub(macd_ema).div(denominator)\n",
    "        stc_raw = stc_raw.fillna(0).clip(-1, 1)\n",
    "        \n",
    "        return stc_raw.ewm(span=cycle, adjust=False).mean()\n",
    "    \n",
    "    df['STC'] = calculate_stc(df['Close'], stc_fast, stc_slow, stc_cycle)\n",
    "\n",
    "    # === 2. Улучшенный Fisher Transform ===\n",
    "    def calculate_fisher(high, low, length):\n",
    "        hl2 = (high + low) / 2\n",
    "        min_hl2 = hl2.rolling(length, min_periods=1).min()\n",
    "        max_hl2 = hl2.rolling(length, min_periods=1).max()\n",
    "        \n",
    "        # Более безопасная нормализация\n",
    "        range_hl2 = max_hl2 - min_hl2\n",
    "        normalized = 2 * ((hl2 - min_hl2) / range_hl2.mask(range_hl2 < 1e-10, np.nan)) - 1\n",
    "        normalized = normalized.fillna(0).clip(-0.999, 0.999)\n",
    "        \n",
    "        fisher = 0.5 * np.log((1 + normalized) / (1 - normalized))\n",
    "        return fisher.ewm(span=length, adjust=False).mean()\n",
    "    \n",
    "    df['Fisher'] = calculate_fisher(df['High'], df['Low'], fisher_length)\n",
    "\n",
    "    # === 3. Улучшенный Keltner Channel Width ===\n",
    "    def calculate_keltner_width(high, low, close, atr_period, multiplier):\n",
    "        typical_price = (high + low + close) / 3\n",
    "        ema = typical_price.ewm(span=atr_period, adjust=False).mean()\n",
    "        \n",
    "        # Более точный расчет ATR\n",
    "        tr = pd.concat([\n",
    "            high - low,\n",
    "            (high - close.shift()).abs(),\n",
    "            (low - close.shift()).abs()\n",
    "        ], axis=1).max(axis=1)\n",
    "        \n",
    "        atr = tr.ewm(span=atr_period, adjust=False).mean()\n",
    "        return (2 * multiplier * atr) / ema  # Нормализованная ширина\n",
    "    \n",
    "    df['KeltnerWidth'] = calculate_keltner_width(\n",
    "        df['High'], df['Low'], df['Close'], \n",
    "        keltner_atr_period, keltner_multiplier\n",
    "    )\n",
    "\n",
    "    # === Улучшенная обработка и нормализация ===\n",
    "    features = ['STC', 'Fisher', 'KeltnerWidth']\n",
    "    \n",
    "    # Двухэтапная очистка\n",
    "    df[features] = df[features].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Адаптивная нормализация (учет распределения)\n",
    "    for col in features:\n",
    "        q_low = df[col].quantile(0.01)\n",
    "        q_high = df[col].quantile(0.99)\n",
    "        df[col] = df[col].clip(q_low, q_high)\n",
    "        df[col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-10)  # Z-score нормализация\n",
    "    \n",
    "    return df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72107eb5-4cbf-4aca-92c2-fbe01459ef61",
   "metadata": {},
   "source": [
    "**RSI HTF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b05249da-734e-4216-bd3d-060a049e3b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsi_htf(df, timeframes=['1H'], rsi_periods=[7, 14], price_col='Close', date_col='Data'):\n",
    "    \"\"\"\n",
    "    Добавляет в DataFrame нормализованные RSI с старших таймфреймов\n",
    "    \n",
    "    Параметры:\n",
    "    df - исходный DataFrame\n",
    "    timeframes - список таймфреймов для расчета (по умолчанию ['1H'])\n",
    "    rsi_periods - списки периодов RSI (по умолчанию [7, 14])\n",
    "    price_col - название столбца с ценами (по умолчанию 'Close')\n",
    "    date_col - название столбца с датой (по умолчанию 'Data')\n",
    "    \"\"\"\n",
    "    # Сохраняем исходное состояние DataFrame\n",
    "    original_index = df.index\n",
    "    original_columns = df.columns.tolist()\n",
    "    was_index_datetime = isinstance(df.index, pd.DatetimeIndex)\n",
    "    \n",
    "    # Если 'Data' не в индексе или индекс не datetime, временно делаем datetime индексом\n",
    "    if date_col in df.columns:\n",
    "        df = df.set_index(date_col)\n",
    "    \n",
    "    # Убедимся, что индекс преобразован в DatetimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    for tf in timeframes:\n",
    "        # Ресемплинг на старший таймфрейм\n",
    "        df_htf = df[[price_col]].resample(tf.lower()).last().dropna()\n",
    "        \n",
    "        # Расчет RSI для каждого периода\n",
    "        for period in rsi_periods:\n",
    "            col_name = f'RSI{period}_{tf}'\n",
    "            df_htf[col_name] = ta.rsi(df_htf[price_col], length=period)\n",
    "        \n",
    "        # Объединение с исходным DataFrame\n",
    "        df = df.join(df_htf[[f'RSI{period}_{tf}' for period in rsi_periods]], how='left')\n",
    "        \n",
    "        # Заполнение пропусков и нормализация\n",
    "        for period in rsi_periods:\n",
    "            col_name = f'RSI{period}_{tf}'\n",
    "            norm_col = f'RSI{period}_{tf}_norm'\n",
    "            \n",
    "            df[col_name] = df[col_name].ffill()\n",
    "            df[norm_col] = df[col_name] / 100\n",
    "            df.drop(col_name, axis=1, inplace=True)\n",
    "    \n",
    "    # Восстанавливаем исходную структуру индекса и колонок\n",
    "    if date_col not in original_index.names and date_col not in original_columns:\n",
    "        df = df.reset_index()\n",
    "    elif date_col in original_columns and date_col not in df.columns:\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    # Если исходный индекс не был datetime, возвращаем его как было\n",
    "    if not was_index_datetime and date_col in df.columns:\n",
    "        df = df.set_index(original_index.name if original_index.name else original_index)\n",
    "    \n",
    "    # Убедимся, что колонка 'Data' осталась на месте\n",
    "    if date_col in original_columns and date_col not in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df.index if date_col == original_index.name else original_index)\n",
    "    \n",
    "    return df\n",
    "# def rsi_htf(df, timeframes=['1H'], rsi_periods=[7, 14], price_col='Close', date_col='Data'):\n",
    "#     \"\"\"\n",
    "#     Добавляет в DataFrame нормализованные RSI с старших таймфреймов\n",
    "    \n",
    "#     Параметры:\n",
    "#     df - исходный DataFrame\n",
    "#     timeframes - список таймфреймов для расчета (по умолчанию ['1H'])\n",
    "#     rsi_periods - списки периодов RSI (по умолчанию [7, 14])\n",
    "#     price_col - название столбца с ценами (по умолчанию 'Close')\n",
    "#     date_col - название столбца с датой (по умолчанию 'Data')\n",
    "#     \"\"\"\n",
    "#     # Сохраняем исходное состояние индекса\n",
    "#     was_index = date_col in df.index.names\n",
    "#     if not was_index:\n",
    "#         df = df.set_index(date_col)\n",
    "    \n",
    "#     for tf in timeframes:\n",
    "#         # Ресемплинг на старший таймфрейм (исправленная строка)\n",
    "        \n",
    "#         df_htf = df[[price_col]].resample(tf.lower()).last().dropna()\n",
    "        \n",
    "#         # Расчет RSI для каждого периода\n",
    "#         for period in rsi_periods:\n",
    "#             col_name = f'RSI{period}_{tf}'\n",
    "#             df_htf[col_name] = ta.rsi(df_htf[price_col], length=period)\n",
    "        \n",
    "#         # Объединение с исходным DataFrame\n",
    "#         df = df.join(df_htf[[f'RSI{period}_{tf}' for period in rsi_periods]], how='left')\n",
    "        \n",
    "#         # Заполнение пропусков и нормализация\n",
    "#         for period in rsi_periods:\n",
    "#             col_name = f'RSI{period}_{tf}'\n",
    "#             norm_col = f'RSI{period}_{tf}_norm'\n",
    "            \n",
    "#             df[col_name] = df[col_name].ffill()\n",
    "#             df[norm_col] = df[col_name] / 100\n",
    "#             df.drop(col_name, axis=1, inplace=True)\n",
    "    \n",
    "#     # Восстанавливаем исходную структуру индекса\n",
    "#     if not was_index:\n",
    "#         df = df.reset_index()\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b856f41d-eb0b-4c64-bbb4-2221a66635ab",
   "metadata": {},
   "source": [
    "**Дивергенция RSI, Stoch, MACD, OBV и цены**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2cacc25-0f42-4b43-9f65-d706e8254f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smooth_divergence(df, \n",
    "                              window=14,\n",
    "                              indicators={\n",
    "                                  'RSI': {'window': 14},\n",
    "                                  'Stoch': {'window': 14, 'k': 3, 'd': 3},\n",
    "                                  'MACD': {'fast': 12, 'slow': 26, 'signal': 9},\n",
    "                                  'OBV': {}\n",
    "                              }):\n",
    "    \"\"\"\n",
    "    Возвращает столбец с плавными значениями дивергенции (-1 до 1).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Расчет индикаторов\n",
    "    # RSI\n",
    "    if 'RSI' in indicators:\n",
    "        df['RSI'] = ta.rsi(df['Close'], length=indicators['RSI']['window'])\n",
    "    \n",
    "    # Stochastic\n",
    "    if 'Stoch' in indicators:\n",
    "        stoch = ta.stoch(df['High'], df['Low'], df['Close'],\n",
    "                        k=indicators['Stoch']['k'],\n",
    "                        d=indicators['Stoch']['d'])\n",
    "        stoch_col = [col for col in stoch.columns if 'STOCHk' in col][0]\n",
    "        df['Stoch_K'] = stoch[stoch_col]\n",
    "    \n",
    "    # MACD\n",
    "    if 'MACD' in indicators:\n",
    "        macd = ta.macd(df['Close'],\n",
    "                      fast=indicators['MACD']['fast'],\n",
    "                      slow=indicators['MACD']['slow'],\n",
    "                      signal=indicators['MACD']['signal'])\n",
    "        macd_col = [col for col in macd.columns if 'MACD_' in col][0]\n",
    "        df['MACD_line'] = macd[macd_col]\n",
    "    \n",
    "    # OBV\n",
    "    if 'OBV' in indicators:\n",
    "        df['OBV'] = ta.obv(df['Close'], df['Volume'])\n",
    "        df['OBV_norm'] = (df['OBV'] - df['OBV'].rolling(window).mean()) / (df['OBV'].rolling(window).std() + 1e-10)\n",
    "    \n",
    "    # 2. Расчет дивергенции\n",
    "    df['divergence'] = 0.0\n",
    "    \n",
    "    for i in range(window, len(df)):\n",
    "        values = []\n",
    "        weights = []\n",
    "        \n",
    "        # Собираем значения индикаторов\n",
    "        if 'RSI' in df.columns:\n",
    "            rsi_val = df['RSI'].iloc[i]\n",
    "            values.append((rsi_val - 30) / (70 - 30))  # Нормализация RSI 30-70\n",
    "            weights.append(0.35)\n",
    "        \n",
    "        if 'Stoch_K' in df.columns:\n",
    "            stoch_val = df['Stoch_K'].iloc[i]\n",
    "            values.append((stoch_val - 20) / (80 - 20))  # Нормализация Stochastic 20-80\n",
    "            weights.append(0.25)\n",
    "        \n",
    "        if 'MACD_line' in df.columns:\n",
    "            macd_val = df['MACD_line'].iloc[i]\n",
    "            # Нормализация MACD относительно его экстремумов в окне\n",
    "            macd_window = df['MACD_line'].iloc[i-window:i]\n",
    "            if len(macd_window) > 0:\n",
    "                macd_max = macd_window.max()\n",
    "                macd_min = macd_window.min()\n",
    "                if macd_max != macd_min:\n",
    "                    macd_norm = (macd_val - macd_min) / (macd_max - macd_min)\n",
    "                else:\n",
    "                    macd_norm = 0\n",
    "                values.append(macd_norm * 2 - 1)  # Приводим к диапазону [-1, 1]\n",
    "                weights.append(0.25)\n",
    "        \n",
    "        if 'OBV_norm' in df.columns:\n",
    "            obv_val = df['OBV_norm'].iloc[i]\n",
    "            values.append(np.tanh(obv_val))  # Ограничиваем диапазон\n",
    "            weights.append(0.15)\n",
    "        \n",
    "        # Рассчитываем дивергенцию только если есть минимум 2 индикатора\n",
    "        if len(values) >= 2:\n",
    "            # Взвешенное направление\n",
    "            direction = np.sum(np.diff(values) * weights[:-1]) / np.sum(weights[:-1])\n",
    "            \n",
    "            # Мера дисперсии\n",
    "            dispersion = (max(values) - min(values)) / (np.mean(np.abs(values)) + 1e-10)\n",
    "            \n",
    "            # Итоговое значение\n",
    "            df.at[df.index[i], 'divergence'] = direction * dispersion\n",
    "    \n",
    "    # 3. Нормализация результатов\n",
    "    max_div = df['divergence'].abs().max()\n",
    "    if max_div > 0:\n",
    "        df['divergence'] = df['divergence'] / max_div\n",
    "    \n",
    "    return df[['divergence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498e77b-debb-4856-904b-c3112aabae27",
   "metadata": {},
   "source": [
    "**EMA20, EMA50, VWMA20, HMA20, SMA100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1f462d9-e550-4a5a-8756-e8cbf0673650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_MA(df, price_col='Close', eps=1e-8):\n",
    "    price = df[price_col]\n",
    "    volume = df['Volume']\n",
    "\n",
    "    def normalize_diff(ma):\n",
    "        return np.clip((ma - price) / (price + eps), -1, 1)\n",
    "\n",
    "    # Скользящие средние\n",
    "    ema50 = ta.ema(price, 50)\n",
    "    ema200 = ta.ema(price, 200)\n",
    "    sma100 = ta.sma(price, 100)\n",
    "    sma200 = ta.sma(price, 200)\n",
    "\n",
    "    # Углы наклона\n",
    "    slope_ema50 = np.arctan(ema50.diff()) / np.pi\n",
    "    slope_ema200 = np.arctan(ema200.diff()) / np.pi\n",
    "\n",
    "    # Разности MA\n",
    "    ema50_ema200_diff = ema50 - ema200\n",
    "    sma100_sma200_diff = sma100 - sma200\n",
    "\n",
    "    # Бинарные признаки пересечений\n",
    "    ema50_above_ema200 = (ema50 > ema200).astype(int)\n",
    "    ema50_cross_up = ((ema50 > ema200) & (ema50.shift(1) <= ema200.shift(1))).astype(int)\n",
    "    ema50_cross_down = ((ema50 < ema200) & (ema50.shift(1) >= ema200.shift(1))).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        # Базовые отклонения\n",
    "        #'ema50_diff': normalize_diff(ema50),\n",
    "        'ema200_diff': normalize_diff(ema200),\n",
    "        #'sma100_diff': normalize_diff(sma100),\n",
    "        #'sma200_diff': normalize_diff(sma200),\n",
    "        \n",
    "        # Взаимодействие MA\n",
    "        'ema50_ema200_diff': ema50_ema200_diff,\n",
    "        'sma100_sma200_diff': sma100_sma200_diff,\n",
    "        'ema50_above_ema200': ema50_above_ema200,\n",
    "        'ema50_cross_up': ema50_cross_up,\n",
    "        'ema50_cross_down': ema50_cross_down,\n",
    "        \n",
    "        # Наклоны\n",
    "        'ema50_slope': slope_ema50,\n",
    "        'ema200_slope': slope_ema200,\n",
    "        'ema50_slope_vs_ema200': slope_ema50 - slope_ema200\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5381052-c045-4e04-b91c-35fc4d4f7c99",
   "metadata": {},
   "source": [
    "# EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b76c15-6949-4144-bc09-46265f82928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_EMA(df, price_col='Close', eps=1e-8):\n",
    "    price = df[price_col]\n",
    "\n",
    "    # Нормализованная цена для каждого окна\n",
    "    price_norm = price / price.rolling(60).mean()\n",
    "\n",
    "    # EMA по нормализованной цене\n",
    "    ema9 = ta.ema(price_norm, 9)\n",
    "    ema20 = ta.ema(price_norm, 20)\n",
    "    ema50 = ta.ema(price_norm, 50)\n",
    "    ema100 = ta.ema(price_norm, 100)\n",
    "    ema200 = ta.ema(price_norm, 200)\n",
    "\n",
    "    # Углы наклона EMA\n",
    "    slope_ema20 = np.arctan(ema20.diff(5)) / np.pi\n",
    "    slope_ema50 = np.arctan(ema50.diff(5)) / np.pi\n",
    "    slope_ema100 = np.arctan(ema100.diff(5)) / np.pi\n",
    "    slope_ema200 = np.arctan(ema200.diff(5)) / np.pi\n",
    "\n",
    "    # Изменения EMA\n",
    "    ema20_diff = ema20.diff(10)\n",
    "    ema50_diff = ema50.diff(25)\n",
    "    ema100_diff = ema100.diff(50)\n",
    "    ema200_diff = ema200.diff(100)\n",
    "\n",
    "    # Разности между EMA\n",
    "    ema20_ema50_diff = ema20 - ema50\n",
    "    ema50_ema100_diff = ema50 - ema100\n",
    "    ema100_ema200_diff = ema100 - ema200\n",
    "\n",
    "    # Цена выше EMA (используем обычную цену и обычные EMA)\n",
    "    df['price_above_ema20'] = (price > ta.ema(price, 20)).astype(int)\n",
    "    df['price_above_ema50'] = (price > ta.ema(price, 50)).astype(int)\n",
    "    df['price_above_ema100'] = (price > ta.ema(price, 100)).astype(int)\n",
    "    df['price_above_ema200'] = (price > ta.ema(price, 200)).astype(int)\n",
    "\n",
    "    # Кроссы\n",
    "    def crossed(series1, series2, window=5):\n",
    "        cross = ((series1 > series2) != (series1.shift(1) > series2.shift(1))).astype(int)\n",
    "        return cross.rolling(window).max().fillna(0).astype(int)\n",
    "\n",
    "    df['ema9_crossed_ema20_last5'] = crossed(ta.ema(price, 9), ta.ema(price, 20))\n",
    "    df['ema20_crossed_ema50_last5'] = crossed(ta.ema(price, 20), ta.ema(price, 50))\n",
    "    df['ema50_crossed_ema100_last5'] = crossed(ta.ema(price, 50), ta.ema(price, 100))\n",
    "    df['ema100_crossed_ema200_last5'] = crossed(ta.ema(price, 100), ta.ema(price, 200))\n",
    "\n",
    "    # Добавляем EMA-признаки\n",
    "    df['ema20_norm'] = ema20\n",
    "    df['ema50_norm'] = ema50\n",
    "    df['ema100_norm'] = ema100\n",
    "    df['ema200_norm'] = ema200\n",
    "\n",
    "    df['ema20_slope'] = slope_ema20\n",
    "    df['ema50_slope'] = slope_ema50\n",
    "    df['ema100_slope'] = slope_ema100\n",
    "    df['ema200_slope'] = slope_ema200\n",
    "\n",
    "    df['ema20_ema50_diff'] = ema20_ema50_diff\n",
    "    df['ema50_ema100_diff'] = ema50_ema100_diff\n",
    "    df['ema100_ema200_diff'] = ema100_ema200_diff\n",
    "\n",
    "    df['ema20_diff'] = ema20_diff\n",
    "    df['ema50_diff'] = ema50_diff\n",
    "    df['ema100_diff'] = ema100_diff\n",
    "    df['ema200_diff'] = ema200_diff\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc71ed-eba6-4de4-b384-300817157224",
   "metadata": {},
   "source": [
    "## EMA binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9d65bb43-112d-4a16-83f8-5f66535097e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_EMA_binary(df, price_col='Close'):\n",
    "    df = df.copy()\n",
    "    price = df[price_col]\n",
    "    \n",
    "    # EMA\n",
    "    ema9 = ta.ema(price, 9)\n",
    "    ema20 = ta.ema(price, 20)\n",
    "    ema50 = ta.ema(price, 50)\n",
    "    ema100 = ta.ema(price, 100)\n",
    "    ema200 = ta.ema(price, 200)\n",
    "\n",
    "    # Цена выше EMA\n",
    "    df['price_above_ema9'] = (price > ema9).astype(int)\n",
    "    df['price_above_ema20'] = (price > ema20).astype(int)\n",
    "    df['price_above_ema50'] = (price > ema50).astype(int)\n",
    "    df['price_above_ema100'] = (price > ema100).astype(int)\n",
    "    df['price_above_ema200'] = (price > ema200).astype(int)\n",
    "\n",
    "    # Пересечения EMA за последние 5 свечей\n",
    "    def crossed(series1, series2, window=5):\n",
    "        cross = ((series1 > series2) != (series1.shift(1) > series2.shift(1))).astype(int)\n",
    "        return cross.rolling(window).max().fillna(0).astype(int)\n",
    "\n",
    "    df['ema9_crossed_ema20_last5'] = crossed(ema9, ema20)\n",
    "    df['ema20_crossed_ema50_last5'] = crossed(ema20, ema50)\n",
    "    df['ema50_crossed_ema100_last5'] = crossed(ema50, ema100)\n",
    "    df['ema100_crossed_ema200_last5'] = crossed(ema100, ema200)\n",
    "\n",
    "    # Углы EMA и их рост (в радианах / pi)\n",
    "    def slope_increased(series, window=5):\n",
    "        slope = np.arctan(series.diff(window)) / np.pi\n",
    "        return (slope > slope.shift(window)).astype(int)\n",
    "\n",
    "    df['ema20_slope_increased'] = slope_increased(ema20)\n",
    "    df['ema50_slope_increased'] = slope_increased(ema50)\n",
    "    df['ema100_slope_increased'] = slope_increased(ema100)\n",
    "    df['ema200_slope_increased'] = slope_increased(ema200)\n",
    "\n",
    "    # ema20 и ema50 оба растут и расходятся\n",
    "    slope_20 = ema20.diff(5)\n",
    "    slope_50 = ema50.diff(5)\n",
    "    spread = ema20 - ema50\n",
    "    spread_change = spread.diff(5)\n",
    "    df['ema20_and_50_rising_diverging'] = ((slope_20 > 0) & (slope_50 > 0) & (spread_change > 0)).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5731bc3-efb9-499d-93a6-fa606fa24d11",
   "metadata": {},
   "source": [
    "# Объемные индикаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "36dede05-bb72-4daf-9204-c8a7bd1110dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_base_indicators(df, length=28, short_window=5):\n",
    "    \"\"\"\n",
    "    Добавляет в DataFrame нормализованные индикаторы объема:\n",
    "    - OBV: нормализованный через Z-скор (динамика изменения)\n",
    "    - Volume MA: отношение к текущему объёму (не к максимуму!)\n",
    "    - A/D_cum: процентное изменение за short_window свечей\n",
    "    - Force Index: стандартизированный (Z-норма)\n",
    "    \n",
    "    Параметры:\n",
    "        df: DataFrame с колонками ['Close', 'High', 'Low', 'Volume']\n",
    "        length: период для скользящих окон\n",
    "        short_window: окно для краткосрочных изменений (по умолчанию 5)\n",
    "    \"\"\"\n",
    "    # 1. On-Balance Volume (OBV) с Z-нормализацией\n",
    "    obv = (np.sign(df['Close'].diff()) * df['Volume']).cumsum()\n",
    "    df['obv_zscore'] = (obv - obv.rolling(length).mean()) / obv.rolling(length).std()\n",
    "    \n",
    "    # 2. Volume MA: отношение текущего объёма к его скользящей средней\n",
    "    df['volume_ma_ratio'] = df['Volume'] / df['Volume'].rolling(length).mean()\n",
    "    \n",
    "    # 3. A/D Cum: процентное изменение за short_window свечей\n",
    "    ad = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'].replace(0, 0.0001)) * df['Volume']\n",
    "    ad_cum = ad.rolling(length).sum()\n",
    "    df['ad_cum_pct_change'] = ad_cum.pct_change(short_window)  # % изменение за N свечей\n",
    "    \n",
    "    # 4. Force Index: стандартизированный\n",
    "    force_index = df['Close'].diff() * df['Volume']\n",
    "    df['force_index_z'] = (force_index - force_index.rolling(length).mean()) / force_index.rolling(length).std()\n",
    "    \n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7606b0f7-cb20-485f-8447-6a24b0e341e7",
   "metadata": {},
   "source": [
    "## volume_base_indicators_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "583d6960-7b05-432a-8559-d3433ae28eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def volume_base_indicators_binary(df: pd.DataFrame,\n",
    "                                   obv_ema_period: int = 20,\n",
    "                                   ad_ema_period: int = 20,\n",
    "                                   force_ema_period: int = 13,\n",
    "                                   vol_ma_period: int = 20,\n",
    "                                   vol_ma_type: str = 'ema') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет бинарные признаки объёмных индикаторов:\n",
    "    - OBV выше своей EMA\n",
    "    - A/D выше своей EMA\n",
    "    - Force Index выше своей EMA\n",
    "    - Текущий объём выше MA объёма\n",
    "    \n",
    "    Параметры:\n",
    "    - *_period: период сглаживания\n",
    "    - vol_ma_type: 'ema' или 'sma' для volume_ma_ratio\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # Проверка нужных колонок\n",
    "    required_cols = ['Close', 'High', 'Low', 'Volume']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"В DataFrame отсутствует обязательная колонка: {col}\")\n",
    "\n",
    "    # --- OBV ---\n",
    "    obv = np.where(df['Close'] > df['Close'].shift(), df['Volume'],\n",
    "          np.where(df['Close'] < df['Close'].shift(), -df['Volume'], 0))\n",
    "    obv = pd.Series(obv).cumsum()\n",
    "    obv_ema = obv.ewm(span=obv_ema_period, adjust=False).mean()\n",
    "    df[f'obv_gt_ema{obv_ema_period}'] = (obv > obv_ema).astype(int)\n",
    "\n",
    "    # --- A/D Line ---\n",
    "    mfm = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / ((df['High'] - df['Low']).replace(0, eps))\n",
    "    mfv = mfm * df['Volume']\n",
    "    ad = mfv.cumsum()\n",
    "    ad_ema = ad.ewm(span=ad_ema_period, adjust=False).mean()\n",
    "    df[f'ad_gt_ema{ad_ema_period}'] = (ad > ad_ema).astype(int)\n",
    "\n",
    "    # --- Force Index ---\n",
    "    force_index = df['Close'].diff() * df['Volume']\n",
    "    force_ema = force_index.ewm(span=force_ema_period, adjust=False).mean()\n",
    "    df[f'force_gt_ema{force_ema_period}'] = (force_index > force_ema).astype(int)\n",
    "\n",
    "    # --- Volume MA ratio ---\n",
    "    if vol_ma_type.lower() == 'ema':\n",
    "        vol_ma = df['Volume'].ewm(span=vol_ma_period, adjust=False).mean()\n",
    "        suffix = f'ema{vol_ma_period}'\n",
    "    else:\n",
    "        vol_ma = df['Volume'].rolling(window=vol_ma_period).mean()\n",
    "        suffix = f'sma{vol_ma_period}'\n",
    "\n",
    "    df[f'volume_gt_{suffix}'] = (df['Volume'] > vol_ma).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# df = volume_base_indicators_binary(df,obv_ema_period=21,ad_ema_period=14,force_ema_period=13,vol_ma_period=20,vol_ma_type='sma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef273a00-c2af-4f7b-9da6-eceb5b7dc292",
   "metadata": {},
   "source": [
    "## bullish_volume_dominance_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "77b866aa-9bb0-40d7-bb69-ca16999a50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bullish_volume_dominance_binary(df: pd.DataFrame, window: int = 10) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    if not all(col in df.columns for col in ['Open', 'Close', 'Volume']):\n",
    "        raise ValueError(\"DataFrame должен содержать колонки 'Open', 'Close', 'Volume'\")\n",
    "\n",
    "    bullish = (df['Close'] > df['Open']).astype(int)\n",
    "    bearish = (df['Close'] < df['Open']).astype(int)\n",
    "\n",
    "    bullish_volume = df['Volume'] * bullish\n",
    "    bearish_volume = df['Volume'] * bearish\n",
    "\n",
    "    bullish_sum = bullish_volume.rolling(window).sum()\n",
    "    bearish_sum = bearish_volume.rolling(window).sum()\n",
    "\n",
    "    column_name = f'bullish_volume_dominance_{window}'\n",
    "    df[column_name] = (bullish_sum > bearish_sum).astype(int)\n",
    "\n",
    "    return df  # теперь вернётся df со всеми колонками\n",
    "\n",
    "\n",
    "# df = bullish_volume_dominance_binary(df, window=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd2ab64-9e94-4273-818d-afa83d00d6a9",
   "metadata": {},
   "source": [
    "# vwap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "950dbca0-9361-4e62-b4a3-1d5541dfc6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VWAP(df: pd.DataFrame,\n",
    "         slope_period: int = 28,\n",
    "         slope_column_name: str = 'VWAP_slope') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет VWAP-индикаторы в DataFrame:\n",
    "    - vwap_intraday: VWAP со сбросом на полночь UTC\n",
    "    - vwap_ema6h: EMA от VWAP по 6 часам\n",
    "    - vwap_intraday_norm: нормализация через разницу с vwap_5\n",
    "    - vwap_ema6h_norm: нормализация через разницу с vwap_5\n",
    "    - VWAP_slope: процентное изменение VWAP за N периодов\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    eps = 1e-8\n",
    "\n",
    "    if not all(col in df.columns for col in ['Data', 'Close', 'Volume']):\n",
    "        raise ValueError(\"DataFrame должен содержать колонки 'Data', 'Close', 'Volume'\")\n",
    "    \n",
    "    df['datetime'] = pd.to_datetime(df['Data']).dt.tz_localize('UTC')\n",
    "    df['date_only'] = df['datetime'].dt.date\n",
    "\n",
    "    # VWAP\n",
    "    df['tp'] = df['Close']\n",
    "    df['tpv'] = df['tp'] * df['Volume']\n",
    "    df['cum_tpv'] = df.groupby('date_only')['tpv'].cumsum()\n",
    "    df['cum_vol'] = df.groupby('date_only')['Volume'].cumsum().replace(0, eps)\n",
    "    df['vwap_intraday'] = df['cum_tpv'] / df['cum_vol']\n",
    "\n",
    "    # EMA от VWAP (6ч = 360 свеч)\n",
    "    df['vwap_ema6h'] = df['vwap_intraday'].ewm(span=360, adjust=False).mean()\n",
    "\n",
    "    # Быстрый VWAP для нормализации\n",
    "    df['vwap_5'] = df['vwap_intraday'].ewm(span=5, adjust=False).mean()\n",
    "\n",
    "    # Альтернативная нормализация (как ты делал для EMA/ATR/Keltner)\n",
    "    df['vwap_intraday_norm'] = (df['vwap_intraday'] - df['vwap_5']) / (df['vwap_intraday'] + eps)\n",
    "    df['vwap_ema6h_norm'] = (df['vwap_ema6h'] - df['vwap_5']) / (df['vwap_ema6h'] + eps)\n",
    "\n",
    "    # Наклон VWAP в процентах\n",
    "    df[slope_column_name] = df['vwap_intraday'].pct_change(periods=slope_period) * 100\n",
    "    df[slope_column_name] = df[slope_column_name].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    # Удаление временных колонок\n",
    "    df.drop(columns=[\n",
    "        'datetime', 'date_only', 'tp', 'tpv', 'cum_tpv', 'cum_vol',\n",
    "        'vwap_intraday', 'vwap_ema6h', 'vwap_5'\n",
    "    ], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d985f5c-a353-428c-a2e2-dabdb0d7c29d",
   "metadata": {},
   "source": [
    "## vwap binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e25f1bf7-d409-4001-a2d3-14cf670ab5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VWAP_binary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет два бинарных признака:\n",
    "    - price_above_vwap_intraday: цена выше VWAP\n",
    "    - price_above_vwap_ema6h: цена выше EMA от VWAP (6 часов)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    eps = 1e-8\n",
    "\n",
    "    if not all(col in df.columns for col in ['Data', 'Close', 'Volume']):\n",
    "        raise ValueError(\"DataFrame должен содержать колонки 'Data', 'Close', 'Volume'\")\n",
    "    \n",
    "    df['datetime'] = pd.to_datetime(df['Data']).dt.tz_localize('UTC')\n",
    "    df['date_only'] = df['datetime'].dt.date\n",
    "\n",
    "    # VWAP (по Close)\n",
    "    df['tpv'] = df['Close'] * df['Volume']\n",
    "    df['cum_tpv'] = df.groupby('date_only')['tpv'].cumsum()\n",
    "    df['cum_vol'] = df.groupby('date_only')['Volume'].cumsum().replace(0, eps)\n",
    "    vwap_intraday = df['cum_tpv'] / df['cum_vol']\n",
    "\n",
    "    # EMA от VWAP (6 часов = 360 свечей при 1м таймфрейме)\n",
    "    vwap_ema6h = vwap_intraday.ewm(span=360, adjust=False).mean()\n",
    "\n",
    "    # Бинарные признаки\n",
    "    df['price_above_vwap_intraday'] = (df['Close'] > vwap_intraday).astype(int)\n",
    "    df['price_above_vwap_ema6h'] = (df['Close'] > vwap_ema6h).astype(int)\n",
    "\n",
    "    # Удалим временные и служебные колонки\n",
    "    df.drop(columns=['datetime', 'date_only', 'tpv', 'cum_tpv', 'cum_vol'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500d7bd-bad7-42d2-b8ba-494005400211",
   "metadata": {},
   "source": [
    "## VWAP updates: add_vwap_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cc76a48-2d26-4de8-90cc-f9225c8481d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vwap_features_with_norm(df: pd.DataFrame,\n",
    "                                 ema_span: int = 9,\n",
    "                                 vwap_ema_span: int = 360,\n",
    "                                 zscore_window: int = 100,\n",
    "                                 rsi_period: int = 14,\n",
    "                                 debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет три признака:\n",
    "    - vwap_ema6h_zscore: z-score от vwap_ema6h_norm\n",
    "    - vwap_price_distance: отклонение ema9 от vwap_ema6h_norm\n",
    "    - RSI_vwap_divergence: расхождение RSI и направления vwap_ema6h_norm\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "    eps = 1e-8\n",
    "\n",
    "    try:\n",
    "        if not all(col in df.columns for col in ['Close', 'Volume', 'Data']):\n",
    "            raise ValueError(\"В df должны быть колонки: 'Close', 'Volume', 'Data' (временные метки)\")\n",
    "\n",
    "        # EMA9 от цены\n",
    "        df['ema9'] = df['Close'].ewm(span=ema_span, adjust=False).mean()\n",
    "\n",
    "        # VWAP: интрадеевый по UTC\n",
    "        df['datetime'] = pd.to_datetime(df['Data']).dt.tz_localize('UTC')\n",
    "        df['date_only'] = df['datetime'].dt.date\n",
    "\n",
    "        df['tpv'] = df['ema9'] * df['Volume']\n",
    "        df['cum_tpv'] = df.groupby('date_only')['tpv'].cumsum()\n",
    "        df['cum_vol'] = df.groupby('date_only')['Volume'].cumsum().replace(0, eps)\n",
    "        df['vwap_intraday'] = df['cum_tpv'] / df['cum_vol']\n",
    "\n",
    "        # EMA от VWAP\n",
    "        df['vwap_ema6h'] = df['vwap_intraday'].ewm(span=vwap_ema_span, adjust=False).mean()\n",
    "        df['vwap_5'] = df['vwap_intraday'].ewm(span=5, adjust=False).mean()\n",
    "\n",
    "        # Нормализация\n",
    "        df['vwap_ema6h_norm'] = (df['vwap_ema6h'] - df['vwap_5']) / (df['vwap_ema6h'] + eps)\n",
    "\n",
    "        # Признак 1: z-score нормализованного VWAP\n",
    "        mean_norm = df['vwap_ema6h_norm'].rolling(zscore_window).mean()\n",
    "        std_norm = df['vwap_ema6h_norm'].rolling(zscore_window).std().replace(0, eps)\n",
    "        df['vwap_ema6h_zscore'] = (df['vwap_ema6h_norm'] - mean_norm) / std_norm\n",
    "\n",
    "        # Признак 2: отклонение ema9 от нормализованного VWAP\n",
    "        df['vwap_price_distance_direct'] = np.sign(df['ema9'] - df['vwap_ema6h']) * (np.abs(df['ema9'] - df['vwap_ema6h']) / df['Close'].rolling(100).std().clip(lower=eps))\n",
    "        df['vwap_price_distance'] = ((df['ema9'] - df['vwap_ema6h']) - (df['ema9'] - df['vwap_ema6h']).rolling(100).mean()) / ((df['ema9'] - df['vwap_ema6h']).rolling(100).std() + eps)\n",
    "\n",
    "        # RSI по ema9\n",
    "        delta = df['ema9'].diff()\n",
    "        gain = delta.clip(lower=0)\n",
    "        loss = -delta.clip(upper=0)\n",
    "\n",
    "        avg_gain = gain.rolling(rsi_period).mean()\n",
    "        avg_loss = loss.rolling(rsi_period).mean()\n",
    "        rs = avg_gain / (avg_loss + eps)\n",
    "        df['rsi'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "        # Признак 3: RSI vs VWAP\n",
    "        df['RSI_vwap_divergence'] = ((df['rsi'] - 50) / 50) - np.sign(df['vwap_ema6h_norm'])\n",
    "\n",
    "        # Убираем временные и промежуточные колонки\n",
    "        df.drop(columns=[\n",
    "            'datetime', 'date_only', 'tpv', 'cum_tpv', 'cum_vol',\n",
    "            'vwap_intraday', 'vwap_ema6h', 'vwap_5', 'rsi','ema9'\n",
    "        ], inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"❌ Ошибка в add_vwap_features_with_norm: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4cc279-ee81-4a75-b9b3-bacc4a2f5585",
   "metadata": {},
   "source": [
    "**liquidity_imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a5c89b09-86a5-4926-857a-f410838a228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_liquidity_imbalance(df, period=20, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    # Разница между покупками и продажами (упрощенная версия)\n",
    "    df['liq_imb'] = (2*df['Close'] - df['High'] - df['Low']) / (df['High'] - df['Low'] + 1e-10) * np.log1p(df['Volume'])\n",
    "    \n",
    "    # Скользящая нормализация\n",
    "    rolling_max = df['liq_imb'].abs().rolling(period).max()\n",
    "    df['liq_imb'] = df['liq_imb'] / (rolling_max + 1e-10)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce38e17-7e6b-44d0-a4a5-49c4d7ed4d49",
   "metadata": {},
   "source": [
    "**hidden_divergence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a0f9d81a-c654-438e-9ff7-2b879971d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hidden_divergence(df, rsi_period=14, inplace=False):\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    # Классический RSI\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(rsi_period).mean()\n",
    "    avg_loss = loss.rolling(rsi_period).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-10)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Скрытая дивергенция\n",
    "    price_higher = df['Close'] > df['Close'].shift(1)\n",
    "    rsi_lower = rsi < rsi.shift(1)\n",
    "    df['hidden_div'] = (\n",
    "    (price_higher & rsi_lower).astype(int) - \n",
    "    ((df['Close'] < df['Close'].shift(1)) & (rsi > rsi.shift(1))).astype(int)\n",
    ")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ca977-09e0-423c-bf00-5b51ddeadf92",
   "metadata": {},
   "source": [
    "**Ускорение цены**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7354a25-1590-4cb8-8a21-c65f64db356d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_price_acceleration(df, window=5):\n",
    "\n",
    "    # Создаем копию DataFrame чтобы не менять исходные данные\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Рассчитываем ускорение\n",
    "    velocity = result_df['Close'].diff(1)\n",
    "    acceleration = velocity.diff(1)\n",
    "    smoothed_accel = acceleration.rolling(window=window).mean()\n",
    "    \n",
    "    # Нормализация через tanh\n",
    "    price_std = result_df['Close'].pct_change().std()\n",
    "    if price_std > 0:\n",
    "        normalized_accel = np.tanh(smoothed_accel / price_std)\n",
    "    else:\n",
    "        normalized_accel = smoothed_accel * 0  # Если волатильность нулевая\n",
    "    \n",
    "    # Добавляем столбец\n",
    "    result_df['Acceleration'] = normalized_accel\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Пример использования\n",
    "# new_df = add_price_acceleration(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1957f4d7-3c14-486a-8cfc-97f689fbccc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86505d4b-bbc4-470d-9f32-cd106f9d13b7",
   "metadata": {},
   "source": [
    "**Модели сессий**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135914dd-4de5-4bf7-97d2-4901108d8556",
   "metadata": {},
   "source": [
    "Сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d2e6391-a4a6-4c23-93dd-1768c7a2a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trading_sessions(df):\n",
    "    df_sessions = df.copy()\n",
    "    df_sessions['Date'] = pd.to_datetime(df_sessions['Date'])\n",
    "    \n",
    "    # Векторизованное определение DST для всех дат сразу\n",
    "    tz_london = timezone('Europe/London')\n",
    "    tz_ny = timezone('America/New_York')\n",
    "    \n",
    "    # Локализуем даты в соответствующих временных зонах\n",
    "    london_dates = df_sessions['Date'].dt.tz_localize(tz_london, ambiguous='NaT', nonexistent='NaT')\n",
    "    ny_dates = df_sessions['Date'].dt.tz_localize(tz_ny, ambiguous='NaT', nonexistent='NaT')\n",
    "    \n",
    "    # Преобразуем в серии datetime и применяем dst()\n",
    "    df_sessions['London_DST'] = london_dates.apply(lambda x: x.dst().total_seconds() if pd.notna(x) else 0) != 0\n",
    "    df_sessions['NewYork_DST'] = ny_dates.apply(lambda x: x.dst().total_seconds() if pd.notna(x) else 0) != 0\n",
    "    \n",
    "    hour = df_sessions['Date'].dt.hour\n",
    "    minute = df_sessions['Date'].dt.minute\n",
    "    \n",
    "    # Asia (03:00–10:00 МСК зимой, 02:00–09:00 летом)\n",
    "    df_sessions['Asia'] = (\n",
    "        (~df_sessions['London_DST'] & (hour >= 3) & (hour < 10)) | \n",
    "        (df_sessions['London_DST'] & (hour >= 2) & (hour < 9))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Frankfurt (10:00–11:00 МСК зимой, 09:00–10:00 летом)\n",
    "    df_sessions['Frankfurt'] = (\n",
    "        (~df_sessions['London_DST'] & (hour >= 10) & (hour < 11)) | \n",
    "        (df_sessions['London_DST'] & (hour >= 9) & (hour < 10))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # London (11:00–20:00 МСК зимой, 10:00–19:00 летом)\n",
    "    df_sessions['London'] = (\n",
    "        (~df_sessions['London_DST'] & (hour >= 11) & (hour < 13)) | \n",
    "        (df_sessions['London_DST'] & (hour >= 10) & (hour < 12))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # NewYork (16:00–01:00 МСК зимой, 15:00–00:00 летом)\n",
    "    df_sessions['NewYork'] = (\n",
    "        (~df_sessions['NewYork_DST'] & ((hour >= 16) | (hour < 1))) | \n",
    "        (df_sessions['NewYork_DST'] & ((hour >= 15) | (hour < 0)))\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Lunch (07:00–08:00 МСК зимой, 06:00–07:00 летом)\n",
    "    df_sessions['Lunch'] = (\n",
    "        (~df_sessions['London_DST'] & ((hour == 13) | ((hour == 16) & (minute == 0)))) | \n",
    "        (df_sessions['London_DST'] & ((hour == 12) | ((hour == 15) & (minute == 0))))\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_sessions.drop(['London_DST', 'NewYork_DST'], axis=1, inplace=True)\n",
    "    \n",
    "    return df_sessions\n",
    "\n",
    "# Пример использования:\n",
    "# df = add_trading_sessions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d380f-84f7-4bf2-a601-cef1884801e2",
   "metadata": {},
   "source": [
    "Снятие сессий и дистанция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9827d89c-5550-45fa-942c-cdc53be5b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_distance(df):\n",
    "    sessions = ['Asia', 'Frankfurt', 'London', 'NewYork', 'Lunch']\n",
    "    df = df.copy()\n",
    "    \n",
    "    for session in sessions:\n",
    "        # Находим моменты начала сессий\n",
    "        session_starts = (df[session] == 1) & (df[session].shift(1) != 1)\n",
    "        \n",
    "        # Создаем столбцы с экстремумами предыдущей сессии\n",
    "        prev_high = df['High'].where(session_starts).ffill()\n",
    "        prev_low = df['Low'].where(session_starts).ffill()\n",
    "        \n",
    "        # Вычисляем диапазон предыдущей сессии (избегаем деления на 0)\n",
    "        prev_range = np.where((prev_high - prev_low) != 0, \n",
    "                             prev_high - prev_low, \n",
    "                             np.nan)\n",
    "        \n",
    "        # Вычисляем расстояния\n",
    "        distance = np.zeros(len(df))\n",
    "        \n",
    "        # Пробитие сверху (используем High текущей свечи)\n",
    "        above_mask = (df['High'] > prev_high) & (df[session] == 1)\n",
    "        distance[above_mask] = (df['High'] - prev_high)[above_mask] / prev_range[above_mask]\n",
    "        \n",
    "        # Пробитие снизу (используем Low текущей свечи)\n",
    "        below_mask = (df['Low'] < prev_low) & (df[session] == 1)\n",
    "        distance[below_mask] = (df['Low'] - prev_low)[below_mask] / prev_range[below_mask]\n",
    "        \n",
    "        # Ограничиваем значения и добавляем в DataFrame\n",
    "        df[f'{session}_Distance'] = np.clip(distance, -1, 1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb789a4-5a79-4e01-8039-8b86560b2b3f",
   "metadata": {},
   "source": [
    "**Снятие фракталов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "240aacb2-6ddf-4853-88fe-e5a0c0c6a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfp_fractals(df):\n",
    "    df = df.copy()\n",
    "    df['fractals_broken_count'] = 0\n",
    "    df['fractals_broken_ratio'] = -1.0\n",
    "    \n",
    "    # Предварительно вычисляем все фракталы (3-свечные минимумы) один раз\n",
    "    lows = df['Low'].values\n",
    "    fractal_lows = []\n",
    "    for i in range(1, len(df)-1):\n",
    "        if lows[i] < lows[i-1] and lows[i] < lows[i+1]:\n",
    "            fractal_lows.append((i, lows[i]))\n",
    "    \n",
    "    # Преобразуем в numpy массив для быстрой обработки\n",
    "    fractal_indices = np.array([x[0] for x in fractal_lows])\n",
    "    fractal_values = np.array([x[1] for x in fractal_lows])\n",
    "    \n",
    "    # Основной цикл\n",
    "    for i in range(len(df)):\n",
    "        start_idx = max(0, i - 14)\n",
    "        \n",
    "        # Быстрая фильтрация фракталов в окне с использованием numpy\n",
    "        mask = (fractal_indices >= start_idx) & (fractal_indices < i)\n",
    "        window_indices = fractal_indices[mask]\n",
    "        window_values = fractal_values[mask]\n",
    "        \n",
    "        if len(window_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        broken_count = 0\n",
    "        ratios = []\n",
    "        \n",
    "        # Проверяем последние 3 свечи + текущую\n",
    "        check_lows = lows[max(0, i-3):i+1]\n",
    "        \n",
    "        for j in range(len(window_values)):\n",
    "            if np.any(check_lows < window_values[j]):\n",
    "                broken_count += 1\n",
    "                \n",
    "                # Рассчитываем ratio только для самого нижнего фрактала\n",
    "                if window_values[j] == np.min(window_values):\n",
    "                    fractal_idx = window_indices[j]\n",
    "                    min_after = np.min(lows[fractal_idx+1:i+1])\n",
    "                    max_after = np.max(df['High'].values[fractal_idx+1:i+1])\n",
    "                    current_close = df['Close'].iloc[i]\n",
    "                    \n",
    "                    if current_close > window_values[j]:\n",
    "                        ratio = (current_close - window_values[j]) / (max_after - window_values[j]) if max_after > window_values[j] else 0.0\n",
    "                    else:\n",
    "                        ratio = (current_close - window_values[j]) / (window_values[j] - min_after) if min_after < window_values[j] else 0.0\n",
    "                    ratios.append(ratio)\n",
    "        \n",
    "        if broken_count > 0:\n",
    "            df.at[i, 'fractals_broken_count'] = broken_count\n",
    "            df.at[i, 'fractals_broken_ratio'] = np.clip(np.mean(ratios), -1, 1) if ratios else 0.0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a158bc-e062-457f-bc6d-b7b53ba7721e",
   "metadata": {},
   "source": [
    "**Kagi Phase Cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36763f90-5a2d-4dd5-bc16-aae91ee65abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kagi_conversion_line(df, conversion_period=9, baseline_period=26, vortex_period=14):\n",
    "    \"\"\"\n",
    "    Добавляет в DataFrame столбцы:\n",
    "    1. 'kagi_conversion' (норм. -1 до 1) - аналог Tenkan-sen на основе Kagi\n",
    "    2. 'phase_baseline' (норм. 0 до 1) - аналог Kijun-sen с Theta-фильтром\n",
    "    3. 'vortex_cloud' (норм. -1 до 1) - разница VI+/VI- (направление облака)\n",
    "    4. 'kagi_phase_diff' (норм. -1 до 1) - разница между 1 и 2 (трендовый импульс)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Kagi Conversion Line (аналог Tenkan)\n",
    "    def _kagi_line(close, threshold_series):\n",
    "        kagi = [close.iloc[0]]\n",
    "        direction = 1  # 1 = up, -1 = down\n",
    "        \n",
    "        for i in range(1, len(close)):\n",
    "            current_threshold = threshold_series.iloc[i]  # Берем текущее значение ATR\n",
    "            move = close.iloc[i] - kagi[-1]\n",
    "            \n",
    "            if (direction == 1 and move > 0) or (direction == -1 and move < 0):\n",
    "                kagi.append(kagi[-1] + move)\n",
    "            elif abs(move) >= current_threshold:\n",
    "                direction *= -1\n",
    "                kagi.append(kagi[-1] + move)\n",
    "            else:\n",
    "                kagi.append(kagi[-1])\n",
    "                \n",
    "        return pd.Series(kagi, index=close.index)\n",
    "    \n",
    "    # Вычисляем ATR для Kagi\n",
    "    atr = (df['High'] - df['Low']).rolling(conversion_period).mean()\n",
    "    kagi = _kagi_line(df['Close'], threshold_series=1.5*atr)\n",
    "    df['kagi_conversion'] = savgol_filter(kagi, window_length=conversion_period, polyorder=2)\n",
    "    \n",
    "    # 2. Phase Baseline (аналог Kijun с Theta-фильтром)\n",
    "    def _theta_filter(series, period):\n",
    "        theta = [series.iloc[:period].mean()]\n",
    "        for i in range(period, len(series)):\n",
    "            drift = (series.iloc[i-period:i].mean() - theta[-1]) / period\n",
    "            theta.append(theta[-1] + drift + 0.5*(series.iloc[i] - series.iloc[i-period]))\n",
    "        return pd.Series(theta, index=series.index[period-1:])\n",
    "    \n",
    "    median_price = (df['High'] + df['Low']) / 2\n",
    "    theta_baseline = _theta_filter(median_price, baseline_period)\n",
    "    df['phase_baseline'] = theta_baseline.reindex(df.index, method='ffill')\n",
    "    \n",
    "    # 3. Vortex Cloud (на основе VI+ и VI-)\n",
    "    tr = np.maximum(\n",
    "        df['High'] - df['Low'],\n",
    "        np.maximum(\n",
    "            abs(df['High'] - df['Close'].shift(1)),\n",
    "            abs(df['Low'] - df['Close'].shift(1))\n",
    "        )\n",
    "    )\n",
    "    vm_plus = abs(df['High'] - df['Low'].shift(1))\n",
    "    vm_minus = abs(df['Low'] - df['High'].shift(1))\n",
    "    \n",
    "    vi_plus = vm_plus.rolling(vortex_period).sum() / tr.rolling(vortex_period).sum()\n",
    "    vi_minus = vm_minus.rolling(vortex_period).sum() / tr.rolling(vortex_period).sum()\n",
    "    df['vortex_cloud'] = vi_plus - vi_minus  # разница для направления\n",
    "    \n",
    "    # 4. Комбинированный столбец (разница Kagi и Baseline)\n",
    "    df['kagi_phase_diff'] = df['kagi_conversion'] - df['phase_baseline']\n",
    "    \n",
    "    # Нормализация (-1 до 1 для трендовых, 0-1 для уровневых)\n",
    "    for col in ['kagi_conversion', 'vortex_cloud', 'kagi_phase_diff']:\n",
    "        df[col] = 2 * (df[col] - df[col].rolling(100).min()) / (\n",
    "            df[col].rolling(100).max() - df[col].rolling(100).min() + 1e-10) - 1\n",
    "    \n",
    "    df['phase_baseline'] = (df['phase_baseline'] - df['phase_baseline'].rolling(100).min()) / (\n",
    "        df['phase_baseline'].rolling(100).max() - df['phase_baseline'].rolling(100).min() + 1e-10)\n",
    "    \n",
    "    # Заполнение NaN (первые 100 баров)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b786d940-186d-4b04-bed6-a12d7319adb7",
   "metadata": {},
   "source": [
    "**Слом структуры**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "686f0739-ee88-4da5-99d7-602dfd571c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_structure_break_long(df: pd.DataFrame, window: int = 14) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Быстрая версия: ищет сломы структуры в long без тяжелых циклов.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['structure_break_long'] = 0\n",
    "\n",
    "    # 1. Найти high-фракталы Вильямса\n",
    "    is_fractal = (\n",
    "        (df['High'] > df['High'].shift(1)) &\n",
    "        (df['High'] > df['High'].shift(2)) &\n",
    "        (df['High'] > df['High'].shift(-1)) &\n",
    "        (df['High'] > df['High'].shift(-2))\n",
    "    )\n",
    "    fractal_indices = np.where(is_fractal)[0]\n",
    "    fractal_highs = df['High'].values\n",
    "\n",
    "    # 2. Создаем бинарный вектор длиной df, где 1 — если был слом в этом баре\n",
    "    break_marks = np.zeros(len(df), dtype=int)\n",
    "\n",
    "    for idx in fractal_indices:\n",
    "        # Проверка выхода за границы\n",
    "        if idx + 1 >= len(df):\n",
    "            continue\n",
    "\n",
    "        # Закрытия после фрактала\n",
    "        post_close = df['Close'].values[idx+1:]\n",
    "        break_found = np.where(post_close > fractal_highs[idx])[0]\n",
    "\n",
    "        if break_found.size > 0:\n",
    "            break_idx = idx + 1 + break_found[0]\n",
    "            break_marks[break_idx] = 1\n",
    "\n",
    "    # 3. Для каждого бара проверим, были ли \"сломы\" в предыдущих window барах\n",
    "    rolling_breaks = pd.Series(break_marks).rolling(window=window, min_periods=1).max().fillna(0).astype(int)\n",
    "    df['structure_break_long'] = rolling_breaks.values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e202bee-dfc7-400f-afe1-b48058313eb2",
   "metadata": {},
   "source": [
    "**Liquidity Imbalance Short-Squeeze Score\" (LISS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df863f33-595f-484e-aa37-2d6cacbbec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_fisher_transform(series, epsilon=1e-7):\n",
    "    \"\"\"Устойчивое преобразование Фишера без inf значений\"\"\"\n",
    "    series = np.clip(series, -1 + epsilon, 1 - epsilon)  # ограничиваем (-1, 1)\n",
    "    return 0.5 * np.log((1 + series) / (1 - series))\n",
    "\n",
    "def calculate_LISS(df):\n",
    "    # Нормализуем объем (если ещё не нормализован)\n",
    "    volume_norm = df['Volume'] / df['Volume'].rolling(20).max().replace(0, 1)\n",
    "    \n",
    "    # Устойчивый LISS\n",
    "    LISS = (2 * df['VP_Norm']) / (1 + expit(-df['atr_14_norm'])) - (df['STC'] * safe_fisher_transform(volume_norm))\n",
    "    \n",
    "    # Масштабируем в [0, 1] для удобства\n",
    "    LISS = (LISS - LISS.min()) / (LISS.max() - LISS.min() + 1e-10)\n",
    "    return LISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a0ba5-2d17-46f3-b822-bc2efa1fd18b",
   "metadata": {},
   "source": [
    "Тест группы новых индикаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97ed653-d913-40c1-85b9-819efb26b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "\n",
    "# 1. Tenkan-sen Breakout (Ишимоку)\n",
    "def add_tenkan_breakout(df, period=9):\n",
    "    df['tenkan_sen'] = (df['High'].rolling(period).max().shift(1) + \n",
    "                        df['Low'].rolling(period).min().shift(1)) / 2\n",
    "    df['tenkan_breakout'] = ((df['Close'] > df['tenkan_sen']) & \n",
    "                            (df['Close'].shift(1) <= df['tenkan_sen'].shift(1))).astype(int)\n",
    "    return df\n",
    "\n",
    "# 2. RSI Divergence (без look-ahead)\n",
    "def add_rsi_divergence(df, window=14, lookback=5):\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # Заменяем RSIIndicator на pandas_ta версию\n",
    "    rsi_values = rsi(close=df['Close'], length=window)\n",
    "    \n",
    "    df['rsi_low'] = rsi_values.rolling(lookback).min().shift(1)\n",
    "    df['price_low'] = df['Low'].rolling(lookback).min().shift(1)\n",
    "    df['bullish_div'] = ((df['rsi_low'].diff() > 0) & \n",
    "                        (df['price_low'].diff() < 0)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 3. Fisher Transform (без look-ahead)\n",
    "def add_fisher_transform(df, period=10):\n",
    "    hl2 = (df['High'] + df['Low']) / 2\n",
    "    max_hl2 = hl2.rolling(period).max().shift(1)  # shift(1) исключает текущую свечу\n",
    "    min_hl2 = hl2.rolling(period).min().shift(1)\n",
    "    normalized = (hl2 - min_hl2) / (max_hl2 - min_hl2 + eps)\n",
    "    fisher = 0.5 * np.log((1 + normalized) / (1 - normalized + eps))\n",
    "    df['fisher'] = fisher.clip(-1, 1)\n",
    "    return df\n",
    "\n",
    "# 4. False Breakout Detector (уже в целом правильно, но можно уточнить)\n",
    "def add_false_breakout(df, window=10):\n",
    "    df['high_prev'] = df['High'].rolling(window).max().shift(1)\n",
    "    df['low_prev'] = df['Low'].rolling(window).min().shift(1)\n",
    "    df['fake_bull'] = ((df['Low'] < df['low_prev']) & \n",
    "                      (df['Close'] > df['low_prev'])).astype(int)\n",
    "    return df\n",
    "\n",
    "# 5. H1 Trend (без look-ahead)\n",
    "def add_h1_trend(df, period=12):\n",
    "    sma_h1 = df['Close'].rolling(period).mean().shift(1)  # shift(1) исключает текущую свечу\n",
    "    trend = (df['Close'] - sma_h1) / (sma_h1 + eps)\n",
    "    df['h1_trend'] = trend.clip(-1, 1)\n",
    "    return df\n",
    "\n",
    "# 6. ATR Ratio (без look-ahead)\n",
    "def add_atr_ratio(df, short_period=14, long_period=12):\n",
    "    eps = 1e-8\n",
    "\n",
    "    atr_values = atr(\n",
    "        high=df['High'],\n",
    "        low=df['Low'],\n",
    "        close=df['Close'],\n",
    "        length=short_period\n",
    "    )\n",
    "    \n",
    "    atr_ratio = atr_values / (atr_values.rolling(long_period).mean().shift(1) + eps)\n",
    "    df['atr_ratio'] = atr_ratio.clip(0, 2) / 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 7. Long-term Liquidity Level (без look-ahead)\n",
    "def add_liquidity_distance(df, window=1000):\n",
    "    rounded = df['Close'].round(2)  # округление повышает шанс повторов\n",
    "    liq = rounded.rolling(window).apply(\n",
    "        lambda x: x.value_counts().idxmax() if not x.value_counts().empty else np.nan\n",
    "    ).shift(1)\n",
    "    df['liq_distance'] = ((df['Close'] - liq) / (liq + eps)).clip(-1, 1)\n",
    "    return df\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Применяем все функции к DataFrame\n",
    "# ---------------------------------------------------\n",
    "def add_all_features(df):\n",
    "    start_total = time.time()\n",
    "    \n",
    "    # 1. Tenkan Breakout\n",
    "    start = time.time()\n",
    "    df = add_tenkan_breakout(df)\n",
    "    #print(f\"Tenkan Breakout выполнена за {time.time() - start:.2f} сек\")\n",
    "    \n",
    "    # 2. RSI Divergence\n",
    "    start = time.time()\n",
    "    df = add_rsi_divergence(df)\n",
    "    #print(f\"RSI Divergence выполнена за {time.time() - start:.2f} сек\")\n",
    "    \n",
    "    # 3. Fisher Transform\n",
    "    start = time.time()\n",
    "    df = add_fisher_transform(df)\n",
    "    #print(f\"Fisher Transform выполнена за {time.time() - start:.2f} сек\")\n",
    "    \n",
    "    # 4. False Breakout\n",
    "    start = time.time()\n",
    "    df = add_false_breakout(df)\n",
    "    #print(f\"False Breakout выполнена за {time.time() - start:.2f} сек\")\n",
    "    \n",
    "    # 5. H1 Trend\n",
    "    start = time.time()\n",
    "    df = add_h1_trend(df)\n",
    "    #print(f\"H1 Trend выполнена за {time.time() - start:.2f} сек\")\n",
    "    \n",
    "    # 6. ATR Ratio\n",
    "    start = time.time()\n",
    "    df = add_atr_ratio(df)\n",
    "    #print(f\"ATR Ratio выполнена за {time.time() - start:.2f} сек\")\n",
    "    \n",
    "    # # 7. Liquidity Distance\n",
    "    # start = time.time()\n",
    "    # df = add_liquidity_distance(df)\n",
    "    # #print(f\"Liquidity Distance выполнена за {time.time() - start:.2f} сек\")\n",
    "    \n",
    "    # Удаляем промежуточные колонны\n",
    "    start = time.time()\n",
    "    cols_to_drop = ['tenkan_sen', 'rsi_low', 'price_low', 'high_prev', 'low_prev', 'liq_level']\n",
    "    df.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True)\n",
    "    #print(f\"Очистка промежуточных данных выполнена за {time.time() - start:.2f} сек\")\n",
    "    \n",
    "    #print(f\"\\nВсе операции выполнены за {time.time() - start_total:.2f} сек\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Пример использования:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# df = add_all_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fedac28-c961-4f31-8f6b-4171d6cbd237",
   "metadata": {},
   "source": [
    "**Теория четвертей**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1811554c-e0f0-4562-ace0-b453697c7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarter_theory(df, levels=2):\n",
    "    \"\"\"\n",
    "    Улучшенная версия индикатора четвертей с автоматической проверкой даты.\n",
    "    \n",
    "    Параметры:\n",
    "        df (pd.DataFrame): DataFrame с колонкой 'Data' (строка или datetime)\n",
    "        levels (int): Глубина разбиения (1=4 четверти, 2=16, 3=64 и т.д.)\n",
    "    \n",
    "    Возвращает:\n",
    "        pd.DataFrame: Копия исходного DF с добавленными колонками Q_L{level}_{quarter}\n",
    "        \n",
    "    Исключения:\n",
    "        ValueError: Если колонка 'Date' отсутствует или не может быть преобразована\n",
    "    \"\"\"\n",
    "    # Создаем копию, чтобы не менять исходный DF\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Проверка наличия колонки\n",
    "    if 'Data' not in result_df.columns:\n",
    "        raise ValueError(\"DataFrame должен содержать колонку 'Data'\")\n",
    "    \n",
    "    # Преобразование и проверка типа даты\n",
    "    try:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(result_df['Data']):\n",
    "            result_df['Data'] = pd.to_datetime(result_df['Data'], errors='raise')\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Ошибка преобразования даты: {str(e)}\")\n",
    "    \n",
    "    # Оптимизированное вычисление времени\n",
    "    dt = result_df['Data'].dt\n",
    "    time_in_sec = dt.hour * 3600 + dt.minute * 60 + dt.second\n",
    "    normalized_time = time_in_sec / 86400  # Нормализуем до [0, 1)\n",
    "    \n",
    "    # Предварительное вычисление всех индикаторов\n",
    "    quarter_cols = {}\n",
    "    for level in range(1, levels + 1):\n",
    "        num_quarters = 4 ** level\n",
    "        quarter_bins = np.arange(num_quarters + 1) / num_quarters\n",
    "        quarters = np.digitize(normalized_time, quarter_bins, right=False) - 1\n",
    "        \n",
    "        for q in range(num_quarters):\n",
    "            quarter_cols[f'Q_L{level}_{q+1}'] = (quarters == q).astype(np.int8)\n",
    "    \n",
    "    # Добавляем все колонки за одну операцию\n",
    "    result_df = pd.concat([result_df, pd.DataFrame(quarter_cols)], axis=1)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91eebf-8501-48ff-86c6-c1e57d9be5fe",
   "metadata": {},
   "source": [
    "ML индикаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9820a7f0-3b10-4e65-89a4-efeeabb71ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def _compute_tau(prices, lags):\n",
    "    tau = np.empty(len(lags))\n",
    "    for i in range(len(lags)):\n",
    "        lag = lags[i]\n",
    "        if lag >= len(prices):\n",
    "            tau[i] = np.nan\n",
    "        else:\n",
    "            diffs = prices[lag:] - prices[:-lag]\n",
    "            tau[i] = np.std(diffs)\n",
    "    return tau\n",
    "\n",
    "def hurst_exponent(prices, max_lag=100, poly_deg=1):\n",
    "    prices = np.asarray(prices)\n",
    "    if len(prices) < 20:  # абсолютный минимум\n",
    "        return np.nan\n",
    "\n",
    "    effective_max_lag = min(max_lag, len(prices) - 1)\n",
    "    lags = np.arange(2, effective_max_lag)\n",
    "\n",
    "    tau = _compute_tau(prices, lags)\n",
    "    # фильтруем недопустимые значения\n",
    "    valid = ~np.isnan(tau) & (tau > 0)\n",
    "    if valid.sum() < poly_deg + 1:\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        hurst = np.polyfit(np.log(lags[valid]), np.log(tau[valid]), deg=poly_deg)[0]\n",
    "        return hurst\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "@njit\n",
    "def sample_entropy(close_prices, m=2, r=0.2):\n",
    "    n = len(close_prices)\n",
    "    if n <= m:\n",
    "        return 0.0\n",
    "    std = np.std(close_prices)\n",
    "    if std == 0:\n",
    "        return 0.0\n",
    "    r *= std\n",
    "    \n",
    "    count = 0\n",
    "    patterns = np.lib.stride_tricks.sliding_window_view(close_prices, m)\n",
    "    \n",
    "    for i in range(len(patterns)):\n",
    "        for j in range(i+1, len(patterns)):\n",
    "            if np.max(np.abs(patterns[i] - patterns[j])) <= r:\n",
    "                count += 1\n",
    "                \n",
    "    return -np.log(count / (n - m)) if count > 0 else 0.0\n",
    "\n",
    "def dominant_frequency(close_prices):\n",
    "    n = len(close_prices)\n",
    "    yf = rfft(close_prices - np.mean(close_prices))\n",
    "    xf = rfftfreq(n, 1)\n",
    "    return xf[np.argmax(np.abs(yf))]\n",
    "\n",
    "def nar_residual(close_prices, lag=5):\n",
    "    X = np.array([close_prices[i-lag:i] for i in range(lag, len(close_prices))])\n",
    "    y = close_prices[lag:]\n",
    "    model = Ridge(alpha=1.0).fit(X, y)\n",
    "    return (y - model.predict(X))[-1] if len(y) > 0 else np.nan\n",
    "\n",
    "def wasserstein_distance(prices_window, window=100, compare=50):\n",
    "    if len(prices_window) < window:\n",
    "        return np.nan\n",
    "    current = prices_window[-compare:]\n",
    "    reference = prices_window[-window:-compare]\n",
    "    return ot.wasserstein_1d(current, reference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc8f07b-7bb6-4efc-841a-e9c787570b3f",
   "metadata": {},
   "source": [
    "# Keltner_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87af86ff-d946-41d7-a6b3-8e38f058d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Keltner_func(df, \n",
    "                 base_atr_period=40, \n",
    "                 base_multiplier=2,\n",
    "                 alt_atr_period=100,\n",
    "                 alt_multiplier=1.5,\n",
    "                 alt_ema_shift=5):\n",
    "    \n",
    "    df = df.copy()\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # 1. Базовый Keltner Channel Width\n",
    "    def _base_keltner(h, l, c, atr_period, multiplier):\n",
    "        typical_price = (h + l + c) / 3\n",
    "        ema = typical_price.ewm(span=atr_period, adjust=False).mean()\n",
    "        \n",
    "        tr = pd.concat([\n",
    "            h - l,\n",
    "            (h - c.shift()).abs(),\n",
    "            (l - c.shift()).abs()\n",
    "        ], axis=1).max(axis=1)\n",
    "        \n",
    "        atr = tr.ewm(span=atr_period, adjust=False).mean()\n",
    "        return (2 * multiplier * atr) / (ema + eps)\n",
    "    \n",
    "    # 2. Альтернативный Keltner Width\n",
    "    def _alt_keltner(h, l, c, atr_period, multiplier, shift):\n",
    "        typical_price = (h + l + c) / 3\n",
    "        ema = typical_price.shift(shift).ewm(span=atr_period, adjust=False).mean()\n",
    "        \n",
    "        tr = pd.concat([\n",
    "            h - l,\n",
    "            (h - c.shift()).abs(),\n",
    "            (l - c.shift()).abs()\n",
    "        ], axis=1).max(axis=1)\n",
    "        \n",
    "        atr = tr.ewm(span=atr_period * 2, adjust=False).mean()\n",
    "        return (multiplier * atr) / (ema + eps)\n",
    "\n",
    "    # Расчет быстрой версии Keltner для нормализации\n",
    "    df['KeltnerWidth_5'] = _base_keltner(df['High'], df['Low'], df['Close'], 5, 1.0)\n",
    "    df['KeltnerWidth_5'] = df['KeltnerWidth_5'].replace([np.inf, -np.inf], np.nan).ffill()\n",
    "    keltner5 = df['KeltnerWidth_5']  # Series для удобства\n",
    "\n",
    "    # Вычисляем основные Keltner Width\n",
    "    df['KeltnerWidth_v2'] = _base_keltner(df['High'], df['Low'], df['Close'],\n",
    "                                          base_atr_period, base_multiplier)\n",
    "    \n",
    "    df['KeltnerWidth_v3'] = _alt_keltner(df['High'], df['Low'], df['Close'],\n",
    "                                         alt_atr_period, alt_multiplier, alt_ema_shift)\n",
    "    \n",
    "    # Альтернативная нормализация\n",
    "    for col in ['KeltnerWidth_v2', 'KeltnerWidth_v3']:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan).ffill()\n",
    "        df[f'{col}_norm'] = (df[col] - keltner5) / (df[col] + eps)\n",
    "    df.drop(columns=['KeltnerWidth_5', 'KeltnerWidth_v2', 'KeltnerWidth_v3'     ], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb1d70-fc75-45f0-b6c7-081c1f51369c",
   "metadata": {},
   "source": [
    "**Супер Тренд**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d3745aca-2bc9-47d4-a5cd-cc7bab835d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_trend(df: pd.DataFrame, period: int = 10, multiplier: float = 3.0, eps: float = 1e-8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Расчёт индикатора SuperTrend с нормализованным выходом [-1, 1].\n",
    "    Добавляет один столбец: super_trend_{period}_{multiplier} ∈ [-1, 1]\n",
    "    где 1 - сильный бычий сигнал, -1 - сильный медвежий\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"Input DataFrame is empty or None\")\n",
    "\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Close']\n",
    "\n",
    "    hl2 = (high + low) / 2.0\n",
    "\n",
    "    # True Range с защитой от NaN\n",
    "    tr1 = (high - low).abs()\n",
    "    tr2 = (high - close.shift(1)).abs()\n",
    "    tr3 = (low - close.shift(1)).abs()\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1).fillna(0)\n",
    "\n",
    "    # ATR (SMA) — только прошлое\n",
    "    atr = tr.rolling(period).mean().shift(1)\n",
    "\n",
    "    # Upper/Lower Bands\n",
    "    upper_band = (hl2 + multiplier * atr).shift(1)\n",
    "    lower_band = (hl2 - multiplier * atr).shift(1)\n",
    "\n",
    "    # Инициализация\n",
    "    supertrend = pd.Series(index=df.index, dtype='float64')\n",
    "    trend_direction = pd.Series(index=df.index, dtype='float64')  # 1 или -1\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if i < period:\n",
    "            supertrend.iloc[i] = np.nan\n",
    "            trend_direction.iloc[i] = 0\n",
    "        elif i == period:\n",
    "            if close.iloc[i] < upper_band.iloc[i]:\n",
    "                supertrend.iloc[i] = upper_band.iloc[i]\n",
    "                trend_direction.iloc[i] = -1\n",
    "            else:\n",
    "                supertrend.iloc[i] = lower_band.iloc[i]\n",
    "                trend_direction.iloc[i] = 1\n",
    "        else:\n",
    "            prev_st = supertrend.iloc[i-1]\n",
    "            if trend_direction.iloc[i-1] > 0:\n",
    "                new_st = max(lower_band.iloc[i], prev_st)\n",
    "                if close.iloc[i] < new_st:\n",
    "                    supertrend.iloc[i] = upper_band.iloc[i]\n",
    "                    trend_direction.iloc[i] = -1\n",
    "                else:\n",
    "                    supertrend.iloc[i] = new_st\n",
    "                    trend_direction.iloc[i] = 1\n",
    "            else:\n",
    "                new_st = min(upper_band.iloc[i], prev_st)\n",
    "                if close.iloc[i] > new_st:\n",
    "                    supertrend.iloc[i] = lower_band.iloc[i]\n",
    "                    trend_direction.iloc[i] = 1\n",
    "                else:\n",
    "                    supertrend.iloc[i] = new_st\n",
    "                    trend_direction.iloc[i] = -1\n",
    "\n",
    "    # Нормализация сигнала с учетом направления и расстояния\n",
    "    normalized_signal = trend_direction * (1 - (atr / (abs(close - supertrend) + eps)).clip(0, 1))\n",
    "    normalized_signal = normalized_signal.fillna(0).clip(-1, 1)\n",
    "\n",
    "    # Добавляем столбец с уникальным именем, зависящим от параметров\n",
    "    df[f'super_trend_{period}_{multiplier}'] = normalized_signal\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2112a5-212f-4e1c-b34f-d5450ed5e053",
   "metadata": {},
   "source": [
    "**fibo_dinamic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c0b3d1ef-5590-4c76-b59e-af188942d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibo_dinamic(df: pd.DataFrame, period: int = 300, eps: float = 1e-8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет индикатор fibo_dinamic_<period>, нормализованный от -1 до 1.\n",
    "    Отражает положение Close относительно середины диапазона последних <period> свечей.\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"Input DataFrame is None or empty\")\n",
    "        \n",
    "    high_roll = df['High'].rolling(window=period)\n",
    "    low_roll = df['Low'].rolling(window=period)\n",
    "    \n",
    "    max_price = high_roll.max()\n",
    "    min_price = low_roll.min()\n",
    "    range_price = max_price - min_price\n",
    "    \n",
    "    midpoint = min_price + 0.5 * range_price  # уровень 0.5 фибо\n",
    "    \n",
    "    relative_position = (df['Close'] - midpoint) / (range_price + eps)\n",
    "    relative_position = relative_position.clip(-1, 1)  # ограничиваем диапазон\n",
    "    \n",
    "    col_name = f'fibo_dinamic_{period}'\n",
    "    df[col_name] = relative_position\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0992847b-07f9-4b1b-b78b-72dc2d637f41",
   "metadata": {},
   "source": [
    "**Deep learning индикаторы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a916c631-fff5-4b49-ae7e-c46798628bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Параметры по умолчанию\n",
    "EPS = 1e-8\n",
    "\n",
    "def spectral_entropy(signal, window_size=100):\n",
    "    \"\"\"Вычисляет спектральную энтропию для последнего окна\"\"\"\n",
    "    if len(signal) < window_size:\n",
    "        return np.nan\n",
    "\n",
    "    window = signal[-window_size:]\n",
    "    f, Pxx = welch(window - np.mean(window), nperseg=len(window))\n",
    "    Pxx = Pxx / (np.sum(Pxx) + EPS)\n",
    "    entropy = -np.sum(Pxx * np.log2(Pxx + EPS))\n",
    "    return entropy / np.log2(len(Pxx))  # нормализация\n",
    "\n",
    "def trend_stability(close, window=100):\n",
    "    \"\"\"Считает устойчивость тренда: сколько раз сменился знак\"\"\"\n",
    "    if len(close) < window:\n",
    "        return np.nan\n",
    "    diffs = np.diff(np.sign(np.diff(close[-window:])))\n",
    "    changes = np.sum(diffs != 0)\n",
    "    return 1 - changes / (window - 2 + EPS)  # нормализуем: меньше смен — выше стабильность\n",
    "\n",
    "def higuchi_fd(signal, k_max=5):\n",
    "    \"\"\"Быстрая аппроксимация фрактальной размерности методом Хигучи\"\"\"\n",
    "    if len(signal) < k_max + 1:\n",
    "        return np.nan\n",
    "    L = []\n",
    "    x = signal - np.mean(signal)\n",
    "    N = len(x)\n",
    "    for k in range(1, k_max + 1):\n",
    "        Lk = np.mean([\n",
    "            np.sum(np.abs(np.diff(x[m::k]))) * (N - 1) / (((N - m) // k) * k + EPS)\n",
    "            for m in range(k)\n",
    "        ])\n",
    "        L.append(Lk)\n",
    "    lnL = np.log(L)\n",
    "    lnk = np.log(np.arange(1, k_max + 1))\n",
    "    if np.any(np.isnan(lnL)):\n",
    "        return np.nan\n",
    "    coeffs = np.polyfit(lnk, lnL, 1)\n",
    "    return coeffs[0]  # результат обычно от 1 до 2\n",
    "\n",
    "# Обёртка для DataFrame\n",
    "def add_structural_features(df: pd.DataFrame, window: int = 100) -> pd.DataFrame:\n",
    "    closes = df['Close'].values\n",
    "\n",
    "    spectral = []\n",
    "    stability = []\n",
    "    fractal = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        window_data = closes[max(0, i - window + 1):i + 1]\n",
    "        spectral.append(spectral_entropy(window_data, window))\n",
    "        stability.append(trend_stability(window_data, window))\n",
    "        fractal.append(higuchi_fd(window_data, k_max=5))\n",
    "\n",
    "    df[f'spectral_entropy_{window}'] = spectral\n",
    "    df[f'trend_stability_{window}'] = stability\n",
    "    df[f'fractal_dim_{window}'] = fractal\n",
    "\n",
    "    # нормализация признаков к [0, 1] с защитой от деления на 0\n",
    "    se = df[f'spectral_entropy_{window}']\n",
    "    ts = df[f'trend_stability_{window}']\n",
    "    fd = df[f'fractal_dim_{window}']\n",
    "\n",
    "    # Фрактальная размерность [1, 2] → [0, 1]\n",
    "    fd_norm = (fd - 1.0) / (2.0 - 1.0 + EPS)\n",
    "\n",
    "    df[f'signal_complexity_{window}'] = se * ts * fd_norm\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b8063-7af7-4756-97cb-5dff4d81691d",
   "metadata": {},
   "source": [
    "**MACD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cde81910-f3f3-4536-8350-a951bc0be4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_macd(df: pd.DataFrame, fast=12, slow=26, signal=9, window_norm=100, eps=1e-8) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Возвращает нормализованный MACD (гистограмма) в виде Series.\n",
    "    Назначай его в df: df['macd_f_12_s26'] = add_macd(df, fast=12, slow=26)\n",
    "\n",
    "    - fast: период быстрой EMA\n",
    "    - slow: период медленной EMA\n",
    "    - signal: период сигнальной линии\n",
    "    - window_norm: окно для нормализации гистограммы\n",
    "    \"\"\"\n",
    "    if df is None or df.empty or 'Close' not in df.columns:\n",
    "        raise ValueError(\"DataFrame пустой или не содержит колонку 'Close'\")\n",
    "\n",
    "    ema_fast = df['Close'].ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = df['Close'].ewm(span=slow, adjust=False).mean()\n",
    "    macd = ema_fast - ema_slow\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "    macd_hist = macd - signal_line\n",
    "\n",
    "    # Нормализация гистограммы (от -1 до 1 на скользящем окне)\n",
    "    rolling_max = macd_hist.rolling(window=window_norm).max()\n",
    "    rolling_min = macd_hist.rolling(window=window_norm).min()\n",
    "    macd_rel = 2 * (macd_hist - rolling_min) / (rolling_max - rolling_min + eps) - 1\n",
    "\n",
    "    return macd_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369b4d1-5141-4b37-8b89-e642f32927a2",
   "metadata": {},
   "source": [
    "# Кастомные индикаторы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7f04a-2789-497c-aaf3-3d7b43b9a674",
   "metadata": {},
   "source": [
    "## Уровень сопротивления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "933b419e-77d7-4fca-a82c-4828adf262ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_resistance_distance(df: pd.DataFrame,\n",
    "                                  long_window: int = 200,\n",
    "                                  short_window: int = 20,\n",
    "                                  segment_size: int = 20,\n",
    "                                  price_col: str = 'Close',\n",
    "                                  ema_span: int = 9,\n",
    "                                  eps: float = 1e-8,\n",
    "                                  debug: bool = False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Индикатор расстояния до нисходящей наклонной линии сопротивления, построенной от глобального максимума.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(df) < long_window:\n",
    "            return pd.Series(1, index=df.index, name='resistance_slope_dist')\n",
    "\n",
    "        highs = df['High'].values\n",
    "        closes = df[price_col].values\n",
    "        ema9 = pd.Series(closes).ewm(span=ema_span, adjust=False).mean().values\n",
    "        result = np.full(len(df), 1.0, dtype=np.float32)\n",
    "\n",
    "        for i in range(long_window, len(df)):\n",
    "            window_highs = highs[i - long_window:i]\n",
    "            recent_highs = highs[i - short_window:i]\n",
    "\n",
    "            max_200 = np.max(window_highs)\n",
    "            max_20 = np.max(recent_highs)\n",
    "\n",
    "            if max_200 <= max_20:\n",
    "                continue  # нет нисходящего тренда\n",
    "\n",
    "            # Индекс глобального максимума в окне\n",
    "            global_max_idx = np.argmax(window_highs)\n",
    "            if global_max_idx >= long_window - segment_size:\n",
    "                continue  # слишком мало места после пика для сегментов\n",
    "\n",
    "            # Область после глобального максимума\n",
    "            segment_start = global_max_idx + 1\n",
    "            remaining = long_window - segment_start\n",
    "            num_segments = remaining // segment_size\n",
    "            if num_segments < 2:\n",
    "                continue\n",
    "\n",
    "            segment_highs = window_highs[segment_start:segment_start + num_segments * segment_size]\n",
    "            segment_highs_reshaped = segment_highs.reshape(num_segments, segment_size)\n",
    "            segment_maxima = np.nanmax(segment_highs_reshaped, axis=1)\n",
    "\n",
    "            if np.any(np.isnan(segment_maxima)):\n",
    "                continue\n",
    "\n",
    "            # Включаем сам пик как начальную точку\n",
    "            y_points = np.concatenate([[max_200], segment_maxima])\n",
    "            x_points = np.arange(len(y_points)).reshape(-1, 1)\n",
    "\n",
    "            # Линейная регрессия\n",
    "            model = LinearRegression()\n",
    "            model.fit(x_points, y_points)\n",
    "            predicted_resistance = model.predict([[len(y_points) - 1]])[0]\n",
    "\n",
    "            current_ema = ema9[i]\n",
    "            distance = (current_ema - predicted_resistance) / (abs(predicted_resistance) + eps)\n",
    "            result[i] = np.clip(distance, -1, 1)\n",
    "\n",
    "        return pd.Series(result, index=df.index, name='resistance_slope_dist')\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"⚠️ Resistance indicator error: {e}\")\n",
    "        return pd.Series(1, index=df.index, name='resistance_slope_dist')\n",
    "\n",
    "# df['resistance_slope_dist'] = calculate_resistance_distance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf744ab-2bc0-4ea3-b843-153a8a9f0c75",
   "metadata": {},
   "source": [
    "## Уровень поддержки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "da0ab1a7-fdb7-4b0c-ab2d-d4f69a20e4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support_distance(\n",
    "    df: pd.DataFrame,\n",
    "    long_window: int = 200,\n",
    "    short_window: int = 20,\n",
    "    segment_size: int = 20,\n",
    "    price_col: str = 'Close',\n",
    "    ema_span: int = 9,\n",
    "    eps: float = 1e-8,\n",
    "    debug: bool = False\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Индикатор расстояния до линии восходящей поддержки, построенной от глобального минимума.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(df) < long_window:\n",
    "            return pd.Series(-1, index=df.index, name='support_slope_dist')\n",
    "\n",
    "        lows = df['Low'].values\n",
    "        closes = df[price_col].values\n",
    "        ema9 = pd.Series(closes).ewm(span=ema_span, adjust=False).mean().values\n",
    "        result = np.full(len(df), -1.0, dtype=np.float32)\n",
    "\n",
    "        for i in range(long_window, len(df)):\n",
    "            window_lows = lows[i - long_window:i]\n",
    "            recent_lows = lows[i - short_window:i]\n",
    "\n",
    "            min_200 = np.min(window_lows)\n",
    "            min_20 = np.min(recent_lows)\n",
    "\n",
    "            # Условие на восходящую поддержку\n",
    "            if min_200 >= min_20:\n",
    "                continue\n",
    "\n",
    "            # Индекс глобального минимума в окне\n",
    "            global_min_idx = np.argmin(window_lows)\n",
    "            if global_min_idx >= long_window - segment_size:\n",
    "                continue  # слишком мало места после минимума для сегментов\n",
    "\n",
    "            # Область после глобального минимума\n",
    "            segment_start = global_min_idx + 1\n",
    "            remaining = long_window - segment_start\n",
    "            num_segments = remaining // segment_size\n",
    "            if num_segments < 2:\n",
    "                continue\n",
    "\n",
    "            segment_lows = window_lows[segment_start:segment_start + num_segments * segment_size]\n",
    "            segment_lows_reshaped = segment_lows.reshape(num_segments, segment_size)\n",
    "            segment_minima = np.nanmin(segment_lows_reshaped, axis=1)\n",
    "\n",
    "            if np.any(np.isnan(segment_minima)):\n",
    "                continue\n",
    "\n",
    "            # Включаем сам минимум как первую точку\n",
    "            y_points = np.concatenate([[min_200], segment_minima])\n",
    "            x_points = np.arange(len(y_points)).reshape(-1, 1)\n",
    "\n",
    "            # Линейная регрессия\n",
    "            model = LinearRegression()\n",
    "            model.fit(x_points, y_points)\n",
    "            predicted_support = model.predict([[len(y_points) - 1]])[0]\n",
    "\n",
    "            current_ema = ema9[i]\n",
    "            distance = (current_ema - predicted_support) / (abs(predicted_support) + eps)\n",
    "            result[i] = np.clip(distance, -1, 1)\n",
    "\n",
    "        return pd.Series(result, index=df.index, name='support_slope_dist')\n",
    "\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"⚠️ Support indicator error: {e}\")\n",
    "        return pd.Series(-1, index=df.index, name='support_slope_dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446f0c2-b403-4bdb-b083-84066e143f9a",
   "metadata": {},
   "source": [
    "# Кастомные индикаторы объема"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e0a0de33-df92-442a-a11e-9f0f60576329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_volume_dynamics(df: pd.DataFrame, col: str = 'Volume', spike_sigma: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет производные признаки от объема:\n",
    "    - rolling std, diff, percent change, 2-я производная\n",
    "    - volume spikes (всплески выше среднего + x * std)\n",
    "    - кластеры вверх/вниз\n",
    "    - нормализация [-1, 1] всех новых признаков\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Стандартное отклонение\n",
    "    df[f'{col}_rolling_std_10'] = df[col].rolling(window=10).std()\n",
    "    df[f'{col}_rolling_std_30'] = df[col].rolling(window=30).std()\n",
    "\n",
    "    # Первая производная\n",
    "    df[f'{col}_diff_1'] = df[col].diff()\n",
    "    df[f'{col}_rolling_diff_mean_10'] = df[col].diff().rolling(window=10).mean()\n",
    "\n",
    "    # Процентное изменение\n",
    "    df[f'{col}_pct_change'] = df[col].pct_change()\n",
    "    df[f'{col}_pct_change_rolling_mean_10'] = df[col].pct_change().rolling(window=10).mean()\n",
    "\n",
    "    # Вторая производная\n",
    "    df[f'{col}_diff2'] = df[col].diff().diff()\n",
    "\n",
    "    # Volume spikes (объем выше среднего + x * std)\n",
    "    vol_mean = df[col].rolling(window=20).mean()\n",
    "    vol_std = df[col].rolling(window=20).std()\n",
    "    spike_threshold = vol_mean + spike_sigma * vol_std\n",
    "    df[f'{col}_spike'] = (df[col] > spike_threshold).astype(int)\n",
    "    df[f'{col}_spike'] = df[f'{col}_spike'].rolling(window=10).sum()  # сумма спайков за окно\n",
    "\n",
    "    # Кластеры объема вверх/вниз\n",
    "    df['direction'] = np.sign(df['Close'] - df['Open'])  # +1 = вверх, -1 = вниз, 0 = без изменения\n",
    "    df[f'{col}_up_volume'] = df[col] * (df['direction'] > 0)\n",
    "    df[f'{col}_down_volume'] = df[col] * (df['direction'] < 0)\n",
    "\n",
    "    df[f'{col}_up_cluster'] = df[f'{col}_up_volume'].rolling(window=10).sum()\n",
    "    df[f'{col}_down_cluster'] = df[f'{col}_down_volume'].rolling(window=10).sum()\n",
    "\n",
    "    # Разность кластеров (направленная активность)\n",
    "    df[f'{col}_volume_cluster_diff'] = df[f'{col}_up_cluster'] - df[f'{col}_down_cluster']\n",
    "\n",
    "    # Удаляем промежуточные колонки\n",
    "    df.drop(columns=['direction', f'{col}_up_volume', f'{col}_down_volume'], inplace=True)\n",
    "\n",
    "    # Нормализация всех новых признаков (robust min-max)\n",
    "    for colname in df.columns:\n",
    "        if colname.startswith(col + '_') and not df[colname].isna().all():\n",
    "            rolling_min = df[colname].rolling(window=100).min()\n",
    "            rolling_max = df[colname].rolling(window=100).max()\n",
    "            df[colname] = 2 * (df[colname] - rolling_min) / (rolling_max - rolling_min + 1e-8) - 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e35b2f8-e3c3-459f-89eb-40b8125ba036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_volume_features(df: pd.DataFrame, window: int = 20, eps: float = 1e-9) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет признаки:\n",
    "    - Volume_skew: асимметрия (скошенность) объема\n",
    "    - Volume_volatility: стандартное отклонение объема (нормализованное)\n",
    "    - Volume_spike: на сколько текущий объем превышает медиану (от 0 до 1)\n",
    "\n",
    "    Защита от деления на 0 через eps. Без подглядываний (все lagged).\n",
    "    \"\"\"\n",
    "    vol = df['Volume']\n",
    "\n",
    "    # Скользящие метрики\n",
    "    rolling_median = vol.rolling(window).median()\n",
    "    rolling_mean = vol.rolling(window).mean()\n",
    "    rolling_std = vol.rolling(window).std()\n",
    "    rolling_skew = vol.rolling(window).skew()\n",
    "\n",
    "    # Volume_skew (асимметрия)\n",
    "    df[f'volume_skew_{window}'] = rolling_skew.clip(-5, 5) / 5  # нормализуем в диапазон -1..1\n",
    "\n",
    "    # Volume_volatility (волатильность объема)\n",
    "    df[f'volume_volatility_{window}'] = (rolling_std / (rolling_mean + eps)).clip(0, 5) / 5\n",
    "\n",
    "    # Volume_spike (от 0 до 1 — насколько выше медианы)\n",
    "    spike_score = (vol - rolling_median) / (rolling_median + eps)\n",
    "    df[f'volume_spike_{window}'] = spike_score.clip(0, 3) / 3  # только вверх, нормализуем\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279d1d7-e225-4dd4-847e-f3559b0c1dac",
   "metadata": {},
   "source": [
    "**Кастомные индикаторы объема и размера свеч**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9794848-96e8-4e5a-a449-2c478cefa379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_candle_vol_size_features(df, window=20, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Добавляет к df признаки, отражающие взаимосвязь свечных размеров и объема:\n",
    "    - candle_vol_size_prod: (body * full) / volume\n",
    "    - candle_vol_size_hmean: ((body * full) / (body + full)) / volume\n",
    "    Также добавляет версии:\n",
    "    - _tanh: ограничено от -1 до 1\n",
    "    - _log: логарифм от абсолютного значения\n",
    "    \"\"\"\n",
    "    body = (df['Close'] - df['Open']).abs()\n",
    "    full = df['High'] - df['Low']\n",
    "\n",
    "    size_prod = body * full\n",
    "    size_hmean = (body * full) / (body + full + eps)\n",
    "\n",
    "    # Нормализация по среднему объёму\n",
    "    avg_volume = df['Volume'].rolling(window).mean()\n",
    "    med_volume = df['Volume'].rolling(window).median()\n",
    "    V = df['Volume'] / (avg_volume + med_volume + eps)\n",
    "\n",
    "    size_prod_roll = size_prod / (size_prod.rolling(window).mean() + eps)\n",
    "    size_hmean_roll = size_hmean / (size_hmean.rolling(window).mean() + eps)\n",
    "\n",
    "    # Основные признаки\n",
    "    prod = size_prod_roll / (V + eps)\n",
    "    hmean = size_hmean_roll / (V + eps)\n",
    "\n",
    "    # Добавляем признаки в DataFrame\n",
    "    df['candle_vol_size_prod'] = prod\n",
    "    df['candle_vol_size_prod_tanh'] = np.tanh(prod)\n",
    "    df['candle_vol_size_prod_log'] = np.sign(prod) * np.log1p(np.abs(prod))\n",
    "\n",
    "    df['candle_vol_size_hmean'] = hmean\n",
    "    df['candle_vol_size_hmean_tanh'] = np.tanh(hmean)\n",
    "    df['candle_vol_size_hmean_log'] = np.sign(hmean) * np.log1p(np.abs(hmean))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d213671-3164-4a36-84af-1ef736b871b0",
   "metadata": {},
   "source": [
    "# add_choppiness_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d19dce4e-0679-4921-904f-6f595a1ac54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_choppiness_index(df, period=14, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Добавляет столбец 'choppiness_index_{period}' в DataFrame df.\n",
    "    Значения нормализованы от 0 до 1.\n",
    "    \"\"\"\n",
    "    high = df['High']\n",
    "    low = df['Low']\n",
    "    close = df['Close']\n",
    "\n",
    "    tr1 = high - low\n",
    "    tr2 = (high - close.shift(1)).abs()\n",
    "    tr3 = (low - close.shift(1)).abs()\n",
    "    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "\n",
    "    atr_sum = true_range.rolling(period).sum()\n",
    "    high_max = high.rolling(period).max()\n",
    "    low_min = low.rolling(period).min()\n",
    "    range_max_min = high_max - low_min\n",
    "\n",
    "    # CHOP = 100 * log10(ATR_sum / range) / log10(period)\n",
    "    chop = 100 * np.log10((atr_sum + eps) / (range_max_min + eps)) / np.log10(period + eps)\n",
    "\n",
    "    # Нормализация от 0 до 1 (по теоретическому диапазону от ~20 до ~61.8)\n",
    "    chop_norm = (chop - 20) / (61.8 - 20)\n",
    "    df[f'choppiness_index_{period}'] = chop_norm.clip(0, 1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219a915-80da-4c90-af93-6a108e2f60a8",
   "metadata": {},
   "source": [
    "# Bollinger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "47d14199-b149-4505-a4a0-b7fe41b76e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bollinger_features(df, period=20, std_mult=2, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Добавляет признаки по Bollinger Bands:\n",
    "    - расстояние от цены до средней\n",
    "    - нормализованная ширина полос\n",
    "    - z-оценка положения цены в канале\n",
    "    \"\"\"\n",
    "    price = df['Close']\n",
    "    sma = price.rolling(window=period).mean()\n",
    "    std = price.rolling(window=period).std()\n",
    "\n",
    "    upper = sma + std_mult * std\n",
    "    lower = sma - std_mult * std\n",
    "\n",
    "    # Расстояние до средней, нормализованное\n",
    "    df[f'bb_z_{period}'] = (price - sma) / (std + eps)\n",
    "\n",
    "    # Нормализованная ширина канала\n",
    "    df[f'bb_width_{period}'] = (upper - lower) / (sma + eps)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64daa66a-11b8-43ff-82ac-caa4c693e778",
   "metadata": {},
   "source": [
    "bollinger_awesome_alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3895c957-11b2-43cc-9157-63b482fce788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bollinger_awesome_alert(df, bb_use_ema=False, bb_filter=False, sqz_filter=False, \n",
    "                           bb_length=20, bb_mult=2.0, fast_ma_len=3, \n",
    "                           nLengthSlow=34, nLengthFast=5, sqz_length=100, \n",
    "                           sqz_threshold=50):\n",
    "    \"\"\"\n",
    "    Реализация Bollinger Awesome Alert R1.1 by JustUncleL.\n",
    "    Только добавляет новые колонки, ничего не удаляет.\n",
    "    Убраны коррелирующие признаки (оставлен один из каждой коррелирующей пары).\n",
    "    \n",
    "    Параметры:\n",
    "    - df: DataFrame с колонками ['Close', 'Open', 'Low', 'High', 'Volume']\n",
    "    - bb_use_ema: использовать EMA вместо SMA для Bollinger Bands (по умолчанию False)\n",
    "    - bb_filter: фильтровать сигналы по Bollinger Bands (по умолчанию False)\n",
    "    - sqz_filter: фильтровать сигналы по \"сжатию\" Bollinger Bands (по умолчанию False)\n",
    "    - остальные параметры соответствуют оригинальному индикатору\n",
    "    \n",
    "    Возвращает:\n",
    "    - Тот же DataFrame с добавленными колонками индикатора (без коррелирующих признаков)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Создаем копию DataFrame чтобы избежать предупреждений\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Bollinger Bands\n",
    "    df['bb_basis'] = df['Close'].ewm(span=bb_length, adjust=False).mean() if bb_use_ema else \\\n",
    "                     df['Close'].rolling(window=bb_length).mean()\n",
    "    \n",
    "    # Убраны bb_dev, bb_upper, bb_lower (коррелируют с bb_basis и между собой)\n",
    "    \n",
    "    # 2. Быстрая EMA\n",
    "    df['fast_ma'] = df['Close'].ewm(span=fast_ma_len, adjust=False).mean()\n",
    "    \n",
    "    # 3. Awesome Oscillator\n",
    "    hl2 = (df['High'] + df['Low']) / 2\n",
    "    # Убраны xSMA1_hl2 и xSMA2_hl2 (коррелируют с fast_ma и bb_basis)\n",
    "    df['xSMA1_SMA2'] = hl2.rolling(window=nLengthFast).mean() - hl2.rolling(window=nLengthSlow).mean()\n",
    "    \n",
    "    # Направление AO\n",
    "    df['AO'] = np.where(df['xSMA1_SMA2'] >= 0, \n",
    "                       np.where(df['xSMA1_SMA2'] > df['xSMA1_SMA2'].shift(1), 1, 2),\n",
    "                       np.where(df['xSMA1_SMA2'] > df['xSMA1_SMA2'].shift(1), -1, -2))\n",
    "    \n",
    "    # 4. Сжатие Bollinger Bands\n",
    "    # Убраны spread, avgspread, bb_squeeze (используется только bb_squeeze в фильтре)\n",
    "    if sqz_filter:\n",
    "        spread = 2 * df['Close'].rolling(window=bb_length).std() * bb_mult  # bb_upper - bb_lower = 2*bb_dev\n",
    "        avgspread = spread.rolling(window=sqz_length).mean()\n",
    "        df['bb_squeeze'] = spread / avgspread * 100\n",
    "    \n",
    "    # 5. ATR (для полноты, хотя в сигналах не используется)\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['Close'].shift())\n",
    "    low_close = np.abs(df['Low'] - df['Close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df['atr'] = true_range.rolling(window=14).mean()\n",
    "    # Убран bb_offset (коррелирует с atr)\n",
    "    \n",
    "    # 6. Генерация сигналов\n",
    "    df['BA_Signal'] = 0\n",
    "    \n",
    "    # Вычисляем bb_upper и bb_lower временно, если нужно для фильтра\n",
    "    if bb_filter or sqz_filter:\n",
    "        bb_dev = df['Close'].rolling(window=bb_length).std() * bb_mult\n",
    "        bb_upper = df['bb_basis'] + bb_dev\n",
    "        bb_lower = df['bb_basis'] - bb_dev\n",
    "    \n",
    "    # Условия для BUY\n",
    "    buy_cond = (\n",
    "        (df['fast_ma'] > df['bb_basis']) & \n",
    "        (df['fast_ma'].shift(1) <= df['bb_basis'].shift(1)) & \n",
    "        (df['Close'] > df['bb_basis']) & \n",
    "        (np.abs(df['AO']) == 1) & \n",
    "        ((not bb_filter) | (df['Close'] < bb_upper)) & \n",
    "        ((not sqz_filter) | (df['bb_squeeze'] > sqz_threshold))\n",
    "    )\n",
    "    \n",
    "    # Условия для SELL\n",
    "    sell_cond = (\n",
    "        (df['fast_ma'] < df['bb_basis']) & \n",
    "        (df['fast_ma'].shift(1) >= df['bb_basis'].shift(1)) & \n",
    "        (df['Close'] < df['bb_basis']) & \n",
    "        (np.abs(df['AO']) == 2) & \n",
    "        ((not bb_filter) | (df['Close'] > bb_lower)) & \n",
    "        ((not sqz_filter) | (df['bb_squeeze'] > sqz_threshold))\n",
    "    )\n",
    "    \n",
    "    df.loc[buy_cond, 'BA_Signal'] = 1    # BUY сигнал\n",
    "    df.loc[sell_cond, 'BA_Signal'] = -1   # SELL сигнал\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12a946-62a8-45c9-8561-9553582a8b72",
   "metadata": {},
   "source": [
    "# parabolic_sar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b9b4e4-ef2b-427c-b243-e12301e82827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parabolic_sar_feature(df, step=0.02, max_step=0.2):\n",
    "    \"\"\"\n",
    "    Добавляет нормализованный признак на основе индикатора Parabolic SAR.\n",
    "    Значения нормализованы относительно High и Low текущей свечи.\n",
    "    Без подглядывания в будущее.\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "    \n",
    "    psar_values = psar(\n",
    "        high=df['High'],\n",
    "        low=df['Low'],\n",
    "        close=df['Close'],\n",
    "        step=step,\n",
    "        max_step=max_step\n",
    "    )['PSARl_'+str(step)+'_'+str(max_step)]  # Получаем только значения PSAR\n",
    "    \n",
    "    # Нормализация: где находится PSAR по отношению к High/Low\n",
    "    df['parabolic_sar_norm'] = (psar_values - df['Low']) / (df['High'] - df['Low'] + eps)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ab682-5d92-4e36-8bd0-a96d1ece1c2d",
   "metadata": {},
   "source": [
    "# Индикатор волн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6d753feb-5f40-4c11-b1d6-47bc47e9ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wave_phase_position(df, lookback=700, smooth_window=21, price_col='Close'):\n",
    "    # Добавляем динамическое окно сглаживания\n",
    "    smooth_window = min(smooth_window, lookback//10)\n",
    "    \n",
    "    # Модифицированный Z-score (менее чувствительный к выбросам)\n",
    "    median = df[price_col].rolling(lookback).median()\n",
    "    mad = 1.4826 * df[price_col].rolling(lookback).apply(lambda x: np.median(np.abs(x - np.median(x))))\n",
    "    z = (df[price_col] - median) / (mad + 1e-8)\n",
    "    z = z.clip(-4,4)/4  # Более мягкое ограничение\n",
    "    \n",
    "    # Двойное сглаживание\n",
    "    ewma_fast = z.ewm(span=smooth_window//3).mean()\n",
    "    ewma_slow = z.ewm(span=smooth_window).mean()\n",
    "    \n",
    "    # Расхождение как дополнительный признак\n",
    "    divergence = ewma_fast - ewma_slow\n",
    "    \n",
    "    df['wave_phase'] = ewma_slow\n",
    "    df['phase_divergence'] = divergence\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a2eaefa2-2077-4df1-b41e-c93a8eb36810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fft_phase_position(df, price_col='Close', window=700):\n",
    "    \"\"\"\n",
    "    Добавляет колонку 'fft_phase_position' — фазу главной синусоиды, от -1 до 1.\n",
    "    Используется FFT на последних `window` точках.\n",
    "    \"\"\"\n",
    "    prices = df[price_col].values\n",
    "    fft_phase = np.full(len(prices), np.nan)  # результат\n",
    "    \n",
    "    for i in range(window, len(prices)):\n",
    "        segment = prices[i - window:i]\n",
    "        segment = segment - np.mean(segment)  # убираем DC-смещение\n",
    "        \n",
    "        fft = np.fft.fft(segment)\n",
    "        freqs = np.fft.fftfreq(window)\n",
    "        \n",
    "        # Берем только положительные частоты, исключая DC\n",
    "        pos_freqs = freqs[1:window // 2]\n",
    "        magnitudes = np.abs(fft[1:window // 2])\n",
    "        \n",
    "        if len(magnitudes) == 0:\n",
    "            continue\n",
    "        \n",
    "        dominant_idx = np.argmax(magnitudes)\n",
    "        phase = np.angle(fft[dominant_idx + 1])  # +1 из-за исключения DC\n",
    "        \n",
    "        # Нормируем фазу: от -π до π => от -1 до 1\n",
    "        phase_norm = phase / np.pi\n",
    "        fft_phase[i] = phase_norm\n",
    "\n",
    "    df['fft_phase_position'] = fft_phase\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5508b828-cd78-4bb8-8c97-e6937d12e053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_zscore(df, lookback=700, vol_span=100, price_col='Close'):\n",
    "    median = df[price_col].rolling(lookback).median()\n",
    "    ewma = df[price_col].ewm(span=vol_span)\n",
    "    ewma_std = (ewma.var())**0.5\n",
    "    z = (df[price_col] - median) / (ewma_std + 1e-8)\n",
    "    df['adaptive_zscore'] = z\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "24d1bcf0-e5c3-41df-84e9-626e00b43a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_position(df, lookback=700, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Позиция цены в историческом распределении (percentile rank).\n",
    "    Возвращает значения от -1 (минимум за период) до 1 (максимум).\n",
    "    \"\"\"\n",
    "    rolling_rank = df[price_col].rolling(lookback).rank(pct=True)\n",
    "    df['quantile_position'] = 2 * (rolling_rank - 0.5)  # преобразуем в [-1, 1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "90b8d162-1195-420a-bf38-4a5c24f65a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atr_position(df, lookback=700, atr_window=14, price_col='Close'):\n",
    "    # Расчет True Range\n",
    "    hl = df['High'] - df['Low']\n",
    "    hc = abs(df['High'] - df[price_col].shift(1))\n",
    "    lc = abs(df['Low'] - df[price_col].shift(1))\n",
    "    tr = pd.concat([hl, hc, lc], axis=1).max(axis=1)\n",
    "    \n",
    "    atr = tr.rolling(atr_window).mean()\n",
    "    price_ma = df[price_col].rolling(lookback).mean()\n",
    "    pos = (df[price_col] - price_ma) / (2 * atr + 1e-8)\n",
    "    df['atr_position'] = pos.clip(-1, 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5e7844fd-024d-4e70-92c7-d6c44cc46f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsi_stochastic_hybrid(df, rsi_window=14, stoch_window=14, smooth=3, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Гибрид RSI и Stochastic Oscillator.\n",
    "    Возвращает значения от -1 до 1.\n",
    "    \"\"\"\n",
    "    # RSI часть\n",
    "    delta = df[price_col].diff()\n",
    "    gain = delta.clip(lower=0).rolling(rsi_window).mean()\n",
    "    loss = (-delta.clip(upper=0)).rolling(rsi_window).mean()\n",
    "    rs = gain / (loss + 1e-8)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Stochastic часть\n",
    "    high = df['High'].rolling(stoch_window).max()\n",
    "    low = df['Low'].rolling(stoch_window).min()\n",
    "    stoch = 100 * (df[price_col] - low) / (high - low + 1e-8)\n",
    "    stoch_smooth = stoch.rolling(smooth).mean()\n",
    "    \n",
    "    # Комбинация (нормализованная)\n",
    "    hybrid = 0.5*(rsi/50 - 1) + 0.5*(stoch_smooth/50 - 1)\n",
    "    df['rsi_stoch_hybrid'] = hybrid.clip(-1, 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c26350e-5879-448a-bd83-bf695eecff5f",
   "metadata": {},
   "source": [
    "# Нормализованные Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d06138-8eb4-442f-b382-d90470b319a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_close_window_norm_pca(df: pd.DataFrame, window: int = 700, n_components: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет PCA-признаки по нормализованному окну Close за последние window свечей.\n",
    "    Возвращает df с новыми колонками: close_pca_0, ..., close_pca_{n_components-1}\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    close = df['Close'].values\n",
    "    features = []\n",
    "\n",
    "    for i in range(window, len(close)):\n",
    "        segment = close[i - window:i]\n",
    "        # Нормализация от -1 до 1\n",
    "        scaled = 2 * (segment - np.min(segment)) / (np.max(segment) - np.min(segment) + 1e-8) - 1\n",
    "        features.append(scaled)\n",
    "\n",
    "    # Преобразуем в матрицу\n",
    "    X = np.array(features)\n",
    "    \n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Названия колонок\n",
    "    col_names = [f'close_pca_{i}' for i in range(n_components)]\n",
    "\n",
    "    # Добавим NaN в начало (до window строк), чтобы длины совпадали\n",
    "    pca_df = pd.DataFrame(np.full((len(df), n_components), np.nan), columns=col_names)\n",
    "    pca_df.iloc[window:] = X_pca\n",
    "\n",
    "    # Добавляем к исходному df\n",
    "    df = pd.concat([df, pca_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ede21-f6d4-4038-b1da-799adfafec09",
   "metadata": {},
   "source": [
    "Колонки для CNN модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "33bf2177-a04b-4620-864b-1d3fe8d7ca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_close_window_columns(df, window=60, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Безопасное создание оконных признаков без утечки данных\n",
    "    с нормализацией внутри каждого окна.\n",
    "    \"\"\"\n",
    "    close = df[price_col].values\n",
    "    \n",
    "    # Создаём пустой массив для результатов\n",
    "    windowed = np.full((len(df), window), np.nan)\n",
    "    \n",
    "    for i in range(window, len(df)):\n",
    "        window_data = close[i-window:i]\n",
    "        \n",
    "        # Нормализация внутри окна (от -1 до 1)\n",
    "        min_val = window_data.min()\n",
    "        max_val = window_data.max()\n",
    "        normalized = 2 * ((window_data - min_val) / (max_val - min_val + 1e-8)) - 1\n",
    "        \n",
    "        windowed[i] = normalized\n",
    "    \n",
    "    # Добавляем колонки в DataFrame\n",
    "    for i in range(window):\n",
    "        df[f'close_win_{i}'] = windowed[:, i]\n",
    "    \n",
    "    return df.iloc[window:]  # Удаляем начальные NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294904ee-803f-4c1f-a8ca-11bb273e03ea",
   "metadata": {},
   "source": [
    "# TradingView индикаторы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90aa44a-3e2a-4596-b855-db013cda160a",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "fb2e0a81-00cd-4c00-b14e-c216e43093eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(df, \n",
    "        source_col='Close',  # По умолчанию используем Close для избежания look-ahead\n",
    "        long_term_div=True,\n",
    "        div_lookback_period=55,\n",
    "        fastLength=12,\n",
    "        slowLength=26,\n",
    "        signalLength=9,\n",
    "        smoother=2,\n",
    "        signal_duration=3):  # Продолжительность сигнала в свечах\n",
    "    \n",
    "    # Создаем копию DataFrame чтобы не изменять исходный\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Рассчитываем индикаторы (только по закрытым свечам)\n",
    "    fastMA = df[source_col].ewm(span=fastLength, adjust=False).mean()\n",
    "    slowMA = df[source_col].ewm(span=slowLength, adjust=False).mean()\n",
    "    macd = fastMA - slowMA\n",
    "    macd2 = (macd / slowMA) * 100\n",
    "    df['PPO'] = macd2.rolling(window=smoother).mean()\n",
    "    \n",
    "    # Функция для нахождения предыдущих экстремумов без look-ahead\n",
    "    def valuewhen(condition, series, occurrence):\n",
    "        # Создаем копию series с NaN где условие False\n",
    "        masked = series.where(condition)\n",
    "        # Заполняем вперед и сдвигаем\n",
    "        return masked.ffill().shift(occurrence + 1)  # +1 чтобы избежать look-ahead\n",
    "    \n",
    "    # Идентификация минимумов PPO\n",
    "    oscMins = (df['PPO'] > df['PPO'].shift(1)) & (df['PPO'].shift(1) < df['PPO'].shift(2))\n",
    "    \n",
    "    # Идентификация минимумов цены (по закрытым свечам)\n",
    "    low = df['Low']\n",
    "    cond1 = (low > low.shift(1)) & (low.shift(1) < low.shift(2))\n",
    "    cond2 = (low.shift(1) == low.shift(2)) & (low.shift(1) < low) & (low.shift(1) < low.shift(3))\n",
    "    cond3 = (low.shift(1) == low.shift(2)) & (low.shift(1) == low.shift(3)) & (low.shift(1) < low) & (low.shift(1) < low.shift(4))\n",
    "    cond4 = (low.shift(1) == low.shift(2)) & (low.shift(1) == low.shift(3)) & (low.shift(1) == low.shift(4)) & (low.shift(1) < low) & (low.shift(1) < low.shift(5))\n",
    "    \n",
    "    priceMins = cond1 | cond2 | cond3 | cond4\n",
    "    \n",
    "    # Находим экстремумы\n",
    "    current_ppo_bottom = valuewhen(oscMins, df['PPO'].shift(1), 0)\n",
    "    prev_ppo_bottom = valuewhen(oscMins, df['PPO'].shift(1), 1)\n",
    "    current_price_bottom = valuewhen(priceMins, df['Low'].shift(1), 0)\n",
    "    \n",
    "    # Рассчитываем y6 (фильтр для минимумов цены)\n",
    "    def rolling_min_with_offset(s, window):\n",
    "        return s.rolling(window, min_periods=1).min().shift(1)  # Сдвиг для избежания look-ahead\n",
    "    \n",
    "    y6 = valuewhen(oscMins, \n",
    "                  priceMins.rolling(5, min_periods=1).apply(\n",
    "                      lambda x: x.iloc[:-1][::-1].argmax() if x[:-1].any() else np.nan\n",
    "                  ).eq(0) * rolling_min_with_offset(df['Low'], 5), \n",
    "                  1)\n",
    "    \n",
    "    # Задержанные минимумы (без look-ahead)\n",
    "    delayedlow = (priceMins & (oscMins.rolling(3, min_periods=1).sum().shift(1) > 0)).astype(bool) * df['Low'].shift(1)\n",
    "    \n",
    "    # Бычьи дивергенции\n",
    "    bullish_div1 = ((current_price_bottom < y6) & oscMins & (current_ppo_bottom > prev_ppo_bottom))\n",
    "    bullish_div2 = ((delayedlow < y6) & (current_ppo_bottom > prev_ppo_bottom))\n",
    "    \n",
    "    # Долгосрочные дивергенции\n",
    "    if long_term_div:\n",
    "        long_term_bull_filt = valuewhen(priceMins, rolling_min_with_offset(df['Low'], div_lookback_period), 1)\n",
    "        i4 = (current_ppo_bottom > rolling_min_with_offset(df['PPO'], div_lookback_period))\n",
    "        i5 = (current_price_bottom < long_term_bull_filt)\n",
    "        i6 = (delayedlow < long_term_bull_filt)\n",
    "        \n",
    "        bullish_div3 = (oscMins & i4 & i5)\n",
    "        bullish_div4 = (i4 & i6)\n",
    "    else:\n",
    "        bullish_div3 = pd.Series(False, index=df.index)\n",
    "        bullish_div4 = pd.Series(False, index=df.index)\n",
    "    \n",
    "    # Комбинируем все дивергенции\n",
    "    all_bullish = bullish_div1 | bullish_div2 | bullish_div3 | bullish_div4\n",
    "    \n",
    "    # Создаем сигналы с продолжительностью signal_duration свечей\n",
    "    df['bullish_signal'] = 0\n",
    "    for i in range(len(df)):\n",
    "        if all_bullish.iloc[i]:\n",
    "            # Устанавливаем 1 на signal_duration последующих свечей\n",
    "            end_idx = min(i + signal_duration + 1, len(df))\n",
    "            df.loc[df.index[i:end_idx], 'bullish_signal'] = 1\n",
    "    \n",
    "    # Удаляем промежуточные колонки которые не нужны на выходе\n",
    "    cols_to_keep = ['PPO', 'bullish_signal']\n",
    "    result_cols = [col for col in df.columns if col in cols_to_keep]\n",
    "    \n",
    "    return df[result_cols + [c for c in df.columns if c not in result_cols]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84687cf4-8722-4950-924f-2581afd92a12",
   "metadata": {},
   "source": [
    "## TMA_Overlay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89eedb5-567f-4eb0-8ec9-280792446319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TMA_Overlay(\n",
    "    df, \n",
    "    show_100_line=True, \n",
    "    show_trend_fill=True, \n",
    "    show_bullish_3_line=True, \n",
    "    show_bullish_engulfing=True,\n",
    "    eps=1e-10\n",
    "):\n",
    "    \"\"\"\n",
    "    Ускоренная версия TMA_Overlay с сохранением исходной логики.\n",
    "    Все скользящие средние нормализованы относительно Close в диапазоне [-1, 1].\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Расчет SMMA через векторизованные операции\n",
    "    for length in [21, 50, 100, 200]:\n",
    "        if length == 100 and not show_100_line:\n",
    "            continue\n",
    "        \n",
    "        smma_col = f'smma_{length}'\n",
    "        # Первое значение SMMA = SMA\n",
    "        df[smma_col] = df['Close'].rolling(window=length, min_periods=1).mean()\n",
    "        # Рекурсивный расчет SMMA через формулу\n",
    "        df[smma_col] = (df[smma_col].shift(1) * (length - 1) + df['Close'])\n",
    "        df[smma_col] /= length\n",
    "    \n",
    "    # 2. Расчет EMA(2)\n",
    "    df['ema_2'] = df['Close'].ewm(span=2, adjust=False).mean()\n",
    "    \n",
    "    # 3. Нормализация скользящих средних\n",
    "    for col in ['smma_21', 'smma_50', 'smma_100', 'smma_200', 'ema_2']:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_norm'] = (df[col] - df['Close']) / (abs(df['Close']) + eps)\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    # 4. Индикатор тренда (Trend Fill)\n",
    "    if show_trend_fill:\n",
    "        df['trend_fill'] = np.where(\n",
    "            df['ema_2_norm'] > 0, 1, np.where(df['ema_2_norm'] < 0, -1, 0)\n",
    "        )\n",
    "    \n",
    "    # 5. Бычий 3 Line Strike (векторизованная версия)\n",
    "    if show_bullish_3_line:\n",
    "        bearish = df['Close'] < df['Open']\n",
    "        df['bull_3_line'] = 0\n",
    "        # Условия для 4 свечей подряд\n",
    "        condition = (\n",
    "            bearish.shift(3) &  # i-3\n",
    "            bearish.shift(2) &  # i-2\n",
    "            bearish.shift(1) &  # i-1\n",
    "            (df['Close'] > df['Open'].shift(1))  # i > Open(i-1)\n",
    "        )\n",
    "        df.loc[condition, 'bull_3_line'] = 1\n",
    "    \n",
    "    # 6. Бычий Engulfing (векторизованная версия)\n",
    "    if show_bullish_engulfing:\n",
    "        df['bull_engulfing'] = 0\n",
    "        condition = (\n",
    "            (df['Open'] <= df['Close'].shift(1)) &  # Open(i) <= Close(i-1)\n",
    "            (df['Open'] < df['Open'].shift(1)) &   # Open(i) < Open(i-1)\n",
    "            (df['Close'] > df['Open'].shift(1))     # Close(i) > Open(i-1)\n",
    "        )\n",
    "        df.loc[condition, 'bull_engulfing'] = 1\n",
    "    \n",
    "    return df\n",
    "# Применение функции к вашему DataFrame df = TMA_Overlay(df,show_100_line=True,  show_trend_fill=True, show_bullish_3_line=True, show_bullish_engulfing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c448e9-2619-4ba1-8fe2-bd7b5c5bb2b4",
   "metadata": {},
   "source": [
    "## Объёмный-взвешенный MACD (VW-MACD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f1429c-a225-4dac-910a-37b721ff0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Константа для защиты от деления на ноль\n",
    "EPS = 1e-10\n",
    "\n",
    "def VW_MACD(df, fast=12, slow=26, signal=9, normalize=True, eps=1e-8):\n",
    "    \"\"\"Добавляет VW-MACD (Volume-Weighted MACD) в DataFrame с простой нормализацией.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Volume-weighted moving averages\n",
    "    df['vwma_fast'] = (df['Close'] * df['Volume']).rolling(fast).sum() / (df['Volume'].rolling(fast).sum().replace(0, eps))\n",
    "    df['vwma_slow'] = (df['Close'] * df['Volume']).rolling(slow).sum() / (df['Volume'].rolling(slow).sum().replace(0, eps))\n",
    "    \n",
    "    # MACD components\n",
    "    df['vw_macd'] = df['vwma_fast'] - df['vwma_slow']\n",
    "    df['vw_signal'] = df['vw_macd'].ewm(span=signal, adjust=False).mean()\n",
    "    df['vw_hist'] = df['vw_macd'] - df['vw_signal']\n",
    "    \n",
    "    # Удаляем промежуточные колонки\n",
    "    df.drop(['vwma_fast', 'vwma_slow'], axis=1, inplace=True)\n",
    "\n",
    "    # Преобразование NaN и бесконечностей\n",
    "    macd_cols = ['vw_macd', 'vw_signal', 'vw_hist']\n",
    "    df[macd_cols] = df[macd_cols].replace([np.inf, -np.inf], np.nan).ffill().bfill().fillna(0)\n",
    "\n",
    "    # Простая нормализация по глобальному максимуму (по модулю)\n",
    "    if normalize:\n",
    "        for col in macd_cols:\n",
    "            max_val = df[col].abs().max() + eps\n",
    "            df[col] = df[col] / max_val\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3affb4-85a3-42cd-9b39-f174db8455d6",
   "metadata": {},
   "source": [
    "## Адаптивный RSI (Adaptive RSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e501a47-6dbe-450a-84f3-d6a770db40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adaptive_RSI(df, rsi_period=14, atr_period=14, normalize=True):\n",
    "    \"\"\"Добавляет Adaptive RSI в DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Вычисляем ATR (с заглавными буквами)\n",
    "    high_low = df['High'] - df['Low']\n",
    "    high_close = np.abs(df['High'] - df['Close'].shift(1))\n",
    "    low_close = np.abs(df['Low'] - df['Close'].shift(1))\n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(atr_period).mean().replace(0, EPS)\n",
    "    \n",
    "    # Адаптивный период\n",
    "    avg_atr = atr.rolling(atr_period).mean().replace(0, EPS)\n",
    "    volatility_ratio = atr / avg_atr\n",
    "    adaptive_period = (rsi_period / volatility_ratio).clip(5, 30).fillna(rsi_period).astype(int)\n",
    "    \n",
    "    # RSI с переменным окном\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    \n",
    "    def calculate_rsi(window):\n",
    "        avg_gain = window[window > 0].mean()\n",
    "        avg_loss = -window[window < 0].mean()\n",
    "        if avg_loss == 0:\n",
    "            return 100\n",
    "        rs = avg_gain / (avg_loss + EPS)\n",
    "        return 100 - (100 / (1 + rs))\n",
    "    \n",
    "    df['adaptive_rsi'] = delta.rolling(window=adaptive_period.max()).apply(calculate_rsi, raw=True)\n",
    "    \n",
    "    # Нормализация\n",
    "    if normalize:\n",
    "        df['adaptive_rsi'] = MinMaxScaler().fit_transform(df[['adaptive_rsi']])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fec012-b81d-440b-9a89-160b2002dc37",
   "metadata": {},
   "source": [
    "## EWO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "093de549-6151-48a8-b69e-b6fa032b0510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EWO(df, src_col='Close', sma1length=5, sma2length=35, UsePercent=True):\n",
    "    \"\"\"\n",
    "    Добавляет числовой признак Elliott Wave Oscillator для ML моделей\n",
    "    \n",
    "    Параметры:\n",
    "    df - DataFrame с данными\n",
    "    src_col - колонка с исходными данными (по умолчанию 'Close')\n",
    "    sma1length - период короткой SMA (по умолчанию 5)\n",
    "    sma2length - период длинной SMA (по умолчанию 35)\n",
    "    UsePercent - возвращать разницу в процентах (True) или абсолютное значение (False)\n",
    "    \n",
    "    Возвращает:\n",
    "    DataFrame с добавленной колонкой 'EWO'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Вычисляем SMA с минимальным периодом 1 (чтобы не было NaN в начале)\n",
    "    sma1 = df[src_col].rolling(window=sma1length, min_periods=1).mean()\n",
    "    sma2 = df[src_col].rolling(window=sma2length, min_periods=1).mean()\n",
    "    \n",
    "    # Вычисляем EWO (без визуальных меток)\n",
    "    df['EWO'] = (sma1 - sma2) / df[src_col] * 100 if UsePercent else (sma1 - sma2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Для Random Forest (разница в %) \n",
    "# df = EWO(df, src_col='Close', UsePercent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f075eb4-60c2-4b68-94d9-225d6b82f27e",
   "metadata": {},
   "source": [
    "## fibo_3_lines_dinamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7264afbb-7f5d-424e-bb79-159124e93e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibo_3_lines_dinamic(df, length=200, src='hlc3', mult=3.0, \n",
    "                         lines_to_use=[0.236, 0.5, 1.0], eps=1e-10):\n",
    "    \"\"\"\n",
    "    Возвращает DataFrame с колонками:\n",
    "    - fibo_upper: \n",
    "        *  1.0 — цена на верхней границе (upper_line),\n",
    "        *  0.0 — цена на средней линии (middle_line),\n",
    "        * -1.0 — цена сильно выше верхней границы.\n",
    "    - fibo_basis: нормированное отклонение от средней линии.\n",
    "    - fibo_lower:\n",
    "        *  1.0 — цена на нижней границе (lower_line),\n",
    "        *  0.0 — цена на средней линии,\n",
    "        * -1.0 — цена сильно ниже нижней границы.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Вычисляем источник данных (HLC3, Close и т.д.)\n",
    "    if src == 'hlc3':\n",
    "        source = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    elif src == 'close':\n",
    "        source = df['Close']\n",
    "    elif src == 'open':\n",
    "        source = df['Open']\n",
    "    elif src == 'high':\n",
    "        source = df['High']\n",
    "    elif src == 'low':\n",
    "        source = df['Low']\n",
    "    else:\n",
    "        raise ValueError(\"Неподдерживаемый источник данных. Используйте 'hlc3', 'close', 'open', 'high' или 'low'\")\n",
    "    \n",
    "    # Volume-Weighted Moving Average (VWMA)\n",
    "    cum_vol = source.rolling(window=length).sum()\n",
    "    cum_vol_price = (source * df['Volume']).rolling(window=length).sum()\n",
    "    basis = cum_vol_price / (cum_vol + eps)\n",
    "    \n",
    "    # Стандартное отклонение и границы\n",
    "    stdev = source.rolling(window=length).std()\n",
    "    dev = mult * stdev\n",
    "    \n",
    "    upper_line = basis + (lines_to_use[0] * dev)\n",
    "    middle_line = basis\n",
    "    lower_line = basis - (lines_to_use[2] * dev)\n",
    "    \n",
    "    # 1. fibo_upper: учитываем выход за верхнюю границу\n",
    "    df['fibo_upper'] = np.select(\n",
    "        [\n",
    "            df['Close'] >= upper_line,                     # Цена выше верхней границы → -1\n",
    "            (df['Close'] > middle_line) & (upper_line > middle_line),  # Между middle и upper → [0, 1]\n",
    "            df['Close'] <= middle_line                     # Ниже middle → 0\n",
    "        ],\n",
    "        [\n",
    "            -1.0,\n",
    "            (df['Close'] - middle_line) / (upper_line - middle_line + eps),\n",
    "            0.0\n",
    "        ],\n",
    "        default=0.0\n",
    "    ).clip(-1, 1)\n",
    "    \n",
    "    # 2. fibo_basis: нормированное отклонение от средней линии\n",
    "    df['fibo_basis'] = (df['Close'] - middle_line) / (dev + eps)\n",
    "    \n",
    "    # 3. fibo_lower: учитываем выход за нижнюю границу\n",
    "    df['fibo_lower'] = np.select(\n",
    "        [\n",
    "            df['Close'] <= lower_line,                     # Цена ниже нижней границы → -1\n",
    "            (df['Close'] < middle_line) & (middle_line > lower_line),  # Между lower и middle → [0, 1]\n",
    "            df['Close'] >= middle_line                     # Выше middle → 0\n",
    "        ],\n",
    "        [\n",
    "            -1.0,\n",
    "            (lower_line - df['Close']) / (lower_line - middle_line + eps),\n",
    "            0.0\n",
    "        ],\n",
    "        default=0.0\n",
    "    ).clip(-1, 1)\n",
    "    \n",
    "    return df\n",
    "# Предположим, ваш DataFrame называется df\n",
    "# df = fibo_3_lines_dinamic(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42234437-0766-454d-aeac-f712b6137237",
   "metadata": {},
   "source": [
    "## add_support_resistance_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6a597f5-2008-4368-a32a-2185b92a5b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_support_resistance_features(df: pd.DataFrame,\n",
    "                                     window: int = 1000,\n",
    "                                     lookback: int = 20,\n",
    "                                     vol_len: int = 2,\n",
    "                                     tolerance: float = 0.01) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет признаки поддержки и сопротивления на основе pivot-экстремумов и дельта-объёма,\n",
    "    рассчитываемые на скользящем окне (например, 1000 свечей).\n",
    "\n",
    "    Возвращает DF с новыми бинарными колонками:\n",
    "    - near_support_level\n",
    "    - near_resistance_level\n",
    "    - in_support_box\n",
    "    - in_resistance_box\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    eps = 1e-8\n",
    "\n",
    "    # ATR(200)\n",
    "    df['ATR'] = atr(high=df['High'], low=df['Low'], close=df['Close'], length=200)\n",
    "\n",
    "    df['near_support_level'] = 0\n",
    "    df['near_resistance_level'] = 0\n",
    "    df['in_support_box'] = 0\n",
    "    df['in_resistance_box'] = 0\n",
    "\n",
    "    for i in range(window, len(df)):\n",
    "        sub = df.iloc[i - window:i]\n",
    "        close = sub['Close'].values\n",
    "        open_ = sub['Open'].values\n",
    "        volume = sub['Volume'].values\n",
    "        atr_val = df['ATR'].iloc[i]\n",
    "\n",
    "        # Дельта-объём\n",
    "        up_vol = np.where(close > open_, volume, 0)\n",
    "        down_vol = np.where(close < open_, volume, 0)\n",
    "        delta_vol = up_vol - down_vol\n",
    "\n",
    "        vol_series = pd.Series(delta_vol)\n",
    "\n",
    "        vol_hi = vol_series.rolling(vol_len).max().iloc[-1]\n",
    "        vol_lo = vol_series.rolling(vol_len).min().iloc[-1]\n",
    "\n",
    "        # Пивоты\n",
    "        pivots_low = (sub['Low'] == sub['Low'].rolling(lookback, center=True).min())\n",
    "        pivots_high = (sub['High'] == sub['High'].rolling(lookback, center=True).max())\n",
    "\n",
    "        support_levels = sub['Low'][pivots_low & (vol_series > vol_hi)].dropna().values\n",
    "        resistance_levels = sub['High'][pivots_high & (vol_series < vol_lo)].dropna().values\n",
    "\n",
    "        current_price = df['Close'].iloc[i]\n",
    "\n",
    "        # Проверка \"рядом с уровнем\" (в пределах tolerance)\n",
    "        for lvl in support_levels:\n",
    "            if abs(current_price - lvl) / (lvl + eps) <= tolerance:\n",
    "                df.at[df.index[i], 'near_support_level'] = 1\n",
    "            if lvl <= current_price <= lvl + atr_val:\n",
    "                df.at[df.index[i], 'in_support_box'] = 1\n",
    "\n",
    "        for lvl in resistance_levels:\n",
    "            if abs(current_price - lvl) / (lvl + eps) <= tolerance:\n",
    "                df.at[df.index[i], 'near_resistance_level'] = 1\n",
    "            if lvl - atr_val <= current_price <= lvl:\n",
    "                df.at[df.index[i], 'in_resistance_box'] = 1\n",
    "\n",
    "    return df.drop(columns=['ATR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9876b-1eb9-4218-b665-1ae33c8e85ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb7f9c-c61e-454e-bf29-c3876c5b24fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python (trading_env)",
   "language": "python",
   "name": "trading_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
