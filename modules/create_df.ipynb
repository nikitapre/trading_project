{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73e24b21-b68d-4634-84d7-0dd8b4ecdb7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall_kline_changes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_indicators, add_target_column_mod, apply_main_indicators\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import random\n",
    "from .all_kline_changes import apply_indicators, add_target_column_mod, apply_main_indicators  # –ò–º–ø–æ—Ä—Ç –∏–∑ —Å–æ—Å–µ–¥–Ω–µ–≥–æ —Ñ–∞–π–ª–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94434f-b00c-4297-9886-6f787eaaf2a1",
   "metadata": {},
   "source": [
    "### –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–µ—á–µ–π —Å Bybit –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ae646a-34f5-428b-b7e8-de957a2ddf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–µ—á–µ–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "\n",
    "def fetch_kline_data(symbol, tf, start_ms, end_ms, \n",
    "                    max_retries=5, \n",
    "                    max_consecutive_failures=4,\n",
    "                    batch_size=200):\n",
    "    \"\"\"\n",
    "    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–≤–µ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å Bybit\n",
    "    —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–∫–∏ 403 –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
    "    \"\"\"\n",
    "    url = \"https://api.bybit.com/v5/market/kline\"\n",
    "    ms_tf = int(tf) * 60 * 1000\n",
    "    batch_count = math.ceil((end_ms - start_ms) / (batch_size * ms_tf))\n",
    "    \n",
    "    all_data = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Referer': 'https://www.bybit.com',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    consecutive_failures = 0\n",
    "    failed_batches = 0\n",
    "\n",
    "    for i in range(batch_count):\n",
    "        batch_start = start_ms + i * batch_size * ms_tf\n",
    "        batch_end = min(end_ms, batch_start + batch_size * ms_tf)\n",
    "        \n",
    "        params = {\n",
    "            'category': 'linear',\n",
    "            'symbol': symbol,\n",
    "            'interval': str(tf),\n",
    "            'start': int(batch_start),\n",
    "            'end': int(batch_end),\n",
    "            'limit': str(batch_size)\n",
    "        }\n",
    "\n",
    "        current_batch_failed = True\n",
    "        last_error = None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏\n",
    "                time.sleep(random.uniform(0.1, 0.5))\n",
    "                \n",
    "                r = requests.get(url, params=params, headers=headers, timeout=15)\n",
    "                \n",
    "                # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ 403 –æ—à–∏–±–∫–∏\n",
    "                if r.status_code == 403:\n",
    "                    error_msg = r.json().get('retMsg', 'Unknown error')\n",
    "                    raise requests.exceptions.HTTPError(\n",
    "                        f\"403 Forbidden: {error_msg}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: \"\n",
    "                        \"1) –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–∞\\n\"\n",
    "                        \"2) –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ä—ã–Ω–∫–∞\\n\"\n",
    "                        \"3) –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è API –∫–ª—é—á–µ–π\\n\"\n",
    "                        \"4) IP-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è\"\n",
    "                    )\n",
    "                \n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                \n",
    "                # –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ç–≤–µ—Ç–∞\n",
    "                if not data.get('result') or not isinstance(data['result'], dict):\n",
    "                    raise ValueError(\"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ API\")\n",
    "                \n",
    "                result_list = data['result'].get('list', [])\n",
    "                if not result_list:\n",
    "                    print(f\"‚ö†Ô∏è {symbol} | –ë–∞—Ç—á {i+1}/{batch_count} | –ü—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ –¥–∞–Ω–Ω—ã—Ö\")\n",
    "                    break\n",
    "                \n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—à—É —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
    "                df = process_batch_data(result_list)\n",
    "                all_data.append(df)\n",
    "                consecutive_failures = 0\n",
    "                current_batch_failed = False\n",
    "                break\n",
    "                \n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                last_error = str(e)\n",
    "                if '403' in last_error:\n",
    "                    print(f\"üîí –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞: {last_error}\")\n",
    "                    return None\n",
    "                print(f\"‚ùå HTTP Error [{symbol} | –ë–∞—Ç—á {i+1}/{batch_count} | –ü–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries}]: {last_error[:200]}\")\n",
    "                time.sleep(2 ** attempt + random.uniform(0.1, 1.0))\n",
    "            except Exception as e:\n",
    "                last_error = str(e)\n",
    "                print(f\"‚ùå –û—à–∏–±–∫–∞ [{symbol} | –ë–∞—Ç—á {i+1}/{batch_count} | –ü–æ–ø—ã—Ç–∫–∞ {attempt+1}/{max_retries}]: {last_error[:200]}\")\n",
    "                time.sleep(2 ** attempt + random.uniform(0.1, 1.0))\n",
    "        \n",
    "        if current_batch_failed:\n",
    "            consecutive_failures += 1\n",
    "            failed_batches += 1\n",
    "            print(f\"üö´ –ë–∞—Ç—á {i+1}/{batch_count} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error[:200]}\")\n",
    "            \n",
    "            if consecutive_failures >= max_consecutive_failures:\n",
    "                print(f\"‚è© –ü—Ä–µ—Ä—ã–≤–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {symbol} (–ø–æ–¥—Ä—è–¥ {consecutive_failures} –Ω–µ—É–¥–∞—á–Ω—ã—Ö –±–∞—Ç—á–µ–π)\")\n",
    "                break\n",
    "\n",
    "    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "    if not all_data:\n",
    "        print(f\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol} –ø–æ—Å–ª–µ {failed_batches} –Ω–µ—É–¥–∞—á–Ω—ã—Ö –±–∞—Ç—á–µ–π\")\n",
    "        return None\n",
    "        \n",
    "    final_df = pd.concat(all_data).drop_duplicates().sort_values('Data').reset_index(drop=True)\n",
    "    \n",
    "    if failed_batches > 0:\n",
    "        print(f\"üü° {symbol} –∑–∞–≥—Ä—É–∂–µ–Ω —á–∞—Å—Ç–∏—á–Ω–æ: {len(all_data)}/{batch_count} –±–∞—Ç—á–µ–π\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def process_batch_data(result):\n",
    "    \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –±–∞—Ç—á–∞\"\"\"\n",
    "    data = pd.DataFrame(result)\n",
    "    return pd.DataFrame({\n",
    "        'Data': pd.to_datetime(data.iloc[:, 0].astype('float'), unit='ms'),\n",
    "        'Open': data.iloc[:, 1].astype('float'),\n",
    "        'High': data.iloc[:, 2].astype('float'),\n",
    "        'Low': data.iloc[:, 3].astype('float'),\n",
    "        'Close': data.iloc[:, 4].astype('float'),\n",
    "        'Volume': data.iloc[:, 5].astype('float')\n",
    "    }).sort_values('Data')\n",
    "\n",
    "def kline_candles(symbol, tf, start=None, end=None, n_candles=None, \n",
    "                 length=20, target_candles=5, target=0.05, rr_threshold=2,\n",
    "                 min_completeness=0.7):  # –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö (0.7 = 70%)\n",
    "    try:\n",
    "        ms_tf = int(tf) * 60 * 1000\n",
    "\n",
    "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞\n",
    "        if start and end:\n",
    "            start_dt = datetime.strptime(start, \"%Y-%m-%d %H:%M\")\n",
    "            end_dt = datetime.strptime(end, \"%Y-%m-%d %H:%M\")\n",
    "            start_ms = int(start_dt.timestamp() * 1000)\n",
    "            end_ms = int(end_dt.timestamp() * 1000)\n",
    "            expected_candles = ((end_ms - start_ms) / ms_tf) + 1\n",
    "        elif n_candles:\n",
    "            end_ms = int(time.time() * 1000)\n",
    "            start_ms = end_ms - n_candles * ms_tf\n",
    "            expected_candles = n_candles\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "        full_df = fetch_kline_data(symbol, tf, start_ms, end_ms)\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "        if full_df is None:\n",
    "            print(f\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        if full_df.empty:\n",
    "            print(f\"‚ö†Ô∏è –ü—É—Å—Ç–æ–π DataFrame –¥–ª—è {symbol}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö\n",
    "        actual_candles = len(full_df)\n",
    "        completeness = actual_candles / expected_candles\n",
    "        \n",
    "        if completeness < min_completeness:\n",
    "            print(f\"‚ö†Ô∏è {symbol} –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é: {actual_candles}/{int(expected_candles)} —Å–≤–µ—á–µ–π ({completeness:.1%})\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "        #full_df = apply_main_indicators(full_df, length)\n",
    "        full_df = add_target_column_mod(full_df, target_candles, target, rr_threshold)\n",
    "        full_df.dropna(inplace=True)\n",
    "        \n",
    "        if full_df.empty:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è {symbol}\")\n",
    "            \n",
    "        return full_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ kline_candles –¥–ª—è {symbol}: {str(e)[:200]}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c31d0-75c6-4de5-992e-9e16aa7979c7",
   "metadata": {},
   "source": [
    "### –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –ø–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88b87d3a-b406-4d26-9cd5-8569c3679369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kline_data_from_csv(file_path, length=20, target_candles=50, target=0.025, rr_threshold =2):\n",
    "    start_all = time.time()\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "\n",
    "    expected_columns = {'Data', 'Open', 'High', 'Low', 'Close', 'Volume'}\n",
    "    if not expected_columns.issubset(df.columns):\n",
    "        raise ValueError(f\"CSV –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–æ–ª–±—Ü—ã: {expected_columns}\")\n",
    "\n",
    "    \n",
    "    df['Data'] = pd.to_datetime(df['Data'])\n",
    "    df = df.sort_values('Data').reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
    "    t0 = time.time()\n",
    "    #apply_indicators(full_df, length) –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ 55 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "    #apply_main_indicators(full_df, length)  –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –ª—É—á—à–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
    "    df_ind = apply_main_indicators(df.copy(), length)\n",
    "    print(f\"üìà –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: {time.time() - t0:.2f} —Å–µ–∫\")\n",
    "\n",
    "    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
    "    t0 = time.time()\n",
    "    new_columns = [col for col in df_ind.columns if col not in df.columns]\n",
    "    df = pd.concat([df, df_ind[new_columns]], axis=1)\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "    print(f\"üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ: {time.time() - t0:.2f} —Å–µ–∫\")\n",
    "\n",
    "    # –¶–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ\n",
    "    \n",
    "    df = add_target_column_mod(df, target_candles, target, rr_threshold)\n",
    "    \n",
    "\n",
    "    # –û—á–∏—Å—Ç–∫–∞ NaN\n",
    "    rows_before = df.shape[0]\n",
    "    cols_before = df.shape[1]\n",
    "    total_nans_before = df.isna().sum().sum()\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    rows_after = df.shape[0]\n",
    "    cols_after = df.shape[1]\n",
    "    total_nans_after = df.isna().sum().sum()\n",
    "    \n",
    "\n",
    "    print(f\"üßº –£–¥–∞–ª–µ–Ω–æ —Å—Ç—Ä–æ–∫ —Å NaN: {rows_before - rows_after}\")\n",
    "    print(f\"üßº –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ NaN –¥–æ: {total_nans_before}, –ø–æ—Å–ª–µ: {total_nans_after}\")\n",
    "    print(f\"üìä –†–∞–∑–º–µ—Ä —Ç–∞–±–ª–∏—Ü—ã: –¥–æ ‚Äî {rows_before}x{cols_before}, –ø–æ—Å–ª–µ ‚Äî {rows_after}x{cols_after}\")\n",
    "\n",
    "    print(f\"‚è±Ô∏è –û–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {time.time() - start_all:.2f} —Å–µ–∫\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27280a-c618-439d-a49e-d42bc90fd7a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b37b729-abbd-475a-a8c3-37d34f6e4341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bacdf5-f11f-4e20-b892-5aff204d073f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631412f8-a76a-4037-970e-3a3a63090486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
